{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDow+aB0+8R/zfM7/L96MU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/LCEL%2C%20Runnables%20y%20cadenas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. LCEL, LangChain Expression Language**\n",
        "---\n",
        "\n",
        "LCEL se introdujo en Langchain **a mediados de 2023**, espec√≠ficamente con la **versi√≥n 0.0.142**, lanzada el **19 de julio de 2023**\n",
        "\n",
        "La introducci√≥n de **LCEL** en Langchain fue una respuesta a la necesidad de una forma m√°s potente, flexible y f√°cil de usar para construir aplicaciones de lenguaje complejas. Proporcion√≥ una sintaxis declarativa, mejor√≥ la legibilidad, facilit√≥ la depuraci√≥n y habilit√≥ funcionalidades avanzadas como el streaming y la ejecuci√≥n as√≠ncrona, consolid√°ndose como una pieza fundamental del ecosistema de Langchain.\n",
        "\n",
        "**LCEL se basa en la Composici√≥n, no en Tipos Predefinidos:**\n",
        "\n",
        "-   **Operador Pipe (|):** La piedra angular de LCEL es el operador pipe. Este operador te permite encadenar componentes de forma secuencial, enviando la salida de un componente como entrada al siguiente. Esto es inherentemente secuencial, pero no se define como un tipo de cadena \"secuencial\".\n",
        "    \n",
        "-   **Primitivas Runnable:** LCEL se basa en la interfaz Runnable. Cualquier objeto que implemente esta interfaz puede ser parte de una cadena LCEL. Esto incluye modelos de lenguaje, prompts, parsers, retrievers, etc.\n",
        "    \n",
        "-   **Flexibilidad Total:** La clave es que puedes combinar estas primitivas Runnable de cualquier manera que tenga sentido para tu aplicaci√≥n. No est√°s limitado a estructuras predefinidas.\n",
        "\n",
        "**Ventajas de este enfoque:**\n",
        "\n",
        "-   **Mayor Flexibilidad:** No est√°s limitado por las estructuras predefinidas. Puedes crear flujos de trabajo exactamente como los necesitas.\n",
        "    \n",
        "-   **Reutilizaci√≥n de Componentes:** Los componentes individuales pueden ser reutilizados en diferentes cadenas con diferentes flujos de trabajo.\n",
        "    \n",
        "-   **Claridad y Composici√≥n:** El uso del operador pipe hace que la l√≥gica de la cadena sea m√°s clara y f√°cil de entender.\n",
        "    \n",
        "-   **Optimizaci√≥n:** La forma en que construyes la cadena influye en c√≥mo se puede optimizar su ejecuci√≥n (por ejemplo, para paralelismo).\n",
        "    \n",
        "\n",
        "**En resumen, LCEL te proporciona las herramientas y la sintaxis para orquestar tus flujos de trabajo de manera flexible y poderosa. En lugar de imponer tipos de cadenas predefinidos, te da la libertad de construir las cadenas que mejor se adapten a tus necesidades, implementando patrones secuenciales, condicionales o paralelos seg√∫n sea necesario.**\n",
        "\n",
        "Y todo esta abstraci√≥n se basa en los **Runnables**"
      ],
      "metadata": {
        "id": "m-9xlveZwh7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Definici√≥n del Concepto Runnable**\n",
        "---\n",
        "\n",
        "En esencia, un **Runnable** en LangChain es una **unidad b√°sica y ejecutable** dentro de LCEL. Piensa en ellos como los bloques de construcci√≥n fundamentales con los que puedes crear flujos de trabajo complejos. Cada Runnable toma una entrada, realiza alguna operaci√≥n sobre ella y produce una salida.\n",
        "\n",
        "**Analog√≠a:** Imagina una cadena de montaje en una f√°brica. Cada estaci√≥n de trabajo (o m√°quina) realiza una tarea espec√≠fica sobre la pieza que llega y la pasa a la siguiente estaci√≥n. En LangChain, cada Runnable ser√≠a una de esas estaciones de trabajo.  \n",
        "\n",
        "\n",
        "## **Puntos Clave:**\n",
        "\n",
        "- **Ejecutables:** Los Runnables pueden ser \"ejecutados\" para procesar datos.  \n",
        "\n",
        "- **Componibles:** La verdadera potencia de los Runnables reside en su capacidad\n",
        "-**Abstractos:**  La interfaz `Runnable` define un contrato com√∫n, pero la implementaci√≥n espec√≠fica de cada Runnable puede variar enormemente (desde llamar a un modelo de lenguaje hasta realizar una simple transformaci√≥n de datos).\n",
        "\n",
        "## **Interfaz Est√°ndar de Runnables**\n",
        "\n",
        "Una de las grandes ventajas de los Runnables es que comparten una **interfaz est√°ndar**. Esto significa que, una vez que entiendes c√≥mo interactuar con un Runnable, puedes aplicar ese conocimiento a cualquier otro Runnable, ¬°independientemente de lo que haga internamente!\n",
        "\n",
        "La interfaz principal se basa en una serie de m√©todos que permiten ejecutar el Runnable de diferentes maneras.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iHl6jV0Rw6PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together y Anthropic\n",
        "\n",
        "- Instalamos la librer√≠a LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases espec√≠ficas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dej√°ndolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n",
        "El uso de las API de OpenAI y Anthropic es de pago. El resto son gratuitas y para usarlas basta con registrase y generar una API Key.  \n",
        "\n",
        "En el primer cuaderno encontraras los enlaces a estos servicios y este codigo explicado"
      ],
      "metadata": {
        "id": "YILqX_mu3nPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librer√≠as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "wg6dVdgG2yb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. M√©todos Principales**\n",
        "---\n",
        "\n",
        "Vamos a explorar los m√©todos m√°s importantes que encontrar√°s en la interfaz de un Runnable:\n",
        "\n"
      ],
      "metadata": {
        "id": "mhbTXS1c2XKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 `.invoke()`**\n",
        "\n",
        "El m√©todo `.invoke()` es la forma m√°s b√°sica de ejecutar un Runnable. Toma una **√∫nica entrada** y devuelve la **salida** del Runnable despu√©s de procesar esa entrada."
      ],
      "metadata": {
        "id": "6XKWcqmM2mH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos `RunnableLambda`, una forma sencilla de crear un Runnable a partir de una funci√≥n Python (lo veremos en detalle m√°s adelante).\n",
        " - Definimos un Runnable que toma una cadena (`x`) y devuelve su versi√≥n en may√∫sculas.  \n",
        " - Llamamos al m√©todo `.invoke()` pas√°ndole la cadena \"hola mundo\" como entrada.\n",
        " - La variable `resultado` contiene la salida del Runnable, que en este caso es \"HOLA MUNDO\".  \n",
        "  \n",
        "<br>\n",
        "\n",
        "\n",
        " *¬°Int√©ntalo t√∫!* Modifica el c√≥digo anterior para que el Runnable convierta el texto a min√∫sculas."
      ],
      "metadata": {
        "id": "mGbObOOk60-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Creamos un Runnable sencillo que convierte el texto a may√∫sculas\n",
        "mi_runnable = RunnableLambda(lambda x: x.upper())\n",
        "\n",
        "# Ejecutamos el Runnable con el m√©todo invoke()\n",
        "resultado = mi_runnable.invoke(\"hola, mundo\")\n",
        "\n",
        "print(f\"Resultado de invoke(): {resultado}\")"
      ],
      "metadata": {
        "id": "_-bLoFVz2u-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 `.batch()`**\n",
        "\n",
        "El m√©todo `.batch()` te permite ejecutar un Runnable con **m√∫ltiples entradas al mismo tiempo**. Es muy √∫til cuando tienes una lista de datos que quieres procesar de manera eficiente. Devuelve una **lista de salidas**, donde cada salida corresponde a la entrada en la misma posici√≥n.\n",
        "\n",
        "\n",
        "*   Utilizamos el mismo Runnable de antes.\n",
        "*   Creamos una lista de cadenas llamada `entradas`.\n",
        "*   Llamamos a `.batch()` pas√°ndole la lista de entradas.\n",
        "*   La variable `resultados` contiene una lista con las versiones en may√∫sculas de cada palabra en la lista `entradas`.\n"
      ],
      "metadata": {
        "id": "hZIX8zsq9Vh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# El mismo Runnable que convierte a may√∫sculas\n",
        "mi_runnable = RunnableLambda(lambda x: x.upper())\n",
        "\n",
        "# Una lista de entradas\n",
        "entradas = [\"manzana\", \"banana\", \"cereza\"]\n",
        "\n",
        "# Ejecutamos el Runnable con el m√©todo batch()\n",
        "resultados = mi_runnable.batch(entradas)\n",
        "\n",
        "print(f\"Resultados de batch(): {resultados}\")"
      ],
      "metadata": {
        "id": "AwpYqt8z9mR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìùDesaf√≠o:**\n",
        "Crea un Runnable que calcule la longitud de una cadena y luego usa `.batch()` para obtener la longitud de las palabras en la lista `entradas`."
      ],
      "metadata": {
        "id": "QKPnNV-E-pmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe tu soluci√≥n aqu√≠"
      ],
      "metadata": {
        "id": "eWB7YNZB_YZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üëÄ Soluci√≥n:"
      ],
      "metadata": {
        "id": "r2QQgviBAoNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Crea un Runnable que calcule la longitud de una cadena\n",
        "longitud_runnable = RunnableLambda(lambda x: len(x))\n",
        "\n",
        "entradas = [\"manzana\", \"banana\", \"cereza\"]\n",
        "\n",
        "# Usa .batch() para obtener las longitudes\n",
        "longitudes = longitud_runnable.batch(entradas)\n",
        "\n",
        "print(f\"Longitudes de batch(): {longitudes}\")"
      ],
      "metadata": {
        "id": "rSFx9e7CAPjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3 `.stream()`**\n",
        "\n",
        "El m√©todo `.stream()` es diferente a los anteriores. En lugar de devolver toda la salida de una vez, `.stream()` devuelve un **generador**. Esto significa que puedes ir recibiendo las salidas del Runnable **a medida que se van generando**, lo que es especialmente √∫til para procesar grandes cantidades de datos o para aplicaciones que necesitan mostrar resultados en tiempo real.\n",
        "\n",
        "- Creamos un Runnable que a√±ade un saludo a la entrada.  \n",
        "\n",
        "- Llamamos a `.stream()` con una lista de nombres.  \n",
        "\n",
        "- El resultado es un generador. Usamos un bucle `for` para iterar sobre el generador y obtener cada salida individualmente.\n",
        "\n",
        "**Puntos Importantes sobre `.stream()`:**\n",
        "\n",
        "- **Eficiencia de Memoria:** No carga todas las salidas en memoria a la vez.  \n",
        "\n",
        "- **Procesamiento en Tiempo Real:** Permite mostrar resultados a medida que se generan.  \n",
        "\n",
        "- **Iteraci√≥n:** Debes iterar sobre el generador para obtener las salidas.\n"
      ],
      "metadata": {
        "id": "V6331PeMChkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Un Runnable que a√±ade un saludo a cada palabra\n",
        "saludo_runnable = RunnableLambda(lambda x: f\"¬°Hola, {x}!\")\n",
        "\n",
        "entradas = [\"Ana\", \"Beto\", \"Carla\"]\n",
        "\n",
        "# Obtenemos un generador usando stream()\n",
        "flujo = saludo_runnable.stream(entradas)\n",
        "\n",
        "print(\"Resultados de stream():\")\n",
        "for saludo in flujo:\n",
        "    print(saludo)"
      ],
      "metadata": {
        "id": "SdRW8rC2C6EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la interacci√≥n con modelos de lenguaje como GPT, el streaming ofrece una alternativa a la espera de la respuesta completa. En lugar de una entrega √∫nica, el modelo emite la respuesta progresivamente, token por token (fragmentos de palabras o caracteres). Esta transmisi√≥n gradual mejora la experiencia del usuario, quien percibe una respuesta m√°s r√°pida y natural al ver el texto aparecer en tiempo real. Adem√°s, el streaming permite iniciar el procesamiento de la respuesta en tiempo real, sin esperar a que el modelo termine de generar toda la salida. En esencia, el streaming transforma la interacci√≥n con el LLM de una espera prolongada a una recepci√≥n continua y din√°mica."
      ],
      "metadata": {
        "id": "PvRHtVQjEddr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_groq = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# Obtenemos un \"generador\"\n",
        "respuesta = llm_groq.stream(\"Explica los verbos modales en ingl√©s\")\n",
        "\n",
        "respuesta"
      ],
      "metadata": {
        "id": "oTjtpt5VEODR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iteramos el generador para mostrar la respuesta\n",
        "for token in respuesta:\n",
        "    print(token.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "mm-Oe564J5v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4 `ainvoke()`, `abatch()`, `astream()`  **\n",
        "\n",
        "Hasta ahora, hemos visto m√©todos s√≠ncronos, lo que significa que la ejecuci√≥n del programa se pausa hasta que el Runnable termina de procesar la entrada.\n",
        "\n",
        "LangChain tambi√©n proporciona versiones **as√≠ncronas** de estos m√©todos, que permiten realizar otras tareas mientras el Runnable est√° en ejecuci√≥n, mejorando la eficiencia en aplicaciones concurrentes.\n",
        "\n",
        "- El m√©todo `.ainvoke()` es la versi√≥n as√≠ncrona de `.invoke()`.   \n",
        "- El m√©todo `.abatch()` es la versi√≥n as√≠ncrona de `.batch()`.   \n",
        "- El m√©todo `.astream()` es la versi√≥n as√≠ncrona de `.stream()`.\n",
        "\n",
        "Para usarlos, necesitas utilizar la sintaxis `async/await` de Python.\n",
        "\n",
        "Los trataremos m√°s adelante.\n"
      ],
      "metadata": {
        "id": "LLFGAKcjLOCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Referencias**\n",
        "---\n",
        "1. https://python.langchain.com/docs/concepts/runnables/\n",
        "\n",
        "2. https://python.langchain.com/api_reference/core/runnables.html\n",
        "3. https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
        "\n",
        "4. https://dzone.com/articles/guide-to-langchain-runnable-architecture\n",
        "\n",
        "5. https://medium.com/@danushidk507/runnables-in-langchain-e6bfb7b9c0ca\n",
        "\n",
        "6. https://www.youtube.com/watch?v=8aUYzb1aYDU\n",
        "\n",
        "7. https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac\n",
        "\n",
        "8. https://medium.com/@ulrichw/list/langchain-lcel-85af4f4ff883\n",
        "\n",
        "9. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "10. https://www.pinecone.io/learn/series/langchain/langchain-expression-language/"
      ],
      "metadata": {
        "id": "rvw7L78dMQkt"
      }
    }
  ]
}