{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDow+aB0+8R/zfM7/L96MU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/LCEL%2C%20Runnables%20y%20cadenas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. LCEL, LangChain Expression Language**\n",
        "---\n",
        "\n",
        "LCEL se introdujo en Langchain **a mediados de 2023**, específicamente con la **versión 0.0.142**, lanzada el **19 de julio de 2023**\n",
        "\n",
        "La introducción de **LCEL** en Langchain fue una respuesta a la necesidad de una forma más potente, flexible y fácil de usar para construir aplicaciones de lenguaje complejas. Proporcionó una sintaxis declarativa, mejoró la legibilidad, facilitó la depuración y habilitó funcionalidades avanzadas como el streaming y la ejecución asíncrona, consolidándose como una pieza fundamental del ecosistema de Langchain.\n",
        "\n",
        "**LCEL se basa en la Composición, no en Tipos Predefinidos:**\n",
        "\n",
        "-   **Operador Pipe (|):** La piedra angular de LCEL es el operador pipe. Este operador te permite encadenar componentes de forma secuencial, enviando la salida de un componente como entrada al siguiente. Esto es inherentemente secuencial, pero no se define como un tipo de cadena \"secuencial\".\n",
        "    \n",
        "-   **Primitivas Runnable:** LCEL se basa en la interfaz Runnable. Cualquier objeto que implemente esta interfaz puede ser parte de una cadena LCEL. Esto incluye modelos de lenguaje, prompts, parsers, retrievers, etc.\n",
        "    \n",
        "-   **Flexibilidad Total:** La clave es que puedes combinar estas primitivas Runnable de cualquier manera que tenga sentido para tu aplicación. No estás limitado a estructuras predefinidas.\n",
        "\n",
        "**Ventajas de este enfoque:**\n",
        "\n",
        "-   **Mayor Flexibilidad:** No estás limitado por las estructuras predefinidas. Puedes crear flujos de trabajo exactamente como los necesitas.\n",
        "    \n",
        "-   **Reutilización de Componentes:** Los componentes individuales pueden ser reutilizados en diferentes cadenas con diferentes flujos de trabajo.\n",
        "    \n",
        "-   **Claridad y Composición:** El uso del operador pipe hace que la lógica de la cadena sea más clara y fácil de entender.\n",
        "    \n",
        "-   **Optimización:** La forma en que construyes la cadena influye en cómo se puede optimizar su ejecución (por ejemplo, para paralelismo).\n",
        "    \n",
        "\n",
        "**En resumen, LCEL te proporciona las herramientas y la sintaxis para orquestar tus flujos de trabajo de manera flexible y poderosa. En lugar de imponer tipos de cadenas predefinidos, te da la libertad de construir las cadenas que mejor se adapten a tus necesidades, implementando patrones secuenciales, condicionales o paralelos según sea necesario.**\n",
        "\n",
        "Y todo esta abstración se basa en los **Runnables**"
      ],
      "metadata": {
        "id": "m-9xlveZwh7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Definición del Concepto Runnable**\n",
        "---\n",
        "\n",
        "En esencia, un **Runnable** en LangChain es una **unidad básica y ejecutable** dentro de LCEL. Piensa en ellos como los bloques de construcción fundamentales con los que puedes crear flujos de trabajo complejos. Cada Runnable toma una entrada, realiza alguna operación sobre ella y produce una salida.\n",
        "\n",
        "**Analogía:** Imagina una cadena de montaje en una fábrica. Cada estación de trabajo (o máquina) realiza una tarea específica sobre la pieza que llega y la pasa a la siguiente estación. En LangChain, cada Runnable sería una de esas estaciones de trabajo.  \n",
        "\n",
        "\n",
        "## **Puntos Clave:**\n",
        "\n",
        "- **Ejecutables:** Los Runnables pueden ser \"ejecutados\" para procesar datos.  \n",
        "\n",
        "- **Componibles:** La verdadera potencia de los Runnables reside en su capacidad\n",
        "-**Abstractos:**  La interfaz `Runnable` define un contrato común, pero la implementación específica de cada Runnable puede variar enormemente (desde llamar a un modelo de lenguaje hasta realizar una simple transformación de datos).\n",
        "\n",
        "## **Interfaz Estándar de Runnables**\n",
        "\n",
        "Una de las grandes ventajas de los Runnables es que comparten una **interfaz estándar**. Esto significa que, una vez que entiendes cómo interactuar con un Runnable, puedes aplicar ese conocimiento a cualquier otro Runnable, ¡independientemente de lo que haga internamente!\n",
        "\n",
        "La interfaz principal se basa en una serie de métodos que permiten ejecutar el Runnable de diferentes maneras.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iHl6jV0Rw6PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together y Anthropic\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n",
        "El uso de las API de OpenAI y Anthropic es de pago. El resto son gratuitas y para usarlas basta con registrase y generar una API Key.  \n",
        "\n",
        "En el primer cuaderno encontraras los enlaces a estos servicios y este codigo explicado"
      ],
      "metadata": {
        "id": "YILqX_mu3nPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "wg6dVdgG2yb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Métodos Principales**\n",
        "---\n",
        "\n",
        "Vamos a explorar los métodos más importantes que encontrarás en la interfaz de un Runnable:\n",
        "\n"
      ],
      "metadata": {
        "id": "mhbTXS1c2XKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 `.invoke()`**\n",
        "\n",
        "El método `.invoke()` es la forma más básica de ejecutar un Runnable. Toma una **única entrada** y devuelve la **salida** del Runnable después de procesar esa entrada."
      ],
      "metadata": {
        "id": "6XKWcqmM2mH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos `RunnableLambda`, una forma sencilla de crear un Runnable a partir de una función Python (lo veremos en detalle más adelante).\n",
        " - Definimos un Runnable que toma una cadena (`x`) y devuelve su versión en mayúsculas.  \n",
        " - Llamamos al método `.invoke()` pasándole la cadena \"hola mundo\" como entrada.\n",
        " - La variable `resultado` contiene la salida del Runnable, que en este caso es \"HOLA MUNDO\".  \n",
        "  \n",
        "<br>\n",
        "\n",
        "\n",
        " *¡Inténtalo tú!* Modifica el código anterior para que el Runnable convierta el texto a minúsculas."
      ],
      "metadata": {
        "id": "mGbObOOk60-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Creamos un Runnable sencillo que convierte el texto a mayúsculas\n",
        "mi_runnable = RunnableLambda(lambda x: x.upper())\n",
        "\n",
        "# Ejecutamos el Runnable con el método invoke()\n",
        "resultado = mi_runnable.invoke(\"hola, mundo\")\n",
        "\n",
        "print(f\"Resultado de invoke(): {resultado}\")"
      ],
      "metadata": {
        "id": "_-bLoFVz2u-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 `.batch()`**\n",
        "\n",
        "El método `.batch()` te permite ejecutar un Runnable con **múltiples entradas al mismo tiempo**. Es muy útil cuando tienes una lista de datos que quieres procesar de manera eficiente. Devuelve una **lista de salidas**, donde cada salida corresponde a la entrada en la misma posición.\n",
        "\n",
        "\n",
        "*   Utilizamos el mismo Runnable de antes.\n",
        "*   Creamos una lista de cadenas llamada `entradas`.\n",
        "*   Llamamos a `.batch()` pasándole la lista de entradas.\n",
        "*   La variable `resultados` contiene una lista con las versiones en mayúsculas de cada palabra en la lista `entradas`.\n"
      ],
      "metadata": {
        "id": "hZIX8zsq9Vh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# El mismo Runnable que convierte a mayúsculas\n",
        "mi_runnable = RunnableLambda(lambda x: x.upper())\n",
        "\n",
        "# Una lista de entradas\n",
        "entradas = [\"manzana\", \"banana\", \"cereza\"]\n",
        "\n",
        "# Ejecutamos el Runnable con el método batch()\n",
        "resultados = mi_runnable.batch(entradas)\n",
        "\n",
        "print(f\"Resultados de batch(): {resultados}\")"
      ],
      "metadata": {
        "id": "AwpYqt8z9mR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **📝Desafío:**\n",
        "Crea un Runnable que calcule la longitud de una cadena y luego usa `.batch()` para obtener la longitud de las palabras en la lista `entradas`."
      ],
      "metadata": {
        "id": "QKPnNV-E-pmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe tu solución aquí"
      ],
      "metadata": {
        "id": "eWB7YNZB_YZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 👀 Solución:"
      ],
      "metadata": {
        "id": "r2QQgviBAoNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Crea un Runnable que calcule la longitud de una cadena\n",
        "longitud_runnable = RunnableLambda(lambda x: len(x))\n",
        "\n",
        "entradas = [\"manzana\", \"banana\", \"cereza\"]\n",
        "\n",
        "# Usa .batch() para obtener las longitudes\n",
        "longitudes = longitud_runnable.batch(entradas)\n",
        "\n",
        "print(f\"Longitudes de batch(): {longitudes}\")"
      ],
      "metadata": {
        "id": "rSFx9e7CAPjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3 `.stream()`**\n",
        "\n",
        "El método `.stream()` es diferente a los anteriores. En lugar de devolver toda la salida de una vez, `.stream()` devuelve un **generador**. Esto significa que puedes ir recibiendo las salidas del Runnable **a medida que se van generando**, lo que es especialmente útil para procesar grandes cantidades de datos o para aplicaciones que necesitan mostrar resultados en tiempo real.\n",
        "\n",
        "- Creamos un Runnable que añade un saludo a la entrada.  \n",
        "\n",
        "- Llamamos a `.stream()` con una lista de nombres.  \n",
        "\n",
        "- El resultado es un generador. Usamos un bucle `for` para iterar sobre el generador y obtener cada salida individualmente.\n",
        "\n",
        "**Puntos Importantes sobre `.stream()`:**\n",
        "\n",
        "- **Eficiencia de Memoria:** No carga todas las salidas en memoria a la vez.  \n",
        "\n",
        "- **Procesamiento en Tiempo Real:** Permite mostrar resultados a medida que se generan.  \n",
        "\n",
        "- **Iteración:** Debes iterar sobre el generador para obtener las salidas.\n"
      ],
      "metadata": {
        "id": "V6331PeMChkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Un Runnable que añade un saludo a cada palabra\n",
        "saludo_runnable = RunnableLambda(lambda x: f\"¡Hola, {x}!\")\n",
        "\n",
        "entradas = [\"Ana\", \"Beto\", \"Carla\"]\n",
        "\n",
        "# Obtenemos un generador usando stream()\n",
        "flujo = saludo_runnable.stream(entradas)\n",
        "\n",
        "print(\"Resultados de stream():\")\n",
        "for saludo in flujo:\n",
        "    print(saludo)"
      ],
      "metadata": {
        "id": "SdRW8rC2C6EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la interacción con modelos de lenguaje como GPT, el streaming ofrece una alternativa a la espera de la respuesta completa. En lugar de una entrega única, el modelo emite la respuesta progresivamente, token por token (fragmentos de palabras o caracteres). Esta transmisión gradual mejora la experiencia del usuario, quien percibe una respuesta más rápida y natural al ver el texto aparecer en tiempo real. Además, el streaming permite iniciar el procesamiento de la respuesta en tiempo real, sin esperar a que el modelo termine de generar toda la salida. En esencia, el streaming transforma la interacción con el LLM de una espera prolongada a una recepción continua y dinámica."
      ],
      "metadata": {
        "id": "PvRHtVQjEddr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_groq = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# Obtenemos un \"generador\"\n",
        "respuesta = llm_groq.stream(\"Explica los verbos modales en inglés\")\n",
        "\n",
        "respuesta"
      ],
      "metadata": {
        "id": "oTjtpt5VEODR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iteramos el generador para mostrar la respuesta\n",
        "for token in respuesta:\n",
        "    print(token.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "mm-Oe564J5v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4 `ainvoke()`, `abatch()`, `astream()`  **\n",
        "\n",
        "Hasta ahora, hemos visto métodos síncronos, lo que significa que la ejecución del programa se pausa hasta que el Runnable termina de procesar la entrada.\n",
        "\n",
        "LangChain también proporciona versiones **asíncronas** de estos métodos, que permiten realizar otras tareas mientras el Runnable está en ejecución, mejorando la eficiencia en aplicaciones concurrentes.\n",
        "\n",
        "- El método `.ainvoke()` es la versión asíncrona de `.invoke()`.   \n",
        "- El método `.abatch()` es la versión asíncrona de `.batch()`.   \n",
        "- El método `.astream()` es la versión asíncrona de `.stream()`.\n",
        "\n",
        "Para usarlos, necesitas utilizar la sintaxis `async/await` de Python.\n",
        "\n",
        "Los trataremos más adelante.\n"
      ],
      "metadata": {
        "id": "LLFGAKcjLOCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Referencias**\n",
        "---\n",
        "1. https://python.langchain.com/docs/concepts/runnables/\n",
        "\n",
        "2. https://python.langchain.com/api_reference/core/runnables.html\n",
        "3. https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
        "\n",
        "4. https://dzone.com/articles/guide-to-langchain-runnable-architecture\n",
        "\n",
        "5. https://medium.com/@danushidk507/runnables-in-langchain-e6bfb7b9c0ca\n",
        "\n",
        "6. https://www.youtube.com/watch?v=8aUYzb1aYDU\n",
        "\n",
        "7. https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac\n",
        "\n",
        "8. https://medium.com/@ulrichw/list/langchain-lcel-85af4f4ff883\n",
        "\n",
        "9. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "10. https://www.pinecone.io/learn/series/langchain/langchain-expression-language/"
      ],
      "metadata": {
        "id": "rvw7L78dMQkt"
      }
    }
  ]
}