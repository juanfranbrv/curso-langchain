{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTLJg5PGE4TrR7Dm+Ib6Cj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/PydanticOutputParser_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together y Anthropic desde los secretos de Colab para fines de autenticación.\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs.\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar. El uso de las API de OpenAI, Anthropic y DeepSeek es de pago. El resto son gratuitas y para usarlas basta con registrarse y generar una API Key."
      ],
      "metadata": {
        "id": "Bmpb4BEmdz3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "DEEPSEEK_API_KEY=userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "# Importamos para formatear mejor la salida\n",
        "from IPython.display import Markdown, display\n",
        "from pprint import pprint\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Con DeepSeek con su API propia\n",
        "# model=deepseek-reasoner para R1; model=deepseek-chat para V3\n",
        "\n",
        "# translator_llm = ChatOpenAI(model=\"deepseek-reasoner\" ,api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0.3)\n",
        "# accuracy_llm = ChatOpenAI(model=\"deepseek-reasoner\" ,api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0.1)\n",
        "# fluency_llm = ChatOpenAI(model=\"deepseek-reasoner\" ,api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0.7)\n",
        "# fixer_llm = ChatOpenAI(model=\"deepseek-reasoner\" ,api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0)\n",
        "\n",
        "# Con OpenAI\n",
        "\n",
        "# translator_llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=0.3)\n",
        "# accuracy_llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=0.1)\n",
        "# fluency_llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "# fixer_llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Con Deepseek R1 en Groq\n",
        "\n",
        "translator_llm =  ChatGroq(model=\"deepseek-r1-distill-llama-70b\", api_key=GROQ_API_KEY, temperature=0.3)\n",
        "accuracy_llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\", api_key=GROQ_API_KEY, temperature=0.1)\n",
        "fluency_llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\", api_key=GROQ_API_KEY, temperature=0.7)\n",
        "fixer_llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\", api_key=GROQ_API_KEY, temperature=0)"
      ],
      "metadata": {
        "id": "AnrE-u0P2yD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser, RetryWithErrorOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Modelo Pydantic para respuesta simple\n",
        "class TranslationOutput(BaseModel):\n",
        "    translation: str = Field(description=\"The professional translated text\")\n",
        "\n",
        "# Modelo Pydantic para respuesta de un modelo corrector\n",
        "class RespuestaAgente (BaseModel):\n",
        "    translation: str = Field(description=\"Texto con la traduccion generada por el agente.\")\n",
        "    suggestions: List[str] = Field(description=\"Lista de suggestions encontradas en el texto.\")\n",
        "\n",
        "\n",
        "parser_translation = PydanticOutputParser(pydantic_object=TranslationOutput)\n",
        "parser_agent = PydanticOutputParser(pydantic_object=RespuestaAgente)\n",
        "\n",
        "# Parser que intenta corregir errores, podria implementarse tambien RetryWithErrorOutputParser.from_llm\n",
        "# o try-except con retries o algun parser personalizado\n",
        "# el llm para esto podria er alguno concretocon temperatura=0\n",
        "fixing_parser_tranlation = OutputFixingParser.from_llm(parser=parser_translation, llm=fixer_llm)\n",
        "fixing_parser_agent = OutputFixingParser.from_llm(parser=parser_agent, llm=fixer_llm)\n",
        "\n"
      ],
      "metadata": {
        "id": "LNbNy39c1e8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Construir el prompt template con variables explícitas\n",
        "translator_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are a senior legal translator specializing in Intellectual Property documents. \"\n",
        "        \"Translate from {source_language} to {target_language} with:\\n\"\n",
        "        \"- Perfect accuracy\\n\"\n",
        "        \"- Legal terminology consistency\\n\"\n",
        "        \"- Publication-ready quality\\n\\n\"\n",
        "        \"**Strict requirements:**\\n\"\n",
        "        # \"1. Output MUST be valid JSON with double quotes\\n\"\n",
        "        \"2. Follow EXACTLY this structure:\\n{format_instructions}\\n\"\n",
        "        \"3. No additional text or explanations\"\n",
        "    )),\n",
        "    (\"human\", (\n",
        "        \"Translate this legal text with absolute precision:\\n\"\n",
        "        \"{original_text}\\n\\n\"\n",
        "        # \"Output ONLY the raw JSON without formatting:\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "\n",
        "# Definir variables de entrada explícitamente\n",
        "\"\"\"\"\n",
        "El uso de .with_options(input_variables=...) es una decisión conservadora para:\n",
        "1-Explicitación clara: Dejar documentado qué variables se esperan.\n",
        "2-Prevención de errores: En casos donde LangChain no detecte automáticamente todas las variables del template.\n",
        "3-Control de versiones: Garantizar compatibilidad si LangChain cambia su comportamiento de inferencia.\n",
        "\"\"\"\n",
        "# translator_prompt_template = translator_prompt_template.with_options(\n",
        "#     input_variables=[\n",
        "#         \"source_language\",\n",
        "#         \"target_language\",\n",
        "#         \"original_text\",\n",
        "#         \"format_instructions\"\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# Luego, al momento de formatear el prompt para su uso, se pasan todas las variables necesarias,\n",
        "# incluyendo las instrucciones de formato obtenidas de 'parser_translation'\n",
        "# formatted_prompt = translator_prompt_template.format(\n",
        "#     source_language=\"English\",          # o el idioma de origen que necesites\n",
        "#     target_language=\"Spanish\",           # o el idioma destino que necesites\n",
        "#     original_text=\"Your legal text here\",  # el texto legal a traducir\n",
        "#     format_instructions=parser_translation.get_format_instructions()\n",
        "# )\n",
        "\n",
        "# Ahora 'formatted_prompt' contiene el prompt completo con todas las variables sustituidas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# accuracy_reviewer_prompt_template = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", (\n",
        "#         \"You are an Accuracy Reviewer specializing in {source_language} to {target_language} translations.\\n\"\n",
        "#         \"**Strict instructions:**\\n\"\n",
        "#         \"1. Follow EXACTLY this structure:\\n\"\n",
        "#         \"2. Return a JSON object with EXACTLY these fields:\\n\"\n",
        "#         \"   - 'translation': corrected translation text\\n\"\n",
        "#         \"   - 'suggestions': list of suggestions using format '- ERROR: [issue] -> SUGGESTION: [fix]'\\n\\n\"\n",
        "#         \"3. Follow these rules:\\n\"\n",
        "#         \"- Correct only accuracy errors (mistranslations, omissions, untranslated text)\\n\"\n",
        "#         \"- Maintain original style and format\\n\"\n",
        "#         \"- If no changes, return original text and empty list\\n\"\n",
        "#         \"- Use exactly the specified JSON structure\"\n",
        "#     )),\n",
        "#     (\"human\", (\n",
        "#         \"Original Text ({source_language}):\\n\"\n",
        "#         \"{original_text}\\n\\n\"\n",
        "#         \"Current Translation ({target_language}):\\n\"\n",
        "#         \"{translation}\\n\\n\"\n",
        "#         \"Provide corrections in the required JSON format.\"\n",
        "#     ))\n",
        "# ])\n",
        "\n",
        "# # Configuración adicional para el parser\n",
        "# accuracy_reviewer_prompt_template = accuracy_reviewer_prompt_template.partial(\n",
        "#     format_instructions=parser_agent.get_format_instructions()\n",
        "# )\n",
        "\n",
        "accuracy_reviewer_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        (\n",
        "            \"You are an Accuracy Reviewer specializing in {source_language} to {target_language} translations.\\n\"\n",
        "            \"**Strict instructions:**\\n\"\n",
        "            \"1. Follow EXACTLY this structure:\\n\"\n",
        "            \"2. Return a JSON object with EXACTLY these fields:\\n\"\n",
        "            \"   - 'translation': corrected translation text\\n\"\n",
        "            \"   - 'suggestions': list of suggestions using format '- ERROR: [issue] -> SUGGESTION: [fix]'\\n\\n\"\n",
        "            \"3. Follow these rules:\\n\"\n",
        "            \"- Correct only accuracy errors (mistranslations, omissions, untranslated text)\\n\"\n",
        "            \"- Maintain original style and format\\n\"\n",
        "            \"- If no changes, return original text and empty list\\n\"\n",
        "            \"- Use exactly the specified JSON structure\\n\\n\"\n",
        "            \"{format_instructions}\"\n",
        "        )\n",
        "    ),\n",
        "    (\n",
        "        \"human\",\n",
        "        (\n",
        "            \"Original Text ({source_language}):\\n\"\n",
        "            \"{original_text}\\n\\n\"\n",
        "            \"Current Translation ({target_language}):\\n\"\n",
        "            \"{translation}\\n\\n\"\n",
        "            \"Provide corrections in the required JSON format.\"\n",
        "        )\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "# Prompt del Revisor de Fluidez\n",
        "# También pedimos que aplique sus mejoras directamente y devuelva el texto final que propone.\n",
        "fluency_reviewer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are a Fluency Reviewer specializing in {source_language} to {target_language} translations.\\n\\n\"\n",
        "        \"**Strict Requirements:**\\n\"\n",
        "        \"1. Follow EXACTLY this structure:\\n\"\n",
        "            \"Return a JSON object with EXACTLY these fields:\\n\"\n",
        "            \"   - 'translation': corrected translation text\\n\"\n",
        "            \"   - 'suggestions': list of suggestions using format '- ERROR: [issue] -> SUGGESTION: [fix]'\\n\\n\"\n",
        "        \"2. Focus only on:\\n\"\n",
        "        \"- Grammar/spelling errors\\n\"\n",
        "        \"- Natural flow in {target_language}\\n\"\n",
        "        \"- Cultural adaptation\\n\"\n",
        "        \"- If no changes, return original text and empty list\\n\"\n",
        "        \"- Use exactly the specified JSON structure\\n\\n\"\n",
        "        \"{format_instructions}\"\n",
        "    )),\n",
        "    (\"human\", (\n",
        "        \"Review this translation for fluency:\\n\"\n",
        "\n",
        "            \"Original Text ({source_language}):\\n\"\n",
        "            \"{original_text}\\n\\n\"\n",
        "            \"Current Translation ({target_language}):\\n\"\n",
        "            \"{translation}\\n\\n\"\n",
        "            \"Provide corrections in the required JSON format.\"\n",
        "\n",
        "        \"Provide corrections in required JSON format:\"\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "SBuTaU_F2Uxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo recibira un texto original, una traduccion y debe realizar una correccion de la traduccion.\n",
        "Como resultado debe devolver una nueva traduccion y una lista con los problema encontrados en cierto formato"
      ],
      "metadata": {
        "id": "RXJ40o0J3lrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Solicitar al usuario que suba un fichero\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Tomar el nombre del primer fichero subido\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Leer el contenido del fichero\n",
        "with open(file_name, 'r', encoding='utf-8') as f:\n",
        "    original_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "DtwpYJ512Rru",
        "outputId": "a2de785b-1dc4-4846-ef92-efd7e0387272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-51bad9b8-ccc1-4219-8169-90b383007ab1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-51bad9b8-ccc1-4219-8169-90b383007ab1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving texttexttext.txt to texttexttext (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator_chain = translator_prompt_template | translator_llm | fixing_parser_tranlation\n",
        "accuracy_reviewer_chain = accuracy_reviewer_prompt_template | accuracy_llm | fixing_parser_agent\n",
        "fluency_reviewer_chain = fluency_reviewer_prompt | fluency_llm | fixing_parser_agent"
      ],
      "metadata": {
        "id": "KcgBaYt1PJ3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "rEzcmdGHsB5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutando multiples cadenas secuencialmente"
      ],
      "metadata": {
        "id": "rdwV0vFvQuhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_translator = translator_chain.invoke({\n",
        "        \"source_language\": \"English\",\n",
        "        \"target_language\": \"Spanish\",\n",
        "        \"original_text\": original_text,\n",
        "        \"format_instructions\": parser_translation.get_format_instructions()\n",
        "    })\n",
        "\n",
        "# print(type(result_translator))\n",
        "# print(result_translator)\n",
        "\n",
        "display(Markdown(\"###🌍 Resultado del traductor\"))\n",
        "display(Markdown(result_translator.translation))\n",
        "display(Markdown(\"---\"))\n",
        "\n",
        "\n",
        "result_accuracy_reviewer = accuracy_reviewer_chain.invoke({\n",
        "        \"source_language\": \"English\",\n",
        "        \"target_language\": \"Spanish\",\n",
        "        \"original_text\": original_text,\n",
        "        \"translation\": result_translator.translation,\n",
        "        \"format_instructions\": parser_agent.get_format_instructions()\n",
        "    })\n",
        "\n",
        "# print(type(result_accuracy_reviewer))\n",
        "# print(result_accuracy_reviewer)\n",
        "\n",
        "display(Markdown(\"###📝 Resultados del Accuracy reviewer\"))\n",
        "for suggestion in result_accuracy_reviewer.suggestions:\n",
        "    print(suggestion)\n",
        "display(Markdown(\"*Traducción editada:*\"))\n",
        "display(Markdown(result_accuracy_reviewer.translation))\n",
        "display(Markdown(\"---\"))\n",
        "\n",
        "result_fluency_reviewer = fluency_reviewer_chain.invoke({\n",
        "        \"source_language\": \"English\",\n",
        "        \"target_language\": \"Spanish\",\n",
        "        \"original_text\": original_text,\n",
        "        \"translation\": result_accuracy_reviewer.translation,\n",
        "        \"format_instructions\": parser_agent.get_format_instructions()\n",
        "    })\n",
        "\n",
        "# print(type(result_fluency_reviewer))\n",
        "# print(result_fluency_reviewer)\n",
        "\n",
        "display(Markdown(\"###🎯 Resultados del Fluency_reviewer\"))\n",
        "for suggestion in result_fluency_reviewer.suggestions:\n",
        "    print(suggestion)\n",
        "display(Markdown(\"*Traducción editada:*\"))\n",
        "display(Markdown(result_fluency_reviewer.translation))\n",
        "display(Markdown(\"---\"))\n",
        "display(Markdown(\"✅ Proceso finalizado con éxito\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "28bc1DQlQz5z",
        "outputId": "b8584ed9-bf78-48bd-9dca-9b468b43745e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "###🌍 Resultado del traductor"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "La búsqueda filosófica de la sabiduría implica formular preguntas generales y fundamentales. Con frecuencia, no conduce a respuestas directas, pero puede ayudar a una persona a comprender mejor el tema, examinar su vida, disipar la confusión y superar los prejuicios y las ideas autoengañadas asociadas con el sentido común. Por ejemplo, Sócrates afirmó que \"la vida no examinada no merece la pena ser vivida\" para destacar el papel de la indagación filosófica en la comprensión de la propia existencia. Y según Bertrand Russell, \"el hombre que no tiene ninguna tintura de filosofía atraviesa la vida encarcelado en los prejuicios derivados del sentido común, en las creencias habituales de su época o de su nación, y en las convicciones que han crecido en su mente sin la cooperación o el consentimiento de su razón deliberada.\""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "###📝 Resultados del Accuracy reviewer"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- ERROR: 'tintura' puede ser confuso en el contexto filosófico. -> SUGGESTION: Usar 'vestigio' en lugar de 'tintura' para mayor claridad.\n",
            "- ERROR: La estructura de la oración es muy larga y podría ser confusa. -> SUGGESTION: Dividir la oración en dos para mejorar la claridad sin perder el significado.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Traducción editada:*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "La búsqueda filosófica de la sabiduría implica formular preguntas generales y fundamentales. Con frecuencia, no conduce a respuestas directas, pero puede ayudar a una persona a comprender mejor el tema, examinar su vida, disipar la confusión y superar los prejuicios y las ideas autoengañadas asociadas con el sentido común. Por ejemplo, Sócrates afirmó que \"la vida no examinada no merece la pena ser vivida\" para destacar el papel de la indagación filosófica en la comprensión de la propia existencia. Y según Bertrand Russell, \"el hombre que no tiene ningún vestigio de filosofía atraviesa la vida encarcelado en los prejuicios derivados del sentido común, en las creencias habituales de su época o de su nación, y en las convicciones que han crecido en su mente sin la cooperación o el consentimiento de su razón deliberada.\""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "###🎯 Resultados del Fluency_reviewer"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- ERROR: 'formular' sounds formal -> SUGGESTION: Replace with 'hacer' for natural flow\n",
            "- ERROR: 'autoengañadas' -> SUGGESTION: Use 'autoengañosas' for better adjective agreement\n",
            "- ERROR: 'asociadas con el sentido común' -> SUGGESTION: Use 'propias del sentido común' for smoother reading\n",
            "- ERROR: 'no merece la pena' -> SUGGESTION: Replace with 'no vale la pena' for more natural expression\n",
            "- ERROR: 'el hombre' -> SUGGESTION: Replace with 'el ser humano' for inclusivity\n",
            "- ERROR: Redundant 'deliberada' -> SUGGESTION: Simplify to 'su razón' for conciseness\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Traducción editada:*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "La búsqueda filosófica de la sabiduría implica hacer preguntas generales y fundamentales. Con frecuencia, no conduce a respuestas directas, pero puede ayudar a una persona a comprender mejor el tema, examinar su vida, disipar la confusión y superar los prejuicios y las ideas autoengañosas propias del sentido común. Por ejemplo, Sócrates afirmó que \"la vida no examinada no vale la pena ser vivida\" para destacar el papel de la indagación filosófica en la comprensión de la propia existencia. Y según Bertrand Russell, \"el ser humano que no tiene ningún vestigio de filosofía atraviesa la vida encarcelado en los prejuicios derivados del sentido común, en las creencias habituales de su época o de su nación, y en las convicciones que han crecido en su mente sin la cooperación o el consentimiento de su razón deliberada.\""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "✅ Proceso finalizado con éxito"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPKCHlvojpWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXh3ot8xJ8F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noa94-pGJ77C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHTWy3IlJ73r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZW96LC9J7zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxQdv4V5J7qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Con una sola cadena\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "full_chain = RunnablePassthrough.assign(translation=translator_chain) | accuracy_reviewer_chain\n",
        "\n",
        "# Esto no puede ser\n",
        "refull_chain =  translator_prompt_template | llm | parser_translation | accuracy_reviewer_prompt_template | llm | parser_agent\n",
        "\n",
        "\n",
        "resultado_final = full_chain.invoke({\n",
        "    \"source_language\": \"English\",\n",
        "    \"target_language\": \"Spanish\",\n",
        "    \"original_text\": original_text\n",
        "})"
      ],
      "metadata": {
        "id": "Dr7iYNhK1HOz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}