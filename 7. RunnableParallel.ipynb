{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfaxPw7PNUMQk1tkGgeRM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/RunnableParallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. `RunnableParallel`**\n",
        "---\n",
        "RunnableParallel es un componente fundamental en LangChain que permite ejecutar múltiples runnables de manera paralela y combinar sus resultados en un diccionario. Su principal característica es la capacidad de procesar diferentes tareas simultáneamente, mejorando la eficiencia de los flujos de procesamiento.  \n",
        "\n",
        "Las cadenas paralelas permiten ejecutar varias tareas simultáneamente, lo que reduce el tiempo de ejecución general\n",
        "\n",
        "RunnableMap es un \"sinonimo\" de RunnableParallel.\n",
        "\n",
        "- Ejecutan múltiples runnables concurrentemente  \n",
        "- Reciben un diccionario de runnables  \n",
        "- Devuelven un diccionario con los resultados\n"
      ],
      "metadata": {
        "id": "vwVlP-PmEQok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together y Anthropic\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n",
        "El uso de las API de OpenAI y Anthropic es de pago. El resto son gratuitas y para usarlas basta con registrarse y generar una API Key.  \n",
        "\n",
        "En el primer cuaderno encontraras los enlaces a estos servicios y este codigo explicado"
      ],
      "metadata": {
        "id": "6l2ya_hvFHdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "DEEPSEEK_API_KEY=userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "MrwaoEY3FHpT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo 1:\n",
        "---\n",
        "-   Cada función se ejecuta en paralelo\n",
        "-   El resultado es un diccionario con las claves especificadas\n",
        "-   No importa el orden de ejecución, ambas se procesan simultáneamente"
      ],
      "metadata": {
        "id": "rnFGkVGUFCTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihaYmy1rgga_",
        "outputId": "5de9ea33-584d-4f64-e0a2-13134085a795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'duplicado': 20, 'suma': 15}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "\n",
        "# Funciones simples para demostrar paralelismo\n",
        "def duplicar(x):\n",
        "    return x * 2\n",
        "\n",
        "def sumar_cinco(x):\n",
        "    return x + 5\n",
        "\n",
        "# Crear un RunnableParallel que ejecuta ambas operaciones en paralelo\n",
        "parallel_runnable = RunnableParallel(\n",
        "    duplicado=RunnableLambda(duplicar),\n",
        "    suma=RunnableLambda(sumar_cinco)\n",
        ")\n",
        "\n",
        "# Invocar con un valor\n",
        "resultado = parallel_runnable.invoke(10)\n",
        "print(resultado)\n",
        "# Salida esperada: {'duplicado': 20, 'suma': 15}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Kh4vU6c0J-ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "\n",
        "\n",
        "titulo_prompt_template = PromptTemplate.from_template(template=\"Genera un titulo para este texto: {texto}\")\n",
        "\n",
        "palabras_clave_prompt_template = PromptTemplate.from_template(template=\"Genera una lista de 5 palabras clave a partir de este texto: {texto}\")\n",
        "\n",
        "llm_gpt4omini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "\n",
        "def generar_titulo(texto):\n",
        "    return llm_gpt4omini.invoke(titulo_prompt_template.format(texto=texto))\n",
        "\n",
        "def generar_palabras_clave(texto):\n",
        "    return llm_gpt4omini.invoke(palabras_clave_prompt_template.format(texto=texto))\n",
        "\n",
        "tareas = RunnableParallel(\n",
        "                          titulo = RunnableLambda(lambda x: generar_titulo(x)),\n",
        "                          palabras_clave = RunnableLambda(lambda x: generar_palabras_clave(x))\n",
        "                          )\n",
        "\n",
        "texto = '''\n",
        "Eric Arthur Blair (Motihari, Raj Británico, 25 de junio de 1903-Londres, 21 de enero de 1950),1​2​ conocido por su seudónimo de George Orwell, fue un novelista, periodista, ensayista y crítico británico nacido en la India, autor entre otras obras de las novelas distópicas Rebelión en la granja (1945) y 1984 (1949).\n",
        "Su obra lleva la marca de las experiencias autobiográficas vividas por el autor en tres etapas de su vida: su posición en contra del imperialismo británico que lo llevó al compromiso como representante de las fuerzas del orden colonial en Birmania durante su juventud; a favor del socialismo democrático, después de haber observado y sufrido las condiciones de vida de las clases sociales de los trabajadores de Londres y París; y en contra de los totalitarismos nazi y estalinista tras su participación en la guerra civil española, en el bando republicano.\n",
        "Además de cronista, crítico de literatura y novelista, es uno de los ensayistas en lengua inglesa más destacados de las décadas de 1930 y de 1940. También es conocido por sus críticas al totalitarismo en su novela corta alegórica Rebelión en la granja (1945) y su novela distópica 1984 (1949), escrita en sus últimos años de vida y publicada poco antes de su fallecimiento, y en la que crea el concepto de «Gran Hermano», que desde entonces pasó al lenguaje común de la crítica de las técnicas modernas de vigilancia.\n",
        "En 2008 figuraba en el puesto número dos del listado de los cincuenta escritores británicos de mayor relevancia desde 1945, elaborado por The Times.El adjetivo «orwelliano» es frecuentemente utilizado en referencia al distópico universo totalitario imaginado por el escritor británico.\n",
        "'''\n",
        "\n",
        "resultado=tareas.invoke({\"texto\":texto})\n",
        "\n",
        "# Resultado es un diccionario con las dos claves de la tareas paralalelas\n",
        "# print(resultado)\n",
        "\n",
        "print(resultado[\"titulo\"].content)\n",
        "print(\"\\n\")\n",
        "print(resultado[\"palabras_clave\"].content)\n"
      ],
      "metadata": {
        "id": "3CxUDrw6J_Jn",
        "outputId": "a6bdb00d-ffd5-4b02-c697-8dd5a3838804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"George Orwell: La Voz Crítica de una Era Distópica y su Lucha contra el Totalitarismo\"\n",
            "\n",
            "\n",
            "1. George Orwell  \n",
            "2. Distopía  \n",
            "3. Totalitarismo  \n",
            "4. Imperialismo  \n",
            "5. Socialismo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "poner aqui ejemplo de como fusionar el resultado de  2 ramas en 1"
      ],
      "metadata": {
        "id": "mpAMArKyN_wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poner aqui el ejemplo de como fusionar N ramas en 1"
      ],
      "metadata": {
        "id": "BXBcuzCCOI5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación encontrarás un ejemplo completo en el que:\n",
        "\n",
        "Tres modelos (simulados) generan un texto cada uno a partir de la misma entrada.\n",
        "Un cuarto modelo (simulado) toma esos tres textos y los “refunde” (los combina en un resultado final unificado).\n",
        "Para simplificar, usaremos PromptTemplate y RunnableLambda en lugar de un LLM real, pero la estructura sería la misma si reemplazas las partes simuladas por un modelo real (por ejemplo,\n",
        "\n",
        "Esta en chatgpt"
      ],
      "metadata": {
        "id": "x8CrfUrwPeWr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwRDI3NhPe6M"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}