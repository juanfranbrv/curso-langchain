{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSNXhkZlRmTXKxyxM3HlT1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadena%20sencilla%20con%20LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuraci√≥n del entorno del cuaderno**\n",
        "\n"
      ],
      "metadata": {
        "id": "E6Kn0E3vamr5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihaYmy1rgga_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dce1dd-fc95-46cd-b2e5-246bd245541e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.2/1.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "# GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "%pip install langgraph -qU\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "# %pip install langchain-groq -qU\n",
        "# %pip install langchain-google-genai -qU\n",
        "# %pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Importamos las clases necesarias para trabajar con cadenas\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain_groq import ChatGroq\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos aqui que los modelos de lenguaje m√°s modernos, como **gpt-3.5-turbo** y **gpt-4**, est√°n dise√±ados para funcionar de manera √≥ptima en un formato conversacional. Esto implica que, en lugar de enviar un √∫nico \"prompt de texto\" como entrada, podemos organizar la informaci√≥n en una secuencia de mensajes con roles espec√≠ficos:\n",
        "\n",
        "-   **system**: Mensaje del sistema (define el contexto o comportamiento del asistente).\n",
        "    \n",
        "-   **user**: Mensaje del usuario (preguntas o instrucciones del usuario).\n",
        "    \n",
        "-   **assistant**: Mensaje del asistente (respuestas generadas por el modelo).\n",
        "    \n",
        "\n",
        "LangChain admite varios tipos de mensajes, incluidos `HumanMessage`, `AIMessage`, `SystemMessage` y `ToolMessage`.\n",
        "\n",
        "Estos representan un mensaje del usuario, del modelo de chat, para que el modelo de chat observe un comportamiento y de una llamada a una herramienta.\n",
        "\n",
        "\n",
        "`**ChatPromptTemplate**` simplifica la creaci√≥n de estos mensajes estructurados, evitando la necesidad de concatenarlos manualmente en una sola cadena de texto.\n",
        "\n",
        "Creemos una lista de mensajes.\n"
      ],
      "metadata": {
        "id": "PbcRb26IYQqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un experto en literatura fantastica y ciencia ficci√≥n\"),\n",
        "    (\"human\", \"Proporciona un listado con los {numero} autores mas importantes de literatura fantastica en la actualidad\"),\n",
        "])\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "respuesta = llm.invoke(chat_prompt_template.format_messages(numero=5))\n",
        "\n",
        "respuesta\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I6JYB76bhCj",
        "outputId": "e7ce94d3-c9f3-42a6-da65-3f305056004c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Aqu√≠ tienes un listado con cinco de los autores m√°s importantes de literatura fant√°stica en la actualidad:\\n\\n1. **Neil Gaiman**: Conocido por obras como \"American Gods\", \"Coraline\" y \"The Ocean at the End of the Lane\", Gaiman ha sido una figura central en la literatura fant√°stica contempor√°nea, combinando mitolog√≠a, folclore y elementos de la cultura popular.\\n\\n2. **Patrick Rothfuss**: Su serie \"The Kingkiller Chronicle\", que incluye \"The Name of the Wind\" y \"The Wise Man\\'s Fear\", ha sido aclamada por su prosa l√≠rica y su profundo desarrollo de personajes, convirti√©ndolo en un referente en la fantas√≠a moderna.\\n\\n3. **N.K. Jemisin**: Ganadora de m√∫ltiples premios Hugo, Jemisin es conocida por su trilog√≠a \"The Broken Earth\", que explora temas de opresi√≥n y poder en un mundo de fantas√≠a complejo y original. Su trabajo ha sido fundamental para diversificar el g√©nero.\\n\\n4. **Brandon Sanderson**: Conocido por su habilidad para construir mundos y sistemas de magia complejos, Sanderson ha escrito series populares como \"Mistborn\" y \"The Stormlight Archive\", atrayendo a una gran base de lectores.\\n\\n5. **Ursula K. Le Guin**: Aunque falleci√≥ en 2018, su influencia en la literatura fant√°stica sigue siendo inmensa. Obras como \"La mano izquierda de la oscuridad\" y \"Los despose√≠dos\" han dejado una huella duradera en el g√©nero, y su legado contin√∫a inspirando a nuevos autores.\\n\\nEstos autores han contribuido significativamente al desarrollo y la popularidad de la literatura fant√°stica en la actualidad, cada uno con su estilo √∫nico y su enfoque innovador.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 360, 'prompt_tokens': 41, 'total_tokens': 401, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-de8c874f-9b85-467e-b1c8-1dd695230985-0', usage_metadata={'input_tokens': 41, 'output_tokens': 360, 'total_tokens': 401, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La respuesta del LLM es un objeto AIMessage.\n",
        "Habitualmente la parte en la que estamos interesado es content\n",
        "\n"
      ],
      "metadata": {
        "id": "jBZRcsDLmFh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "rFBG_pDZmcCB",
        "outputId": "3211407d-fd64-4309-a0a5-86ef5a7b9af5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aqu√≠ tienes un listado con cinco de los autores m√°s importantes de literatura fant√°stica en la actualidad:\\n\\n1. **Neil Gaiman**: Conocido por obras como \"American Gods\", \"Coraline\" y \"The Ocean at the End of the Lane\", Gaiman ha sido una figura central en la literatura fant√°stica contempor√°nea, combinando mitolog√≠a, folclore y elementos de la cultura popular.\\n\\n2. **Patrick Rothfuss**: Su serie \"The Kingkiller Chronicle\", que incluye \"The Name of the Wind\" y \"The Wise Man\\'s Fear\", ha sido aclamada por su prosa l√≠rica y su profundo desarrollo de personajes, convirti√©ndolo en un referente en la fantas√≠a moderna.\\n\\n3. **N.K. Jemisin**: Ganadora de m√∫ltiples premios Hugo, Jemisin es conocida por su trilog√≠a \"The Broken Earth\", que explora temas de opresi√≥n y poder en un mundo de fantas√≠a complejo y original. Su trabajo ha sido fundamental para diversificar el g√©nero.\\n\\n4. **Brandon Sanderson**: Conocido por su habilidad para construir mundos y sistemas de magia complejos, Sanderson ha escrito series populares como \"Mistborn\" y \"The Stormlight Archive\", atrayendo a una gran base de lectores.\\n\\n5. **Ursula K. Le Guin**: Aunque falleci√≥ en 2018, su influencia en la literatura fant√°stica sigue siendo inmensa. Obras como \"La mano izquierda de la oscuridad\" y \"Los despose√≠dos\" han dejado una huella duradera en el g√©nero, y su legado contin√∫a inspirando a nuevos autores.\\n\\nEstos autores han contribuido significativamente al desarrollo y la popularidad de la literatura fant√°stica en la actualidad, cada uno con su estilo √∫nico y su enfoque innovador.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero la respuesta contiene abundantes datos que en ocasiones son de utilidad"
      ],
      "metadata": {
        "id": "nH5gV5bNmfEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta.response_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPD1OkQSmphX",
        "outputId": "a56bcc60-e304-4c72-83ae-c2959e421f6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'token_usage': {'completion_tokens': 360,\n",
              "  'prompt_tokens': 41,\n",
              "  'total_tokens': 401,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
              " 'system_fingerprint': 'fp_d02d531b47',\n",
              " 'finish_reason': 'stop',\n",
              " 'logprobs': None}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las herramientas son √∫tiles siempre que se desea que un modelo interact√∫e con sistemas externos.\n",
        "\n",
        "Los sistemas externos (por ejemplo, las API) a menudo requieren un esquema de entrada o una carga √∫til particular, en lugar de lenguaje natural.\n",
        "\n",
        "Cuando vinculamos una API, por ejemplo, como herramienta, le damos al modelo el conocimiento del esquema de entrada requerido.\n",
        "\n",
        "El modelo elegir√° llamar a una herramienta en funci√≥n de la entrada en lenguaje natural del usuario.\n",
        "\n",
        "Y devolver√° una salida que se ajuste al esquema de la herramienta.\n",
        "\n",
        "Muchos proveedores de LLM admiten la llamada a herramientas y la interfaz de llamada a herramientas en LangChain es sencilla.\n",
        "\n",
        "Simplemente puede pasar cualquier funci√≥n de Python a ChatModel.bind_tools(function).\n",
        "\n",
        "\n",
        "Muchos proveedores de LLM admiten la llamada de herramientas y la interfaz de llamada de herramientas en LangChain es sencilla.  \n",
        "\n",
        "https://python.langchain.com/v0.1/docs/integrations/chat/  \n",
        "https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/\n",
        "\n",
        "puedes pasar directamente funciones de Python a m√©todos como .bind_tools(), y el sistema se encarga de inferir autom√°ticamente los esquemas necesarios a partir de las anotaciones de tipo y las cadenas de documentaci√≥n (docstrings) de las funciones.\n",
        "\n",
        "Esta simplificaci√≥n hace que el proceso sea mucho m√°s intuitivo y reduce la cantidad de c√≥digo repetitivo.\n",
        "\n",
        "Podemos pasar cualquier funci√≥n de Python a ChatModel.bind_tools() , lo que permite que las funciones normales de Python se utilicen directamente como herramientas.\n",
        "\n",
        "Vamos a crear una funcion que multiplica dos numeros y las vamos a pasar como herramienta a nuestro modelo."
      ],
      "metadata": {
        "id": "voWKBi81nOXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiplicar(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplica a y b.\n",
        "\n",
        "    Args:\n",
        "        a: primer int\n",
        "        b: segundo int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm_con_herramientas = llm.bind_tools([multiplicar])"
      ],
      "metadata": {
        "id": "f8dtjwKupVAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora llm_con_herramientas es un nuevo llm con una herramienta, y si lo invocamos con cierta pregunta, obtendremos como resultado una AIMessage que sin contenido pero que es una llamada a la herramienta."
      ],
      "metadata": {
        "id": "lbZIDn-sp2Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "respuesta = llm_con_herramientas.invoke(\"Cuanto es 2 por 3\")\n",
        "respuesta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZAS1rZp3uk",
        "outputId": "74200e0c-ef42-44c6-a770-baa89d1d06f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_E7BskxXd6Ye93PjZl598XOPm', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiplicar'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 62, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8fad7fb1-d892-416b-a70f-5c1b10bbf05d-0', tool_calls=[{'name': 'multiplicar', 'args': {'a': 2, 'b': 3}, 'id': 'call_E7BskxXd6Ye93PjZl598XOPm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 20, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain analizar√° las anotaciones de tipo y las cadenas de documentaci√≥n para inferir los esquemas necesarios."
      ],
      "metadata": {
        "id": "FpyGok_eq3rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta.additional_kwargs['tool_calls']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZbkD_Nvq5bh",
        "outputId": "12b0ea23-83fe-40e7-8b36-4b04814a8f23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'call_E7BskxXd6Ye93PjZl598XOPm',\n",
              "  'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiplicar'},\n",
              "  'type': 'function'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si el prompt no tiene nada que ver no se llamara a la herramienta"
      ],
      "metadata": {
        "id": "SwKqVhFwvdBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta = llm_con_herramientas.invoke(\"Que es un p√°jaro ?\")\n",
        "respuesta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXH5ryiYvdhG",
        "outputId": "24294995-08d1-4566-b365-964d1eb6b89a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Un p√°jaro es un animal vertebrado que pertenece al grupo de las aves. Se caracteriza por tener plumas, un pico sin dientes, y la mayor√≠a de las especies son capaces de volar. Los p√°jaros son de sangre caliente y tienen un sistema respiratorio altamente eficiente. Se reproducen poniendo huevos y se encuentran en casi todos los h√°bitats del planeta, desde bosques y monta√±as hasta desiertos y oc√©anos. Adem√°s, los p√°jaros desempe√±an roles importantes en los ecosistemas, como polinizadores, dispersores de semillas y controladores de insectos.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 61, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-5d2463af-db1f-43ae-9c0c-077131479733-0', usage_metadata={'input_tokens': 61, 'output_tokens': 124, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un ejemplo solo un poco mas complicado. Vamos a dotar a nuestro modelo de cuatro herramientas de calculo sencillas."
      ],
      "metadata": {
        "id": "bKiQbhRt1alT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "\n",
        "\n",
        "def sumar(a: float, b: float) -> float:\n",
        "    \"\"\"Suma dos n√∫meros.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def restar(a: float, b: float) -> float:\n",
        "    \"\"\"Resta dos n√∫meros.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiplicar(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplica dos n√∫meros.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def dividir(a: float, b: float) -> float:\n",
        "    \"\"\"Divide dos n√∫meros.\"\"\"\n",
        "    if b == 0:\n",
        "        return \"Error: No se puede dividir por cero.\"\n",
        "    return a / b\n",
        "\n",
        "# Crear el modelo de OpenAI\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Vincular las herramientas al modelo usando .bind_tools\n",
        "modelo_con_herramientas = modelo.bind_tools([sumar, restar, multiplicar, dividir])\n",
        "\n",
        "# Preguntar al modelo\n",
        "pregunta = \"¬øCu√°nto es 15 - 20? Luego, divide el resultado por 5.\"\n",
        "respuesta = modelo_con_herramientas.invoke(pregunta)\n",
        "\n",
        "# Mostrar la respuesta\n",
        "print(respuesta.additional_kwargs['tool_calls'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHvjlMWE1j88",
        "outputId": "1ff4f07c-063f-4d21-aa31-f9878b51ce73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'call_ZVqJ7SNJ8DdIGFrChexxqZJG', 'function': {'arguments': '{\"a\":15,\"b\":20}', 'name': 'restar'}, 'type': 'function'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa como cambiando el prompt obtenemos resultados diferentes. Desde una llamada a una herramienta a un resultado directo.\n",
        "\n",
        "Hablar del decorador @tool\n",
        "https://python.langchain.com/v0.2/docs/how_to/custom_tools/?ref=blog.langchain.dev#creating-tools-from-functions\n",
        "\n"
      ],
      "metadata": {
        "id": "ySekq0P_7EJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear el estado de nuestro grafo como\n",
        "\n",
        "((MessagesState: Esta clase hereda de TypedDict, lo que significa que define la estructura de un diccionario que debe contener:\n",
        "Una clave messages.\n",
        "El valor asociado a esta clave debe ser una lista de objetos de tipo AnyMessage. (AnyMessage es un tipo de mensaje gen√©rico que proviene de langchain_core.messages. Representa cualquier tipo de mensaje que puedas usar en LangChain, como mensajes de usuario, respuestas del sistema, etc.)))\n",
        "\n",
        "### **`MessagesState` en LangGraph**\n",
        "\n",
        "-   En LangGraph, `MessagesState` es una clase predefinida que representa un estado que contiene una lista de mensajes (`messages`).\n",
        "    \n",
        "-   Esta clase ya est√° configurada para manejar el historial de mensajes en un flujo de trabajo.\n",
        "\n",
        "En un flujo de trabajo de LangGraph, es com√∫n necesitar m√°s que solo mensajes. Por ejemplo:\n",
        "\n",
        "-   Podr√≠as querer almacenar el **contexto** de la conversaci√≥n.        \n",
        "-   O agregar un campo para el **estado del usuario**.        \n",
        "-   O cualquier otra informaci√≥n relevante para tu aplicaci√≥n.\n",
        "    \n",
        "       \n",
        "Al extender `MessagesState`, puedes agregar estas claves adicionales sin perder la funcionalidad predefinida de `messages`.\n",
        "\n",
        "Habitualmente pues estaremos creando una nueva clase que hereda de `MessagesState` para incluir claves adicionales en el estado, adem√°s de `messages` de esta forma:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class MessagesState(MessagesState):\n",
        "    # Add any keys needed beyond messages, which is pre-built\n",
        "    # messages: list[AnyMessage]\n",
        "    pass\n",
        "```\n",
        "Habra que indicar que messages es una lista !!\n",
        "\n",
        "messages: list[AnyMessage]\n",
        "\n",
        "Armados ahora de MessageState vamos a contruit un grafo que contenga un llm con herramientas\n",
        "\n"
      ],
      "metadata": {
        "id": "H21K7cNQ--9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "\n",
        "# Definimos la estructura del estado\n",
        "class MessagesState(MessagesState):\n",
        "     # messages: list[AnyMessage]   Esto se maneja internamente\n",
        "     pass\n",
        "\n",
        "# DEFINIMOS LOS NODOS\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [modelo_con_herramientas.invoke(state[\"messages\"])]}\n",
        "\n",
        "# GRAFO Y A√ëADIMOS LOS NODOS\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "\n",
        "# LOGICA Y ARISTAS\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "\n",
        "# ?\n",
        "# Add\n",
        "grafo = builder.compile()"
      ],
      "metadata": {
        "id": "5bObJTJI-8Dn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora si invocamos el grafo con un simple \"Hola\" al modelo, este responde sin usar la herramientas.  \n",
        "Recuerda que para invocar el grafo hay que pasarle un estado con el esquema que hayamos definido.  En este caso, un diccionario con una clave messages que tiene una lista de mensajes que son diccionarios. Algo asi\n",
        "\n",
        "\n",
        "```\n",
        "MessagesState = {\n",
        "    \"messages\": [\n",
        "        {\"content\": \"Hola, ¬øc√≥mo est√°s?\", \"type\": \"user\"},\n",
        "        {\"content\": \"Estoy bien, gracias.\", \"type\": \"assistant\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3_l09sSmM4nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con un prompt normal, no usar√° herramientas.\n",
        "(Estamos usando el metodo prety_print() que posee incorporado los mensajes)"
      ],
      "metadata": {
        "id": "RPeOBycBS8k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mensajes = grafo.invoke({\"messages\": [(\"system\",\"Eres un sistema que debes mostrarte siempre extremadamente amable y cari√±oso\"),(\"ai\",\"Hola, querido. En que puedo ayudarte ?\"),(\"human\", \"Hola, quien eres tu ?\")] })\n",
        "for m in mensajes['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7phryL_NTGu",
        "outputId": "602197c1-4f73-46d2-f1da-455f554ec1ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "Eres un sistema que debes mostrarte siempre extremadamente amable y cari√±oso\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hola, querido. En que puedo ayudarte ?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hola, quien eres tu ?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "¬°Hola, hermoso! Soy un asistente virtual aqu√≠ para ayudarte con lo que necesites. Estoy muy feliz de poder conversar contigo. ¬øHay algo en particular en lo que te gustar√≠a que te ayude hoy? üåü\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero si invocamos el grafo con una indicacion que requiera el uso de las herramientas de las que dispone el modelo, este lo detectara y hara uso de ellas\n"
      ],
      "metadata": {
        "id": "9n9Tayn6TSBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mensajes = grafo.invoke({\"messages\": [(\"system\",\"Eres un sistema que breve y conciso\"),(\"ai\",\"Hola\"),(\"human\", \"Multiplica 2 por 3 y luego al resultado sumale 10\")] })\n",
        "for m in mensajes['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhvlVfGeTjZw",
        "outputId": "c1dd2bc4-8712-47e7-8640-22ff36cc7b58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "Eres un sistema que breve y conciso\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hola\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiplica 2 por 3 y luego al resultado sumale 10\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiplicar (call_qNa22NZXDJv6jGO3QsgJK905)\n",
            " Call ID: call_qNa22NZXDJv6jGO3QsgJK905\n",
            "  Args:\n",
            "    a: 2\n",
            "    b: 3\n",
            "  sumar (call_WOQiVkDGdqUugCuFk9ggLq7p)\n",
            " Call ID: call_WOQiVkDGdqUugCuFk9ggLq7p\n",
            "  Args:\n",
            "    a: 10\n",
            "    b: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENTENDER TODO ESTE MONTAJE BIEN\n",
        "\n",
        "Vamos a crear un LLM al que le dotaremos de un herramienta que le permite proporcionar al usuario el nombre y el telefono de un cierto agente cada uno de los cuales se ocupa de un cierto problema.\n",
        "\n",
        "Disponemos de un diccionario\n",
        "\n",
        "{\n",
        "  \"RECURSOS_HUMANOS\": [\"Sof√≠a Rodr√≠guez\", \"555-123-4567\"],\n",
        "  \"FINANZAS\": [\"Carlos Mart√≠nez\", \"555-987-6543\"],\n",
        "  \"TECNOLOGIA\": [\"Elena P√©rez\", \"555-246-8013\"],\n",
        "  \"OPERACIONES\": [\"Javier G√≥mez\", \"555-789-1234\"],\n",
        "  \"VENTAS\": [\"Ana L√≥pez\", \"555-369-2580\"],\n",
        "  \"MARKETING\": [\"Diego Torres\", \"555-159-7530\"],\n",
        "  \"COMPRAS\": [\"Isabel D√≠az\", \"555-852-9631\"],\n",
        "  \"LEGAL\": [\"Ricardo Vargas\", \"555-741-9632\"],\n",
        "  \"INVESTIGACION\": [\"Laura Castro\", \"555-632-1478\"],\n",
        "  \"ATENCION_CLIENTE\": [\"Manuel Ruiz\", \"555-951-3682\"]\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UoyERNZZVELU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, START, StateGraph, MessagesState\n",
        "\n",
        "\n",
        "# 1. DEFINIMOS EL ESQUEMA DEL ESTADO\n",
        "# Siempre es algun tipo de diccionario\n",
        "\n",
        "class MessagesState(MessagesState):\n",
        "     # messages: list[AnyMessage]   Esto se maneja internamente\n",
        "     loquesea: str\n",
        "     otracosa: int\n",
        "\n",
        "# 2. INSTANCIAMOS UN GRAFO pas√°ndole el tipo del estado al constructor.\n",
        "\n",
        "grafo = StateGraph(MessagesState)\n",
        "\n",
        "# 3. A√ëADIMOS NODOS AL GRAFO\n",
        "# Usamos la sintaxis graph.add_node(nombre, runnable)\n",
        "# nombre: el nombre del nodo\n",
        "# runnable: funci√≥n o un ejecutable LCEL que se llamar√° al entrar al nodo\n",
        "# Esta funci√≥n/LCEL debe aceptar un diccionario en el mismo formato que el ESTADO como entrada\n",
        "# Y devolver un diccionario tambien con el mismo formato que el estado, que sera el nuevo estado.\n",
        "\n",
        "grafo.add_node(\"agente\", modelo)\n",
        "grafo.add_node(\"herramientas\", nodo_herramientas)\n",
        "\n",
        "# 4. LOGICA Y ARISTAS\n",
        "# El nodo inicial lo definimos haciendo uso de START\n",
        "\n",
        "grafo.add_edge(START, \"agent\")\n",
        "\n",
        "#Cualquier arista no condicional la creamos\n",
        "# grafo.add(nodo1, nodo2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PHy1gs3kcwSz"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}