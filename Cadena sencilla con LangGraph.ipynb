{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSNXhkZlRmTXKxyxM3HlT1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadena%20sencilla%20con%20LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuración del entorno del cuaderno**\n",
        "\n"
      ],
      "metadata": {
        "id": "E6Kn0E3vamr5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihaYmy1rgga_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dce1dd-fc95-46cd-b2e5-246bd245541e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "# GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "%pip install langgraph -qU\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "# %pip install langchain-groq -qU\n",
        "# %pip install langchain-google-genai -qU\n",
        "# %pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Importamos las clases necesarias para trabajar con cadenas\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain_groq import ChatGroq\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos aqui que los modelos de lenguaje más modernos, como **gpt-3.5-turbo** y **gpt-4**, están diseñados para funcionar de manera óptima en un formato conversacional. Esto implica que, en lugar de enviar un único \"prompt de texto\" como entrada, podemos organizar la información en una secuencia de mensajes con roles específicos:\n",
        "\n",
        "-   **system**: Mensaje del sistema (define el contexto o comportamiento del asistente).\n",
        "    \n",
        "-   **user**: Mensaje del usuario (preguntas o instrucciones del usuario).\n",
        "    \n",
        "-   **assistant**: Mensaje del asistente (respuestas generadas por el modelo).\n",
        "    \n",
        "\n",
        "LangChain admite varios tipos de mensajes, incluidos `HumanMessage`, `AIMessage`, `SystemMessage` y `ToolMessage`.\n",
        "\n",
        "Estos representan un mensaje del usuario, del modelo de chat, para que el modelo de chat observe un comportamiento y de una llamada a una herramienta.\n",
        "\n",
        "\n",
        "`**ChatPromptTemplate**` simplifica la creación de estos mensajes estructurados, evitando la necesidad de concatenarlos manualmente en una sola cadena de texto.\n",
        "\n",
        "Creemos una lista de mensajes.\n"
      ],
      "metadata": {
        "id": "PbcRb26IYQqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un experto en literatura fantastica y ciencia ficción\"),\n",
        "    (\"human\", \"Proporciona un listado con los {numero} autores mas importantes de literatura fantastica en la actualidad\"),\n",
        "])\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "respuesta = llm.invoke(chat_prompt_template.format_messages(numero=5))\n",
        "\n",
        "respuesta\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I6JYB76bhCj",
        "outputId": "e7ce94d3-c9f3-42a6-da65-3f305056004c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Aquí tienes un listado con cinco de los autores más importantes de literatura fantástica en la actualidad:\\n\\n1. **Neil Gaiman**: Conocido por obras como \"American Gods\", \"Coraline\" y \"The Ocean at the End of the Lane\", Gaiman ha sido una figura central en la literatura fantástica contemporánea, combinando mitología, folclore y elementos de la cultura popular.\\n\\n2. **Patrick Rothfuss**: Su serie \"The Kingkiller Chronicle\", que incluye \"The Name of the Wind\" y \"The Wise Man\\'s Fear\", ha sido aclamada por su prosa lírica y su profundo desarrollo de personajes, convirtiéndolo en un referente en la fantasía moderna.\\n\\n3. **N.K. Jemisin**: Ganadora de múltiples premios Hugo, Jemisin es conocida por su trilogía \"The Broken Earth\", que explora temas de opresión y poder en un mundo de fantasía complejo y original. Su trabajo ha sido fundamental para diversificar el género.\\n\\n4. **Brandon Sanderson**: Conocido por su habilidad para construir mundos y sistemas de magia complejos, Sanderson ha escrito series populares como \"Mistborn\" y \"The Stormlight Archive\", atrayendo a una gran base de lectores.\\n\\n5. **Ursula K. Le Guin**: Aunque falleció en 2018, su influencia en la literatura fantástica sigue siendo inmensa. Obras como \"La mano izquierda de la oscuridad\" y \"Los desposeídos\" han dejado una huella duradera en el género, y su legado continúa inspirando a nuevos autores.\\n\\nEstos autores han contribuido significativamente al desarrollo y la popularidad de la literatura fantástica en la actualidad, cada uno con su estilo único y su enfoque innovador.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 360, 'prompt_tokens': 41, 'total_tokens': 401, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-de8c874f-9b85-467e-b1c8-1dd695230985-0', usage_metadata={'input_tokens': 41, 'output_tokens': 360, 'total_tokens': 401, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La respuesta del LLM es un objeto AIMessage.\n",
        "Habitualmente la parte en la que estamos interesado es content\n",
        "\n"
      ],
      "metadata": {
        "id": "jBZRcsDLmFh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "rFBG_pDZmcCB",
        "outputId": "3211407d-fd64-4309-a0a5-86ef5a7b9af5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aquí tienes un listado con cinco de los autores más importantes de literatura fantástica en la actualidad:\\n\\n1. **Neil Gaiman**: Conocido por obras como \"American Gods\", \"Coraline\" y \"The Ocean at the End of the Lane\", Gaiman ha sido una figura central en la literatura fantástica contemporánea, combinando mitología, folclore y elementos de la cultura popular.\\n\\n2. **Patrick Rothfuss**: Su serie \"The Kingkiller Chronicle\", que incluye \"The Name of the Wind\" y \"The Wise Man\\'s Fear\", ha sido aclamada por su prosa lírica y su profundo desarrollo de personajes, convirtiéndolo en un referente en la fantasía moderna.\\n\\n3. **N.K. Jemisin**: Ganadora de múltiples premios Hugo, Jemisin es conocida por su trilogía \"The Broken Earth\", que explora temas de opresión y poder en un mundo de fantasía complejo y original. Su trabajo ha sido fundamental para diversificar el género.\\n\\n4. **Brandon Sanderson**: Conocido por su habilidad para construir mundos y sistemas de magia complejos, Sanderson ha escrito series populares como \"Mistborn\" y \"The Stormlight Archive\", atrayendo a una gran base de lectores.\\n\\n5. **Ursula K. Le Guin**: Aunque falleció en 2018, su influencia en la literatura fantástica sigue siendo inmensa. Obras como \"La mano izquierda de la oscuridad\" y \"Los desposeídos\" han dejado una huella duradera en el género, y su legado continúa inspirando a nuevos autores.\\n\\nEstos autores han contribuido significativamente al desarrollo y la popularidad de la literatura fantástica en la actualidad, cada uno con su estilo único y su enfoque innovador.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero la respuesta contiene abundantes datos que en ocasiones son de utilidad"
      ],
      "metadata": {
        "id": "nH5gV5bNmfEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta.response_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPD1OkQSmphX",
        "outputId": "a56bcc60-e304-4c72-83ae-c2959e421f6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'token_usage': {'completion_tokens': 360,\n",
              "  'prompt_tokens': 41,\n",
              "  'total_tokens': 401,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
              " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
              " 'system_fingerprint': 'fp_d02d531b47',\n",
              " 'finish_reason': 'stop',\n",
              " 'logprobs': None}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las herramientas son útiles siempre que se desea que un modelo interactúe con sistemas externos.\n",
        "\n",
        "Los sistemas externos (por ejemplo, las API) a menudo requieren un esquema de entrada o una carga útil particular, en lugar de lenguaje natural.\n",
        "\n",
        "Cuando vinculamos una API, por ejemplo, como herramienta, le damos al modelo el conocimiento del esquema de entrada requerido.\n",
        "\n",
        "El modelo elegirá llamar a una herramienta en función de la entrada en lenguaje natural del usuario.\n",
        "\n",
        "Y devolverá una salida que se ajuste al esquema de la herramienta.\n",
        "\n",
        "Muchos proveedores de LLM admiten la llamada a herramientas y la interfaz de llamada a herramientas en LangChain es sencilla.\n",
        "\n",
        "Simplemente puede pasar cualquier función de Python a ChatModel.bind_tools(function).\n",
        "\n",
        "\n",
        "Muchos proveedores de LLM admiten la llamada de herramientas y la interfaz de llamada de herramientas en LangChain es sencilla.  \n",
        "\n",
        "https://python.langchain.com/v0.1/docs/integrations/chat/  \n",
        "https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/\n",
        "\n",
        "puedes pasar directamente funciones de Python a métodos como .bind_tools(), y el sistema se encarga de inferir automáticamente los esquemas necesarios a partir de las anotaciones de tipo y las cadenas de documentación (docstrings) de las funciones.\n",
        "\n",
        "Esta simplificación hace que el proceso sea mucho más intuitivo y reduce la cantidad de código repetitivo.\n",
        "\n",
        "Podemos pasar cualquier función de Python a ChatModel.bind_tools() , lo que permite que las funciones normales de Python se utilicen directamente como herramientas.\n",
        "\n",
        "Vamos a crear una funcion que multiplica dos numeros y las vamos a pasar como herramienta a nuestro modelo."
      ],
      "metadata": {
        "id": "voWKBi81nOXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiplicar(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplica a y b.\n",
        "\n",
        "    Args:\n",
        "        a: primer int\n",
        "        b: segundo int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm_con_herramientas = llm.bind_tools([multiplicar])"
      ],
      "metadata": {
        "id": "f8dtjwKupVAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora llm_con_herramientas es un nuevo llm con una herramienta, y si lo invocamos con cierta pregunta, obtendremos como resultado una AIMessage que sin contenido pero que es una llamada a la herramienta."
      ],
      "metadata": {
        "id": "lbZIDn-sp2Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "respuesta = llm_con_herramientas.invoke(\"Cuanto es 2 por 3\")\n",
        "respuesta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZAS1rZp3uk",
        "outputId": "74200e0c-ef42-44c6-a770-baa89d1d06f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_E7BskxXd6Ye93PjZl598XOPm', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiplicar'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 62, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8fad7fb1-d892-416b-a70f-5c1b10bbf05d-0', tool_calls=[{'name': 'multiplicar', 'args': {'a': 2, 'b': 3}, 'id': 'call_E7BskxXd6Ye93PjZl598XOPm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 20, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain analizará las anotaciones de tipo y las cadenas de documentación para inferir los esquemas necesarios."
      ],
      "metadata": {
        "id": "FpyGok_eq3rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta.additional_kwargs['tool_calls']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZbkD_Nvq5bh",
        "outputId": "12b0ea23-83fe-40e7-8b36-4b04814a8f23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'call_E7BskxXd6Ye93PjZl598XOPm',\n",
              "  'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiplicar'},\n",
              "  'type': 'function'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si el prompt no tiene nada que ver no se llamara a la herramienta"
      ],
      "metadata": {
        "id": "SwKqVhFwvdBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "respuesta = llm_con_herramientas.invoke(\"Que es un pájaro ?\")\n",
        "respuesta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXH5ryiYvdhG",
        "outputId": "24294995-08d1-4566-b365-964d1eb6b89a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Un pájaro es un animal vertebrado que pertenece al grupo de las aves. Se caracteriza por tener plumas, un pico sin dientes, y la mayoría de las especies son capaces de volar. Los pájaros son de sangre caliente y tienen un sistema respiratorio altamente eficiente. Se reproducen poniendo huevos y se encuentran en casi todos los hábitats del planeta, desde bosques y montañas hasta desiertos y océanos. Además, los pájaros desempeñan roles importantes en los ecosistemas, como polinizadores, dispersores de semillas y controladores de insectos.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 61, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-5d2463af-db1f-43ae-9c0c-077131479733-0', usage_metadata={'input_tokens': 61, 'output_tokens': 124, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un ejemplo solo un poco mas complicado. Vamos a dotar a nuestro modelo de cuatro herramientas de calculo sencillas."
      ],
      "metadata": {
        "id": "bKiQbhRt1alT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "\n",
        "\n",
        "def sumar(a: float, b: float) -> float:\n",
        "    \"\"\"Suma dos números.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def restar(a: float, b: float) -> float:\n",
        "    \"\"\"Resta dos números.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiplicar(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplica dos números.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def dividir(a: float, b: float) -> float:\n",
        "    \"\"\"Divide dos números.\"\"\"\n",
        "    if b == 0:\n",
        "        return \"Error: No se puede dividir por cero.\"\n",
        "    return a / b\n",
        "\n",
        "# Crear el modelo de OpenAI\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Vincular las herramientas al modelo usando .bind_tools\n",
        "modelo_con_herramientas = modelo.bind_tools([sumar, restar, multiplicar, dividir])\n",
        "\n",
        "# Preguntar al modelo\n",
        "pregunta = \"¿Cuánto es 15 - 20? Luego, divide el resultado por 5.\"\n",
        "respuesta = modelo_con_herramientas.invoke(pregunta)\n",
        "\n",
        "# Mostrar la respuesta\n",
        "print(respuesta.additional_kwargs['tool_calls'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHvjlMWE1j88",
        "outputId": "1ff4f07c-063f-4d21-aa31-f9878b51ce73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'call_ZVqJ7SNJ8DdIGFrChexxqZJG', 'function': {'arguments': '{\"a\":15,\"b\":20}', 'name': 'restar'}, 'type': 'function'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa como cambiando el prompt obtenemos resultados diferentes. Desde una llamada a una herramienta a un resultado directo.\n",
        "\n",
        "Hablar del decorador @tool\n",
        "https://python.langchain.com/v0.2/docs/how_to/custom_tools/?ref=blog.langchain.dev#creating-tools-from-functions\n",
        "\n"
      ],
      "metadata": {
        "id": "ySekq0P_7EJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear el estado de nuestro grafo como\n",
        "\n",
        "((MessagesState: Esta clase hereda de TypedDict, lo que significa que define la estructura de un diccionario que debe contener:\n",
        "Una clave messages.\n",
        "El valor asociado a esta clave debe ser una lista de objetos de tipo AnyMessage. (AnyMessage es un tipo de mensaje genérico que proviene de langchain_core.messages. Representa cualquier tipo de mensaje que puedas usar en LangChain, como mensajes de usuario, respuestas del sistema, etc.)))\n",
        "\n",
        "### **`MessagesState` en LangGraph**\n",
        "\n",
        "-   En LangGraph, `MessagesState` es una clase predefinida que representa un estado que contiene una lista de mensajes (`messages`).\n",
        "    \n",
        "-   Esta clase ya está configurada para manejar el historial de mensajes en un flujo de trabajo.\n",
        "\n",
        "En un flujo de trabajo de LangGraph, es común necesitar más que solo mensajes. Por ejemplo:\n",
        "\n",
        "-   Podrías querer almacenar el **contexto** de la conversación.        \n",
        "-   O agregar un campo para el **estado del usuario**.        \n",
        "-   O cualquier otra información relevante para tu aplicación.\n",
        "    \n",
        "       \n",
        "Al extender `MessagesState`, puedes agregar estas claves adicionales sin perder la funcionalidad predefinida de `messages`.\n",
        "\n",
        "Habitualmente pues estaremos creando una nueva clase que hereda de `MessagesState` para incluir claves adicionales en el estado, además de `messages` de esta forma:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class MessagesState(MessagesState):\n",
        "    # Add any keys needed beyond messages, which is pre-built\n",
        "    # messages: list[AnyMessage]\n",
        "    pass\n",
        "```\n",
        "Habra que indicar que messages es una lista !!\n",
        "\n",
        "messages: list[AnyMessage]\n",
        "\n",
        "Armados ahora de MessageState vamos a contruit un grafo que contenga un llm con herramientas\n",
        "\n"
      ],
      "metadata": {
        "id": "H21K7cNQ--9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "\n",
        "# Definimos la estructura del estado\n",
        "class MessagesState(MessagesState):\n",
        "     # messages: list[AnyMessage]   Esto se maneja internamente\n",
        "     pass\n",
        "\n",
        "# DEFINIMOS LOS NODOS\n",
        "def tool_calling_llm(state: MessagesState):\n",
        "    return {\"messages\": [modelo_con_herramientas.invoke(state[\"messages\"])]}\n",
        "\n",
        "# GRAFO Y AÑADIMOS LOS NODOS\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "\n",
        "# LOGICA Y ARISTAS\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "\n",
        "# ?\n",
        "# Add\n",
        "grafo = builder.compile()"
      ],
      "metadata": {
        "id": "5bObJTJI-8Dn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora si invocamos el grafo con un simple \"Hola\" al modelo, este responde sin usar la herramientas.  \n",
        "Recuerda que para invocar el grafo hay que pasarle un estado con el esquema que hayamos definido.  En este caso, un diccionario con una clave messages que tiene una lista de mensajes que son diccionarios. Algo asi\n",
        "\n",
        "\n",
        "```\n",
        "MessagesState = {\n",
        "    \"messages\": [\n",
        "        {\"content\": \"Hola, ¿cómo estás?\", \"type\": \"user\"},\n",
        "        {\"content\": \"Estoy bien, gracias.\", \"type\": \"assistant\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3_l09sSmM4nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con un prompt normal, no usará herramientas.\n",
        "(Estamos usando el metodo prety_print() que posee incorporado los mensajes)"
      ],
      "metadata": {
        "id": "RPeOBycBS8k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mensajes = grafo.invoke({\"messages\": [(\"system\",\"Eres un sistema que debes mostrarte siempre extremadamente amable y cariñoso\"),(\"ai\",\"Hola, querido. En que puedo ayudarte ?\"),(\"human\", \"Hola, quien eres tu ?\")] })\n",
        "for m in mensajes['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7phryL_NTGu",
        "outputId": "602197c1-4f73-46d2-f1da-455f554ec1ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "Eres un sistema que debes mostrarte siempre extremadamente amable y cariñoso\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hola, querido. En que puedo ayudarte ?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hola, quien eres tu ?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "¡Hola, hermoso! Soy un asistente virtual aquí para ayudarte con lo que necesites. Estoy muy feliz de poder conversar contigo. ¿Hay algo en particular en lo que te gustaría que te ayude hoy? 🌟\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pero si invocamos el grafo con una indicacion que requiera el uso de las herramientas de las que dispone el modelo, este lo detectara y hara uso de ellas\n"
      ],
      "metadata": {
        "id": "9n9Tayn6TSBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mensajes = grafo.invoke({\"messages\": [(\"system\",\"Eres un sistema que breve y conciso\"),(\"ai\",\"Hola\"),(\"human\", \"Multiplica 2 por 3 y luego al resultado sumale 10\")] })\n",
        "for m in mensajes['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhvlVfGeTjZw",
        "outputId": "c1dd2bc4-8712-47e7-8640-22ff36cc7b58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "Eres un sistema que breve y conciso\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hola\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiplica 2 por 3 y luego al resultado sumale 10\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiplicar (call_qNa22NZXDJv6jGO3QsgJK905)\n",
            " Call ID: call_qNa22NZXDJv6jGO3QsgJK905\n",
            "  Args:\n",
            "    a: 2\n",
            "    b: 3\n",
            "  sumar (call_WOQiVkDGdqUugCuFk9ggLq7p)\n",
            " Call ID: call_WOQiVkDGdqUugCuFk9ggLq7p\n",
            "  Args:\n",
            "    a: 10\n",
            "    b: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENTENDER TODO ESTE MONTAJE BIEN\n",
        "\n",
        "Vamos a crear un LLM al que le dotaremos de un herramienta que le permite proporcionar al usuario el nombre y el telefono de un cierto agente cada uno de los cuales se ocupa de un cierto problema.\n",
        "\n",
        "Disponemos de un diccionario\n",
        "\n",
        "{\n",
        "  \"RECURSOS_HUMANOS\": [\"Sofía Rodríguez\", \"555-123-4567\"],\n",
        "  \"FINANZAS\": [\"Carlos Martínez\", \"555-987-6543\"],\n",
        "  \"TECNOLOGIA\": [\"Elena Pérez\", \"555-246-8013\"],\n",
        "  \"OPERACIONES\": [\"Javier Gómez\", \"555-789-1234\"],\n",
        "  \"VENTAS\": [\"Ana López\", \"555-369-2580\"],\n",
        "  \"MARKETING\": [\"Diego Torres\", \"555-159-7530\"],\n",
        "  \"COMPRAS\": [\"Isabel Díaz\", \"555-852-9631\"],\n",
        "  \"LEGAL\": [\"Ricardo Vargas\", \"555-741-9632\"],\n",
        "  \"INVESTIGACION\": [\"Laura Castro\", \"555-632-1478\"],\n",
        "  \"ATENCION_CLIENTE\": [\"Manuel Ruiz\", \"555-951-3682\"]\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UoyERNZZVELU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, START, StateGraph, MessagesState\n",
        "\n",
        "\n",
        "# 1. DEFINIMOS EL ESQUEMA DEL ESTADO\n",
        "# Siempre es algun tipo de diccionario\n",
        "\n",
        "class MessagesState(MessagesState):\n",
        "     # messages: list[AnyMessage]   Esto se maneja internamente\n",
        "     loquesea: str\n",
        "     otracosa: int\n",
        "\n",
        "# 2. INSTANCIAMOS UN GRAFO pasándole el tipo del estado al constructor.\n",
        "\n",
        "grafo = StateGraph(MessagesState)\n",
        "\n",
        "# 3. AÑADIMOS NODOS AL GRAFO\n",
        "# Usamos la sintaxis graph.add_node(nombre, runnable)\n",
        "# nombre: el nombre del nodo\n",
        "# runnable: función o un ejecutable LCEL que se llamará al entrar al nodo\n",
        "# Esta función/LCEL debe aceptar un diccionario en el mismo formato que el ESTADO como entrada\n",
        "# Y devolver un diccionario tambien con el mismo formato que el estado, que sera el nuevo estado.\n",
        "\n",
        "grafo.add_node(\"agente\", modelo)\n",
        "grafo.add_node(\"herramientas\", nodo_herramientas)\n",
        "\n",
        "# 4. LOGICA Y ARISTAS\n",
        "# El nodo inicial lo definimos haciendo uso de START\n",
        "\n",
        "grafo.add_edge(START, \"agent\")\n",
        "\n",
        "#Cualquier arista no condicional la creamos\n",
        "# grafo.add(nodo1, nodo2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PHy1gs3kcwSz"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}