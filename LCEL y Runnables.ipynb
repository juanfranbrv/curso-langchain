{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe52+navFOLE14faYXkGhm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/LCEL%20y%20Runnables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. LCEL, LangChain Expression Language**\n",
        "---\n",
        "\n",
        "LCEL se introdujo en Langchain **a mediados de 2023**, específicamente con la **versión 0.0.142**, lanzada el **19 de julio de 2023**\n",
        "\n",
        "La introducción de **LCEL** en Langchain fue una respuesta a la necesidad de una forma más potente, flexible y fácil de usar para construir aplicaciones de lenguaje complejas. Proporcionó una sintaxis declarativa, mejoró la legibilidad, facilitó la depuración y habilitó funcionalidades avanzadas como el streaming y la ejecución asíncrona, consolidándose como una pieza fundamental del ecosistema de Langchain.\n",
        "\n",
        "**LCEL se basa en la Composición, no en Tipos Predefinidos:**\n",
        "\n",
        "-   **Operador Pipe (|):** La piedra angular de LCEL es el operador pipe. Este operador te permite encadenar componentes de forma secuencial, enviando la salida de un componente como entrada al siguiente. Esto es inherentemente secuencial, pero no se define como un tipo de cadena \"secuencial\".\n",
        "    \n",
        "-   **Primitivas Runnable:** LCEL se basa en la interfaz Runnable. Cualquier objeto que implemente esta interfaz puede ser parte de una cadena LCEL. Esto incluye modelos de lenguaje, prompts, parsers, retrievers, etc.\n",
        "    \n",
        "-   **Flexibilidad Total:** La clave es que puedes combinar estas primitivas Runnable de cualquier manera que tenga sentido para tu aplicación. No estás limitado a estructuras predefinidas.\n",
        "\n",
        "**Ventajas de este enfoque:**\n",
        "\n",
        "-   **Mayor Flexibilidad:** No estás limitado por las estructuras predefinidas. Puedes crear flujos de trabajo exactamente como los necesitas.\n",
        "    \n",
        "-   **Reutilización de Componentes:** Los componentes individuales pueden ser reutilizados en diferentes cadenas con diferentes flujos de trabajo.\n",
        "    \n",
        "-   **Claridad y Composición:** El uso del operador pipe hace que la lógica de la cadena sea más clara y fácil de entender.\n",
        "    \n",
        "-   **Optimización:** La forma en que construyes la cadena influye en cómo se puede optimizar su ejecución (por ejemplo, para paralelismo).\n",
        "    \n",
        "\n",
        "**En resumen, LCEL te proporciona las herramientas y la sintaxis para orquestar tus flujos de trabajo de manera flexible y poderosa. En lugar de imponer tipos de cadenas predefinidos, te da la libertad de construir las cadenas que mejor se adapten a tus necesidades, implementando patrones secuenciales, condicionales o paralelos según sea necesario.**\n",
        "\n",
        "Y todo esta abstración se basa en los **Runnables**"
      ],
      "metadata": {
        "id": "p9n7xOvQEQ6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Runnables**\n",
        "---\n",
        "\n",
        "Un *“Runnable”* en LangChain es una abstracción (una interfaz en código) que define un contrato para ejecutar una operación. Concretamente, un `Runnable` expone un método `invoke(input)`, el cual recibe un dato de entrada y produce un resultado de salida. Así, cualquier clase u objeto que cumpla con esta interfaz puede encadenarse o combinarse con otros `Runnables` para construir flujos de trabajo complejos (por ejemplo, secuencias o ejecuciones en paralelo).   \n",
        "\n",
        "Cada \"*Runnable*\" implementa métodos como invoke, batch, stream, y transform, lo que lo hace compatible con diferentes modos de ejecución (sincrónica, asincrónica, en lote, etc.).  \n",
        "Esto permite orquestar y reutilizar fácilmente distintas operaciones dentro de LangChain.\n",
        "\n",
        "Un poco menos técnico...\n",
        "\n",
        "Un *“Runnable”* es como una función que sabe hacer algo muy concreto: recibe cierta información y devuelve un resultado. Lo importante es que, en LangChain, todos los *“Runnables”* siguen la misma “forma de trabajar”. Gracias a esto, podemos ir uniendo varios “Runnables” uno tras otro o en paralelo para crear procesos más grandes sin tener que hacer ajustes complicados. En otras palabras, si cada bloque (*Runnable*) sabe cómo recibir datos y devolverlos transformados, podemos combinar esos bloques como si fueran piezas de LEGO para armar flujos de trabajo completos.\n",
        "\n",
        "## Conceptos clave de runnables\n",
        "-   **Modularidad**\n",
        "    \n",
        "    -   Cada Runnable representa una única tarea u operación (p.ej., ejecutar un modelo, procesar datos, encadenar operaciones).\n",
        "    -   Su diseño en “bloques” facilita la independencia y el intercambio de componentes.\n",
        "-   **Componibilidad**\n",
        "    \n",
        "    -   Varios Runnables pueden vincularse para formar canalizaciones o flujos de trabajo complejos.\n",
        "    -   Esto permite crear soluciones más grandes a partir de piezas pequeñas, flexibles y reutilizables.\n",
        "-   **Reutilizabilidad**\n",
        "    \n",
        "    -   Una vez definido, un Runnable se puede integrar en distintos proyectos sin modificaciones.\n",
        "    -   Es ideal para tareas estándar que se repiten (p.ej., preprocesamiento de datos).\n",
        "-   **Ejecución asincrónica**\n",
        "    \n",
        "    -   Los Runnables pueden ejecutarse de forma asíncrona, optimizando recursos y tiempo, especialmente cuando hay llamadas a servicios externos o E/S de por medio.\n",
        "\n",
        "-   **Ejecución paralela**\n",
        "    \n",
        "    -   Es posible configurar Runnables para que se ejecuten en paralelo, lo que mejora el rendimiento en tareas por lotes o cuando se manejan grandes volúmenes de datos.\n",
        "-   **Manejo de errores**\n",
        "    \n",
        "    -   Suelen incluir mecanismos para capturar y gestionar excepciones, reforzando la solidez del flujo de trabajo.\n",
        "-   **Registro y depuración**\n",
        "    \n",
        "    -   Admiten el registro de metadatos y eventos, lo que facilita rastrear y depurar la ejecución de principio a fin.\n"
      ],
      "metadata": {
        "id": "TXNoJRERJ5l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Chains (Cadenas)**\n",
        "---\n",
        "\n",
        "En Langchain, una Cadena (Chain) representa una secuencia orquestada de llamadas a uno o más Runnables para realizar una tarea específica. Piensa en ella como una tubería o un flujo de trabajo donde la salida de un Runnable se convierte en la entrada del siguiente.  \n",
        "\n",
        "En esencia, una Cadena combina la funcionalidad de múltiples Runnables para llevar a cabo procesos más complejos que los que un solo Runnable podría manejar individualmente. Las Cadenas son el mecanismo principal en Langchain para construir aplicaciones de lenguaje natural con lógica y pasos definidos.\n",
        "\n",
        "En LCEL no existen tipos de cadenas predefinidos como \"secuenciales\", \"condicionales\" o \"paralelas\". Sin embargo, puedes implementar estos patrones de flujo de trabajo utilizando la sintaxis de LCEL junto con funciones adicionales.  "
      ],
      "metadata": {
        "id": "dVHiL2gmcWJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. ¡Uniendo las piezas: Llegó la hora del código!**\n",
        "---\n",
        "\n",
        "En esta sección, pondremos en práctica los conceptos teóricos que hemos explorado. A través de una serie de ejemplos con código, ilustraremos cómo LCEL, los Runnables y las Cadenas se combinan en Langchain para resolver tareas concretas. Nuestro enfoque será didáctico, facilitando la comprensión a la vez que presentamos casos de uso reales.\n",
        "\n",
        "** Langchain va en este momento por la version 0.3. Dado que es una librería en evolución, es posible que se agreguen o refinen más “runnables” en versiones futuras.\n",
        "\n"
      ],
      "metadata": {
        "id": "fG7vAQWRR8pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "Uv1UDKaYSP4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Prompt + LLM**\n",
        "\n",
        "---\n",
        "Posiblemente esta es la cadena mas simple que podamos crear.  \n",
        "En este caso la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo.\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Observa que esto es lo que hemos hecho hasta ahora sin cadenas: Invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia y este enfoque puede ser más simple para tareas sencillas, carece de la flexibilidad y la extensibilidad que ofrecen las cadenas como se aprecia en cuanto la complejidad escala.\n",
        "\n",
        "Ademas LCEL y la nuevas cadenas proporcionan un nivel de abstracción mayor tanto del prompt como del proceso en general."
      ],
      "metadata": {
        "id": "3r0n1rImzRrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos el prompt template de una de esta dos formas\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¿cómo estás?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Francés\", \"texto\":\"Hola, ¿cómo estás?\"})\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "zfkcPT99zsbr",
        "outputId": "846ce421-6f78-44af-adc6-2e14ebcf076c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hola, ¿cómo estás? se traduce al francés como: \"Bonjour, comment ça va ?\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Prompt + LLM + funcion personalizada de transformación de la salida**\n",
        "---\n",
        "La forma principal de añadir una función a una cadena en Langchain (especialmente en LCEL) es utilizando el `RunnableLambda`. `RunnableLambda` es un Runnable que envuelve una función Python, permitiéndole integrarse perfectamente en el flujo de la cadena.\n",
        "\n",
        "(Aunque es posible, la inserción directa de la función `transformar_texto` funciona, esto se debe a que Langchain implícitamente envuelve esa función en un RunnableLambda \"por debajo del capó\" para que pueda encajar dentro del paradigma de la cadena LCEL. Es mejor ser consistente con los principios del framework y usar `RunnableLambda`)\n",
        "\n",
        "En este caso la salida del LLM se procesa con una funcion sencilla para pasarla a mayusculas. (pero puede ser tan complicada como se necesite)\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un objeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar.\n",
        "\n"
      ],
      "metadata": {
        "id": "6KCFlW3j1zWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# # Definimos una función de transformación\n",
        "# def transformar_texto(output):\n",
        "#     return output.content.upper()\n",
        "\n",
        "\n",
        "transformar_texto=RunnableLambda(lambda output: output.content.upper())\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N-YvGoOb16Aa",
        "outputId": "e1258f3b-8b29-4645-c533-b4be8088fd7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "EL OVERFITTING, O SOBREAJUSTE, ES UN FENÓMENO EN EL QUE UN MODELO DE MACHINE LEARNING SE AJUSTA DEMASIADO A LOS DATOS DE ENTRENAMIENTO, CAPTURANDO NO SOLO LAS RELACIONES SUBYACENTES, SINO TAMBIÉN EL RUIDO Y LAS VARIACIONES ALEATORIAS EN ESOS DATOS. COMO RESULTADO, AUNQUE EL MODELO PUEDE MOSTRAR UN RENDIMIENTO EXCEPCIONAL EN EL CONJUNTO DE ENTRENAMIENTO, SU CAPACIDAD PARA GENERALIZAR A DATOS NO VISTOS (COMO UN CONJUNTO DE PRUEBA O EN PRODUCCIÓN) SE VE SERIAMENTE COMPROMETIDA, LO QUE LLEVA A UN RENDIMIENTO DEFICIENTE.\n\n### CAUSAS DEL OVERFITTING:\n1. **MODELO COMPLEJO:** UN MODELO CON DEMASIADOS PARÁMETROS O UNA ARQUITECTURA MUY COMPLEJA PUEDE APRENDER PATRONES ESPECÍFICOS DE LOS DATOS DE ENTRENAMIENTO.\n2. **DATOS INSUFICIENTES:** CON UN TAMAÑO DE MUESTRA PEQUEÑO, ES MÁS PROBABLE QUE EL MODELO CAPTE EL RUIDO EN LUGAR DE LAS TENDENCIAS GENERALES.\n3. **RUIDO EN LOS DATOS:** LA PRESENCIA DE ERRORES O VARIACIONES ALEATORIAS PUEDE LLEVAR AL MODELO A AJUSTARSE A ESTAS IRREGULARIDADES.\n\n### SÍNTOMAS DEL OVERFITTING:\n- ALTA PRECISIÓN EN EL CONJUNTO DE ENTRENAMIENTO.\n- BAJA PRECISIÓN EN EL CONJUNTO DE VALIDACIÓN O PRUEBA.\n- UN MODELO QUE PARECE \"MEMORIZAR\" LOS DATOS EN LUGAR DE APRENDER PATRONES SIGNIFICATIVOS.\n\n### ESTRATEGIAS PARA MITIGAR EL OVERFITTING:\n1. **REGULARIZACIÓN:** MÉTODOS COMO L1 (LASSO) O L2 (RIDGE) QUE AÑADEN UN TÉRMINO DE PENALIZACIÓN AL COSTO DEL MODELO PARA EVITAR QUE LOS PARÁMETROS SE VUELVAN DEMASIADO GRANDES.\n2. **VALIDACIÓN CRUZADA:** UTILIZAR TÉCNICAS DE VALIDACIÓN CRUZADA PARA ASEGURARSE DE QUE EL MODELO GENERALIZA BIEN A DIFERENTES SUBCONJUNTOS DE DATOS.\n3. **REDUCCIÓN DE COMPLEJIDAD:** OPTAR POR MODELOS MÁS SIMPLES O REDUCIR EL NÚMERO DE CARACTERÍSTICAS MEDIANTE TÉCNICAS DE SELECCIÓN DE CARACTERÍSTICAS.\n4. **AUMENTO DE DATOS:** GENERAR DATOS SINTÉTICOS O UTILIZAR TÉCNICAS DE AUMENTO DE DATOS PARA INCREMENTAR EL TAMAÑO DEL CONJUNTO DE ENTRENAMIENTO.\n5. **EARLY STOPPING:** MONITOREAR EL RENDIMIENTO DEL MODELO EN UN CONJUNTO DE VALIDACIÓN Y DETENER EL ENTRENAMIENTO CUANDO EL RENDIMIENTO COMIENZA A DETERIORARSE.\n\nEN RESUMEN, EL OVERFITTING ES UN DESAFÍO CRÍTICO EN EL DESARROLLO DE MODELOS DE MACHINE LEARNING Y REQUIERE ATENCIÓN CUIDADOSA PARA ASEGURAR QUE LOS MODELOS SEAN ROBUSTOS Y CAPACES DE GENERALIZAR A DATOS NO VISTOS."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prompt + LLM + OutputParser + funcion personalizada de transformación de la salida**\n",
        "---\n",
        "\n",
        "En este ejemplo se observa mejor la modularidad reutilizando la cadena en un bucle y usando un OutputParser.  \n",
        "\n",
        "**Los OutputParser en Langchain son Runnables**. Esto les permite ser componentes activos dentro de las cadenas LCEL, tomando la salida de Runnables precedentes y transformándola según su lógica específica. Su capacidad para ser Runnables es fundamental para la flexibilidad y el poder de composición de Langchain."
      ],
      "metadata": {
        "id": "ZdMVH9kL6uOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una función de transformación\n",
        "# Esta función toma la salida del OutPutParser (str) y la convierte a mayúsculas.\n",
        "transformar_texto=RunnableLambda(lambda output: output.upper())\n",
        "\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas en un bucle\n",
        "# Observa cómo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¿Quién fue Napoleón Bonaparte?\"},\n",
        "    {\"tema\": \"Programación\", \"pregunta\": \"¿Qué es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "VKkDds_W6ymg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Prompt + LLM + OutputParser + Guardar en fichero**\n",
        "Crea una cadena que genera una lista de elementos y la guarda en un archivo:\n",
        "\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "4yMTStcf-fPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Creamos la cadena con LCEL y RunnableLambda\n",
        "def guardar_en_archivo_y_mostrar(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Podriamos no hacer esto y meter la funcion directamente en la cadena\n",
        "# Metemos la funcion en un Runnable, reutilizamos la variable\n",
        "guardar_en_archivo_y_mostrar = RunnableLambda(guardar_en_archivo_y_mostrar)\n",
        "\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo_y_mostrar\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en 'mente'\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "aA2_DRpA9gbe",
        "outputId": "50e408d7-ea15-41cd-8e72-fcc79f781be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rápidamente  \n",
            "suavemente  \n",
            "fácilmente  \n",
            "silenciosamente  \n",
            "amablemente  \n",
            "eficazmente  \n",
            "despacio  \n",
            "claro  \n",
            "junto  \n",
            "bien\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Respuesta guardada en 'respuesta.txt'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + JsonOutputParser**\n",
        "---\n",
        "\n",
        "Esta vez genera una respuesta JSON.\n",
        "\n",
        "SimpleJsonOutputParser transforma la salida del modelo en un objeto JSON válido."
      ],
      "metadata": {
        "id": "-oRstVkU_SOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install icecream -qU\n",
        "from icecream import ic\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Resultado(BaseModel):\n",
        "    persona: str = Field(description=\"Nombre de la persona\")\n",
        "    aportacion: str = Field(description=\"Importe en euros\")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve el resultado en formato JSON con las claves 'persona' y 'aportación'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\"})\n",
        "ic(result)"
      ],
      "metadata": {
        "id": "ueUefLwJ_2Ki",
        "outputId": "61284536-2d55-40a8-e18d-2473679f9171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| result: {'personas': [{'aportación': 1500, 'persona': 'Lucas Martínez'},\n",
            "                          {'aportación': 2000, 'persona': 'Sofía Rodríguez'},\n",
            "                          {'aportación': 1200, 'persona': 'Martín Gómez'},\n",
            "                          {'aportación': 1800, 'persona': 'Isabel López'},\n",
            "                          {'aportación': 2500, 'persona': 'Javier Pérez'},\n",
            "                          {'aportación': 1600, 'persona': 'Clara Herrera'},\n",
            "                          {'aportación': 2200, 'persona': 'Diego Fernández'},\n",
            "                          {'aportación': 1300, 'persona': 'Ana Sánchez'},\n",
            "                          {'aportación': 1700, 'persona': 'David Ramírez'},\n",
            "                          {'aportación': 1400, 'persona': 'Lucía Torres'}]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'personas': [{'persona': 'Lucas Martínez', 'aportación': 1500},\n",
              "  {'persona': 'Sofía Rodríguez', 'aportación': 2000},\n",
              "  {'persona': 'Martín Gómez', 'aportación': 1200},\n",
              "  {'persona': 'Isabel López', 'aportación': 1800},\n",
              "  {'persona': 'Javier Pérez', 'aportación': 2500},\n",
              "  {'persona': 'Clara Herrera', 'aportación': 1600},\n",
              "  {'persona': 'Diego Fernández', 'aportación': 2200},\n",
              "  {'persona': 'Ana Sánchez', 'aportación': 1300},\n",
              "  {'persona': 'David Ramírez', 'aportación': 1700},\n",
              "  {'persona': 'Lucía Torres', 'aportación': 1400}]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "8nDjGaPbD2NU",
        "outputId": "c2f836a2-79bd-453a-cf08-1253b1bc50c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'personas': [{'persona': 'Lucas Martínez', 'aportación': 1500},\n",
              "  {'persona': 'Sofía Rodríguez', 'aportación': 2000},\n",
              "  {'persona': 'Martín Gómez', 'aportación': 1200},\n",
              "  {'persona': 'Isabel López', 'aportación': 1800},\n",
              "  {'persona': 'Javier Pérez', 'aportación': 2500},\n",
              "  {'persona': 'Clara Herrera', 'aportación': 1600},\n",
              "  {'persona': 'Diego Fernández', 'aportación': 2200},\n",
              "  {'persona': 'Ana Sánchez', 'aportación': 1300},\n",
              "  {'persona': 'David Ramírez', 'aportación': 1700},\n",
              "  {'persona': 'Lucía Torres', 'aportación': 1400}]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://python.langchain.com/docs/concepts/runnables/\n",
        "\n",
        "2. Info oficial: https://python.langchain.com/api_reference/core/runnables.html\n",
        "3. https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
        "\n",
        "4. https://dzone.com/articles/guide-to-langchain-runnable-architecture\n",
        "\n",
        "5. https://medium.com/@danushidk507/runnables-in-langchain-e6bfb7b9c0ca\n",
        "\n",
        "6. https://www.youtube.com/watch?v=8aUYzb1aYDU\n",
        "\n",
        "7. https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac\n",
        "\n",
        "8. https://medium.com/@ulrichw/list/langchain-lcel-85af4f4ff883\n",
        "\n",
        "9. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "10. https://www.pinecone.io/learn/series/langchain/langchain-expression-language/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJd-xV-oMuhQ"
      }
    }
  ]
}