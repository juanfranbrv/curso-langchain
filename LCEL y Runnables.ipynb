{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBQYeoZAlmqwNtnII/j7Q0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/LCEL%20y%20Runnables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. LCEL, LangChain Expression Language**\n",
        "---\n",
        "\n",
        "LCEL se introdujo en Langchain **a mediados de 2023**, espec√≠ficamente con la **versi√≥n 0.0.142**, lanzada el **19 de julio de 2023**\n",
        "\n",
        "La introducci√≥n de **LCEL** en Langchain fue una respuesta a la necesidad de una forma m√°s potente, flexible y f√°cil de usar para construir aplicaciones de lenguaje complejas. Proporcion√≥ una sintaxis declarativa, mejor√≥ la legibilidad, facilit√≥ la depuraci√≥n y habilit√≥ funcionalidades avanzadas como el streaming y la ejecuci√≥n as√≠ncrona, consolid√°ndose como una pieza fundamental del ecosistema de Langchain.\n",
        "\n",
        "**LCEL se basa en la Composici√≥n, no en Tipos Predefinidos:**\n",
        "\n",
        "-   **Operador Pipe (|):** La piedra angular de LCEL es el operador pipe. Este operador te permite encadenar componentes de forma secuencial, enviando la salida de un componente como entrada al siguiente. Esto es inherentemente secuencial, pero no se define como un tipo de cadena \"secuencial\".\n",
        "    \n",
        "-   **Primitivas Runnable:** LCEL se basa en la interfaz Runnable. Cualquier objeto que implemente esta interfaz puede ser parte de una cadena LCEL. Esto incluye modelos de lenguaje, prompts, parsers, retrievers, etc.\n",
        "    \n",
        "-   **Flexibilidad Total:** La clave es que puedes combinar estas primitivas Runnable de cualquier manera que tenga sentido para tu aplicaci√≥n. No est√°s limitado a estructuras predefinidas.\n",
        "\n",
        "**Ventajas de este enfoque:**\n",
        "\n",
        "-   **Mayor Flexibilidad:** No est√°s limitado por las estructuras predefinidas. Puedes crear flujos de trabajo exactamente como los necesitas.\n",
        "    \n",
        "-   **Reutilizaci√≥n de Componentes:** Los componentes individuales pueden ser reutilizados en diferentes cadenas con diferentes flujos de trabajo.\n",
        "    \n",
        "-   **Claridad y Composici√≥n:** El uso del operador pipe hace que la l√≥gica de la cadena sea m√°s clara y f√°cil de entender.\n",
        "    \n",
        "-   **Optimizaci√≥n:** La forma en que construyes la cadena influye en c√≥mo se puede optimizar su ejecuci√≥n (por ejemplo, para paralelismo).\n",
        "    \n",
        "\n",
        "**En resumen, LCEL te proporciona las herramientas y la sintaxis para orquestar tus flujos de trabajo de manera flexible y poderosa. En lugar de imponer tipos de cadenas predefinidos, te da la libertad de construir las cadenas que mejor se adapten a tus necesidades, implementando patrones secuenciales, condicionales o paralelos seg√∫n sea necesario.**\n",
        "\n",
        "Y todo esta abstraci√≥n se basa en los **Runnables**"
      ],
      "metadata": {
        "id": "p9n7xOvQEQ6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Runnables**\n",
        "---\n",
        "\n",
        "Un *‚ÄúRunnable‚Äù* en LangChain es una abstracci√≥n (una interfaz en c√≥digo) que define un contrato para ejecutar una operaci√≥n. Concretamente, un `Runnable` expone un m√©todo `invoke(input)`, el cual recibe un dato de entrada y produce un resultado de salida. As√≠, cualquier clase u objeto que cumpla con esta interfaz puede encadenarse o combinarse con otros `Runnables` para construir flujos de trabajo complejos (por ejemplo, secuencias o ejecuciones en paralelo).   \n",
        "\n",
        "Cada \"*Runnable*\" implementa m√©todos como invoke, batch, stream, y transform, lo que lo hace compatible con diferentes modos de ejecuci√≥n (sincr√≥nica, asincr√≥nica, en lote, etc.).  \n",
        "Esto permite orquestar y reutilizar f√°cilmente distintas operaciones dentro de LangChain.\n",
        "\n",
        "Un poco menos t√©cnico...\n",
        "\n",
        "Un *‚ÄúRunnable‚Äù* es como una funci√≥n que sabe hacer algo muy concreto: recibe cierta informaci√≥n y devuelve un resultado. Lo importante es que, en LangChain, todos los *‚ÄúRunnables‚Äù* siguen la misma ‚Äúforma de trabajar‚Äù. Gracias a esto, podemos ir uniendo varios ‚ÄúRunnables‚Äù uno tras otro o en paralelo para crear procesos m√°s grandes sin tener que hacer ajustes complicados. En otras palabras, si cada bloque (*Runnable*) sabe c√≥mo recibir datos y devolverlos transformados, podemos combinar esos bloques como si fueran piezas de LEGO para armar flujos de trabajo completos.\n",
        "\n",
        "## Conceptos clave de runnables\n",
        "-   **Modularidad**\n",
        "    \n",
        "    -   Cada Runnable representa una √∫nica tarea u operaci√≥n (p.ej., ejecutar un modelo, procesar datos, encadenar operaciones).\n",
        "    -   Su dise√±o en ‚Äúbloques‚Äù facilita la independencia y el intercambio de componentes.\n",
        "-   **Componibilidad**\n",
        "    \n",
        "    -   Varios Runnables pueden vincularse para formar canalizaciones o flujos de trabajo complejos.\n",
        "    -   Esto permite crear soluciones m√°s grandes a partir de piezas peque√±as, flexibles y reutilizables.\n",
        "-   **Reutilizabilidad**\n",
        "    \n",
        "    -   Una vez definido, un Runnable se puede integrar en distintos proyectos sin modificaciones.\n",
        "    -   Es ideal para tareas est√°ndar que se repiten (p.ej., preprocesamiento de datos).\n",
        "-   **Ejecuci√≥n asincr√≥nica**\n",
        "    \n",
        "    -   Los Runnables pueden ejecutarse de forma as√≠ncrona, optimizando recursos y tiempo, especialmente cuando hay llamadas a servicios externos o E/S de por medio.\n",
        "\n",
        "-   **Ejecuci√≥n paralela**\n",
        "    \n",
        "    -   Es posible configurar Runnables para que se ejecuten en paralelo, lo que mejora el rendimiento en tareas por lotes o cuando se manejan grandes vol√∫menes de datos.\n",
        "-   **Manejo de errores**\n",
        "    \n",
        "    -   Suelen incluir mecanismos para capturar y gestionar excepciones, reforzando la solidez del flujo de trabajo.\n",
        "-   **Registro y depuraci√≥n**\n",
        "    \n",
        "    -   Admiten el registro de metadatos y eventos, lo que facilita rastrear y depurar la ejecuci√≥n de principio a fin.\n"
      ],
      "metadata": {
        "id": "TXNoJRERJ5l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Chains (Cadenas)**\n",
        "---\n",
        "\n",
        "En Langchain, una Cadena (Chain) representa una secuencia orquestada de llamadas a uno o m√°s Runnables para realizar una tarea espec√≠fica. Piensa en ella como una tuber√≠a o un flujo de trabajo donde la salida de un Runnable se convierte en la entrada del siguiente.  \n",
        "\n",
        "En esencia, una Cadena combina la funcionalidad de m√∫ltiples Runnables para llevar a cabo procesos m√°s complejos que los que un solo Runnable podr√≠a manejar individualmente. Las Cadenas son el mecanismo principal en Langchain para construir aplicaciones de lenguaje natural con l√≥gica y pasos definidos.\n",
        "\n",
        "En LCEL no existen tipos de cadenas predefinidos como \"secuenciales\", \"condicionales\" o \"paralelas\". Sin embargo, puedes implementar estos patrones de flujo de trabajo utilizando la sintaxis de LCEL junto con funciones adicionales.  "
      ],
      "metadata": {
        "id": "dVHiL2gmcWJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. ¬°Uniendo las piezas: Lleg√≥ la hora del c√≥digo!**\n",
        "---\n",
        "\n",
        "En esta secci√≥n, pondremos en pr√°ctica los conceptos te√≥ricos que hemos explorado. A trav√©s de una serie de ejemplos con c√≥digo, ilustraremos c√≥mo LCEL, los Runnables y las Cadenas se combinan en Langchain para resolver tareas concretas. Nuestro enfoque ser√° did√°ctico, facilitando la comprensi√≥n a la vez que presentamos casos de uso reales.\n",
        "\n",
        "** Langchain va en este momento por la version 0.3. Dado que es una librer√≠a en evoluci√≥n, es posible que se agreguen o refinen m√°s ‚Äúrunnables‚Äù en versiones futuras.\n",
        "\n"
      ],
      "metadata": {
        "id": "fG7vAQWRR8pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "- Instalamos la librer√≠a LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases espec√≠ficas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dej√°ndolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "Uv1UDKaYSP4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "\n",
        "# Instalar las librer√≠as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Prompt + LLM**\n",
        "\n",
        "---\n",
        "Posiblemente esta es la cadena mas simple que podamos crear.  \n",
        "En este caso la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo.\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Observa que esto es lo que hemos hecho hasta ahora sin cadenas: Invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia y este enfoque puede ser m√°s simple para tareas sencillas, carece de la flexibilidad y la extensibilidad que ofrecen las cadenas como se aprecia en cuanto la complejidad escala.\n",
        "\n",
        "Ademas LCEL y la nuevas cadenas proporcionan un nivel de abstracci√≥n mayor tanto del prompt como del proceso en general."
      ],
      "metadata": {
        "id": "3r0n1rImzRrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos el prompt template de una de esta dos formas\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¬øc√≥mo est√°s?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Franc√©s\", \"texto\":\"Hola, ¬øc√≥mo est√°s?\"})\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "zfkcPT99zsbr",
        "outputId": "1b19a18b-7155-4def-a32a-1db4a8789bdd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Bonjour, comment allez-vous ?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RunnableLambda**\n",
        "\n",
        "La forma principal de a√±adir una funci√≥n a una cadena en Langchain (especialmente en LCEL) es utilizando `RunnableLambda`. `RunnableLambda` es un Runnable que envuelve una funci√≥n Python, permiti√©ndole integrarse perfectamente en el flujo de la cadena.\n",
        "\n",
        "(Aunque es posible, la inserci√≥n directa de la funci√≥n `transformar_texto` funciona, esto se debe a que Langchain impl√≠citamente envuelve esa funci√≥n en un RunnableLambda \"por debajo del cap√≥\" para que pueda encajar dentro del paradigma de la cadena LCEL. Es mejor ser consistente con los principios del framework y usar `RunnableLambda`)\n"
      ],
      "metadata": {
        "id": "jYlj5VuMY9jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Prompt + LLM + funcion personalizada de transformaci√≥n de la salida**\n",
        "---\n",
        "\n",
        "En este caso la salida del LLM se procesa con una funcion sencilla para pasarla a mayusculas. (pero puede ser tan complicada como se necesite)\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un objeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar.\n",
        "\n"
      ],
      "metadata": {
        "id": "6KCFlW3j1zWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# # Definimos una funci√≥n de transformaci√≥n\n",
        "# def transformar_texto(output):\n",
        "#     return output.content.upper()\n",
        "\n",
        "\n",
        "transformar_texto=RunnableLambda(lambda output: output.content.upper())\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¬øQu√© es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N-YvGoOb16Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prompt + LLM + OutputParser + funcion personalizada de transformaci√≥n de la salida**\n",
        "---\n",
        "\n",
        "En este ejemplo se observa mejor la modularidad reutilizando la cadena en un bucle y usando un OutputParser.  \n",
        "\n",
        "**Los OutputParser en Langchain son Runnables**. Esto les permite ser componentes activos dentro de las cadenas LCEL, tomando la salida de Runnables precedentes y transform√°ndola seg√∫n su l√≥gica espec√≠fica. Su capacidad para ser Runnables es fundamental para la flexibilidad y el poder de composici√≥n de Langchain."
      ],
      "metadata": {
        "id": "ZdMVH9kL6uOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una funci√≥n de transformaci√≥n\n",
        "# Esta funci√≥n toma la salida del OutPutParser (str) y la convierte a may√∫sculas.\n",
        "transformar_texto=RunnableLambda(lambda output: output.upper())\n",
        "\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas en un bucle\n",
        "# Observa c√≥mo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¬øQu√© es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¬øQui√©n fue Napole√≥n Bonaparte?\"},\n",
        "    {\"tema\": \"Programaci√≥n\", \"pregunta\": \"¬øQu√© es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "VKkDds_W6ymg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Prompt + LLM + OutputParser + Guardar en fichero**\n",
        "Crea una cadena que genera una lista de elementos y la guarda en un archivo:\n",
        "\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "4yMTStcf-fPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No a√±adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Creamos la cadena con LCEL y RunnableLambda\n",
        "def guardar_en_archivo_y_mostrar(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Podriamos no hacer esto y meter la funcion directamente en la cadena\n",
        "# Metemos la funcion en un Runnable, reutilizamos la variable\n",
        "guardar_en_archivo_y_mostrar = RunnableLambda(guardar_en_archivo_y_mostrar)\n",
        "\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo_y_mostrar\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en 'mente'\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "aA2_DRpA9gbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + JsonOutputParser**\n",
        "---\n",
        "\n",
        "Esta vez genera una respuesta JSON.\n",
        "\n",
        "JsonOutputParser transforma la salida del modelo en un objeto JSON v√°lido."
      ],
      "metadata": {
        "id": "-oRstVkU_SOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve directamente un array JSON. Cada elemento del array debe ser un objeto JSON con las claves 'persona' y 'aportaci√≥n'. No a√±adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\"})\n",
        "\n",
        "result\n"
      ],
      "metadata": {
        "id": "ueUefLwJ_2Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo existe un OutputParser, PydanticOutputParser mucho mas robusto para estructurar la salida. Lo usaremos a continuaci√≥n.\n",
        "\n",
        "**Consideraciones:**\n",
        "\n",
        "-   **Dependencia del Prompt:** Con JsonOutputParser, la precisi√≥n de la salida depende mucho de la claridad y especificidad del prompt. Si el modelo no sigue exactamente las instrucciones o el prompt no es absolutamente claro, el parsing podr√≠a fallar.\n",
        "    \n",
        "-   **Menos validaci√≥n autom√°tica:** A diferencia de PydanticOutputParser, JsonOutputParser simplemente intenta parsear la salida como JSON. No valida si la estructura interna coincide con un esquema espec√≠fico.\n",
        "    \n",
        "-   **Manejo de la salida:** Despu√©s de obtener el JSON, necesitar√°s acceder a la lista de personas a trav√©s de la clave \"personas\" en el diccionario resultante.\n",
        "\n",
        "Si no quieres el diccionario contenedor con la clave \"personas\", sino directamente la lista de diccionarios, necesitas modificar el prompt para que le indique al modelo de lenguaje que genere directamente un array JSON.\n",
        "\n",
        "El prompt de sistema deberia ser algo asi:\n",
        "\n",
        "*Eres un asistente conciso y directo experto. Devuelve directamente un array JSON. Cada elemento del array debe ser un objeto JSON con las claves 'persona' y 'aportaci√≥n'. No a√±adas ningun tipo de comentario*\n",
        "    \n",
        "**El prompt tiene gran importancia**\n",
        "En resumen, si bien es posible usar JsonOutputParser para obtener una lista de objetos, requiere un prompt m√°s espec√≠fico para guiar al modelo a generar un √∫nico JSON con una lista dentro. PydanticOutputParser ofrece una soluci√≥n m√°s robusta y con validaci√≥n autom√°tica cuando se trabaja con estructuras de datos definidas.\n",
        "\n"
      ],
      "metadata": {
        "id": "nLziIjL4Qp64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 6: PydanticOutputParser**\n",
        "\n",
        "PydanticOutputParser es una herramienta dentro de Langchain que facilita la estructuraci√≥n de la salida de modelos de lenguaje en objetos Python definidos con Pydantic. En lugar de simplemente obtener texto plano, este parser te permite especificar un esquema de datos deseado (a trav√©s de una clase Pydantic) y autom√°ticamente convierte la respuesta del modelo en una instancia de esa clase. Esto asegura que la informaci√≥n extra√≠da tenga un formato consistente y predecible.\n",
        "\n",
        "Una de las principales ventajas de PydanticOutputParser es su capacidad de validaci√≥n. Al utilizar Pydantic, se aplican autom√°ticamente las validaciones definidas en el esquema, lo que ayuda a garantizar la calidad y la integridad de los datos extra√≠dos. Adem√°s, simplifica el manejo de la salida del modelo, ya que puedes acceder a los datos como atributos de un objeto Python en lugar de tener que parsear manualmente cadenas de texto o diccionarios JSON.\n",
        "\n",
        "* Pero el fallo en la validacion de la respuesta arrojaria una excepcionque habria que gestionar con alguna estructura del tipo TRY-EXCEPT para que nuestra app no se detuviera !!\n",
        "\n",
        "Finalmente, PydanticOutputParser mejora la claridad y mantenibilidad del c√≥digo. Al definir expl√≠citamente la estructura de la informaci√≥n esperada, se facilita la comprensi√≥n del flujo de datos y se reduce la probabilidad de errores al trabajar con la salida del modelo. Adem√°s, Langchain integra funcionalidades para generar instrucciones de formato para el modelo basadas en el esquema Pydantic, lo que ayuda a guiar al modelo para que produzca la salida en el formato correcto desde el principio.\n",
        "\n"
      ],
      "metadata": {
        "id": "_OpapnJUaeHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "# Definimos nuestra estructura de datos\n",
        "class Resultado(BaseModel):\n",
        "    persona: str = Field(description=\"Nombre de la persona\")\n",
        "    aportacion: str = Field(description=\"Importe en euros\")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve el resultado en formato JSON con las claves 'persona' y 'aportaci√≥n'. No a√±adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Usamos PydanticOutputParser y le pasamos la clase Resultado\n",
        "json_parser = PydanticOutputParser(pydantic_object=Resultado)\n",
        "\n",
        "# Incluimos las instrucciones de formato en el prompt\n",
        "prompt_con_formato = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve el resultado en formato JSON con las claves 'persona' y 'aportaci√≥n'. No a√±adas ningun tipo de comentario. \\n{format_instructions}\\n\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_con_formato | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena y gestionamos la excepci√≥n\n",
        "try:\n",
        "    result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\", \"format_instructions\": json_parser.get_format_instructions()})\n",
        "    print(result)\n",
        "except Exception as e:\n",
        "    print(f\"Se produjo una excepci√≥n: {e}\")\n",
        "    print(\"La validaci√≥n fall√≥ porque el modelo devolvi√≥ una lista de objetos, pero se esperaba un √∫nico objeto.\")"
      ],
      "metadata": {
        "id": "DN_R0XVealwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con unos cambios en el prompt y en las clases de validacion conseguimos obtener una lista JSON correcta"
      ],
      "metadata": {
        "id": "VQKA4rNFezcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "# Definimos la estructura de datos para UN resultado\n",
        "class Resultado(BaseModel):\n",
        "    persona: str = Field(description=\"Nombre de la persona\")\n",
        "    aportacion: str = Field(description=\"Importe en euros\")\n",
        "\n",
        "# Definimos una nueva estructura para una LISTA de resultados\n",
        "class ListaResultados(BaseModel):\n",
        "    resultados: List[Resultado]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve un array JSON con una lista de objetos, donde cada objeto tiene las claves 'persona' y 'aportaci√≥n'. No a√±adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Usamos PydanticOutputParser y le pasamos la clase ListaResultados\n",
        "json_parser = PydanticOutputParser(pydantic_object=ListaResultados)\n",
        "\n",
        "# Incluimos las instrucciones de formato en el prompt\n",
        "prompt_con_formato = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve un array JSON con una lista de objetos, donde cada objeto tiene las claves 'persona' y 'aportaci√≥n'. No a√±adas ningun tipo de comentario. \\n{format_instructions}\\n\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_con_formato | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "try:\n",
        "    result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\", \"format_instructions\": json_parser.get_format_instructions()})\n",
        "    print(result)\n",
        "    # Ahora puedes acceder a la lista de resultados as√≠:\n",
        "    for item in result.resultados:\n",
        "        print(f\"Persona: {item.persona}, Aportaci√≥n: {item.aportacion}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Se produjo una excepci√≥n: {e}\")\n",
        "    print(\"La validaci√≥n fall√≥. Aseg√∫rate de que el modelo devuelve una lista de objetos con la estructura correcta.\")"
      ],
      "metadata": {
        "id": "K4oB0BK8e9kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 7: Cadena con dos LLMs (Generaci√≥n y Resumen)**\n",
        "\n",
        "Este ejemplo nos permitir presentar algunas caracteristicas mas de LCEL.\n",
        "Presentaremos diversas variaciones del mismo.\n",
        "\n",
        "Vamos a crear una cadena donde un LLM genera contenido y otro LLM resumen ese contenido.\n",
        "\n",
        "Posiblemnte el modo mas facil, segun vamos haciendo, sea este. Pero observa que de esta forma no tenemos acceso al texto del articulo que se genera dentro de la cadena y que se pasa a resumir. Solo accedemos al resultado final de la cadena, el texto resumido."
      ],
      "metadata": {
        "id": "OIEXdOhEmn1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primer LLM: Genera contenido\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un art√≠culo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# Segundo LLM: Resumen del contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente art√≠culo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = generate_prompt | llm1 | summarize_prompt | llm2\n",
        "\n",
        "# Ejecutar la cadena\n",
        "salida = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "display(Markdown(salida.content))\n",
        ""
      ],
      "metadata": {
        "id": "l6-lBATTmoWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para mostrar tanto el texto extenso (art√≠culo completo) como el texto resumido usando el enfoque moderno de cadenas en LangChain (LCEL), lo m√°s sencillo es separar la generaci√≥n en dos pasos en lugar de construir una √∫nica cadena que sobrescriba la salida intermedia. As√≠ podemos capturar y reutilizar la respuesta completa de tu primer LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "OCOydiQXuLXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Primer LLM: genera contenido\n",
        "llm1 = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un art√≠culo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# 2) Segundo LLM: resume el contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente art√≠culo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# --- EJECUCI√ìN EN DOS PASOS ---\n",
        "\n",
        "# Paso A: Generar el art√≠culo\n",
        "article_chain = generate_prompt | llm1\n",
        "article_result = article_chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "\n",
        "# Paso B: Resumir el art√≠culo\n",
        "summary_chain = summarize_prompt | llm2\n",
        "summary_result = summary_chain.invoke({\"article\": article_result.content})\n",
        "\n",
        "# Mostramos ambos resultados\n",
        "display(Markdown(\"## Texto extenso (Art√≠culo completo)\"))\n",
        "display(Markdown(article_result.content))\n",
        "\n",
        "display(Markdown(\"## Resumen\"))\n",
        "display(Markdown(summary_result.content))"
      ],
      "metadata": {
        "id": "6ob4RIOGvlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RunnableParallel**\n",
        "\n",
        "RunnableParallel es un **Runnable** que te permite ejecutar **varios Runnables diferentes simult√°neamente (en paralelo)**. Piensa en √©l como un coordinador que lanza varias tareas al mismo tiempo y luego re√∫ne los resultados.\n",
        "\n",
        "**¬øC√≥mo funciona?**\n",
        "\n",
        "- **Recibe un diccionario de Runnables:** Se le proporciona un diccionario donde las **claves son nombres descriptivos** y los **valores son los Runnables** que quieres ejecutar en paralelo.\n",
        "    \n",
        "- **Ejecuta los Runnables en paralelo:** LangChain se encarga de ejecutar todos esos Runnables al mismo tiempo (o lo m√°s simult√°neamente posible).\n",
        "    \n",
        "- **Devuelve un diccionario de resultados:** El resultado de RunnableParallel es un diccionario donde las **claves son las mismas que proporcionaste** y los **valores son los resultados de la ejecuci√≥n de cada Runnable**.\n",
        "\n",
        "Es especialmente √∫til cuando necesitas realizar diferentes transformaciones o procesamientos **sobre los mismos datos**.\n",
        "\n",
        "1. **Input compartido:** El mismo input se pasa a todas las tareas definidas en el paralelo.  \n",
        "\n",
        "2. **Ejecuci√≥n simult√°nea:** Todas las tareas se ejecutan al mismo tiempo.\n",
        "3. **Salida combinada:** Devuelve un diccionario donde cada clave corresponde a la tarea y su respectivo resultado."
      ],
      "metadata": {
        "id": "3gUfMx-Ya9Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo: 3 funciones en paralelo**\n",
        "\n",
        "Supongamos que queremos realizar tres trasnformaciones sobre el mismo texto.\n",
        "\n",
        "1. Contar los caracteres (`len`).\n",
        "2. Convertir el texto a may√∫sculas.\n",
        "3. Invertir el texto."
      ],
      "metadata": {
        "id": "vIwa7u7UaAbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "( Observa dos cosas: Este ejemplo no implica ningun LLM, son solo funciones Python. No hacemos uso de RunnableLambda en las dos ultimas, LangChain envolver√° nuestras unciones por nosotros en un RunnableLambda)"
      ],
      "metadata": {
        "id": "4ZJrE9UIadYv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYfEGjL2acpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "\n",
        "# 1. Crear un RunnableParallel con varias tareas\n",
        "parallel = RunnableParallel({\n",
        "    \"length\": RunnableLambda(lambda x: len(x)),  # Cuenta caracteres\n",
        "    \"uppercase\": lambda x: x.upper(),  # Convierte a may√∫sculas\n",
        "    \"reverse\": lambda x: x[::-1]  # Invierte el texto\n",
        "})\n",
        "\n",
        "# 2. Probar el RunnableParallel\n",
        "input_text = \"LangChain es incre√≠ble\"\n",
        "result = parallel.invoke(input_text)\n",
        "\n",
        "# 3. Imprimir el resultado\n",
        "print(result)"
      ],
      "metadata": {
        "id": "0KD1Zd0OaNNr",
        "outputId": "02e548a2-89d2-4bc4-d150-b1708a783eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'length': 22, 'uppercase': 'LANGCHAIN ES INCRE√çBLE', 'reverse': 'elb√≠ercni se niahCgnaL'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo: Traducciones en paralelo**\n",
        "\n",
        "Deseamos usar cuatro modelos de lenguje diferentes (dos de ellos el mismo con diferente temperatura) para traducir un texto\n",
        "\n"
      ],
      "metadata": {
        "id": "naeAKRyebmpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "gpt4mini_0 = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0)\n",
        "\n",
        "gpt4mini_2 = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=2)\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "llama33 = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "# Prompt para modelos que soportan mensajes de sistema\n",
        "prompt_template_con_sistema = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un experto traductor de ingles a espa√±ol.\"),\n",
        "    (\"human\", \"Traduce el texto siguiente: {texto}\"),\n",
        "])\n",
        "\n",
        "# Prompt simplificado para Gemini (sin mensaje de sistema)\n",
        "prompt_template_gemini = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Traduce el texto siguiente al espa√±ol: {texto}\"),\n",
        "])\n",
        "\n",
        "prompt_text = \"Whenever used in this Agreement, the following words and phrases, unless the context otherwise requires, shall have the following meanings.Such meanings shall be equally applicable to the singular and plural forms of such terms, as the context may require.\"\n",
        "\n",
        "cadena = RunnableParallel({\n",
        "    \"gpt4mini_0\": prompt_template_con_sistema | gpt4mini_0,\n",
        "    \"gpt4mini_2\": prompt_template_con_sistema | gpt4mini_2,\n",
        "    \"gemini\": prompt_template_gemini | gemini,  # Usamos el prompt simplificado para Gemini\n",
        "    \"llama33\": prompt_template_con_sistema | llama33,\n",
        "})\n",
        "\n",
        "resultado = cadena.invoke({\"texto\": prompt_text}) # Pasamos un diccionario con la clave \"texto\"\n",
        "\n",
        "for modelo, traduccion in resultado.items():\n",
        "    print(f\"{modelo}: {traduccion.content}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "Tt93rjo5cFVL",
        "outputId": "be783e1f-d0df-41b0-a898-102648c62788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt4mini_0: Siempre que se utilicen en este Acuerdo, las siguientes palabras y frases, a menos que el contexto exija lo contrario, tendr√°n los siguientes significados. Dichos significados ser√°n igualmente aplicables a las formas singular y plural de dichos t√©rminos, seg√∫n lo requiera el contexto.\n",
            "---\n",
            "gpt4mini_2: Siempre que se utilicen en este Acuerdo, las siguientes palabras y expresiones, a menos que el contexto exija lo contrario, tendr√°n los siguientes significados. Tales significado ser√°n igualmente aplicables a las formas singular y plural de dichos t√©rminos, seg√∫n lo requiera el contexto.\n",
            "---\n",
            "gemini: Siempre que se usen en este Acuerdo, las siguientes palabras y frases, a menos que el contexto requiera lo contrario, tendr√°n los siguientes significados. Dichos significados ser√°n aplicables igualmente a las formas singular y plural de dichos t√©rminos, seg√∫n lo requiera el contexto.\n",
            "---\n",
            "llama33: Siempre que se utilicen en este Acuerdo, las siguientes palabras y frases, a menos que el contexto lo exija de otra manera, tendr√°n los siguientes significados. Dichos significados ser√°n igualmente aplicables a las formas singulares y plurales de dichos t√©rminos, seg√∫n lo requiera el contexto.\n",
            "\n",
            "Nota: He mantenido el tono formal y t√©cnico del texto original, ya que parece ser un fragmento de un contrato o acuerdo legal. Si necesitas una traducci√≥n m√°s informal o adaptada a un contexto espec√≠fico, por favor h√°zmelo saber.\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZXsXGDXWb_Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí°idea : obterner la traduccion con gtranslate y otra por un llm. Pasarsela un segundo llm para que las compare y haga una version final. Esto parerec un caso de fusionar dos ramas\n",
        "\n",
        "Otra variante, producir en paralelo varias traducciones y fusionarlas en una sola"
      ],
      "metadata": {
        "id": "0IUB9BlJFgUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://python.langchain.com/docs/concepts/runnables/\n",
        "\n",
        "2. Info oficial: https://python.langchain.com/api_reference/core/runnables.html\n",
        "3. https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
        "\n",
        "4. https://dzone.com/articles/guide-to-langchain-runnable-architecture\n",
        "\n",
        "5. https://medium.com/@danushidk507/runnables-in-langchain-e6bfb7b9c0ca\n",
        "\n",
        "6. https://www.youtube.com/watch?v=8aUYzb1aYDU\n",
        "\n",
        "7. https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac\n",
        "\n",
        "8. https://medium.com/@ulrichw/list/langchain-lcel-85af4f4ff883\n",
        "\n",
        "9. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "10. https://www.pinecone.io/learn/series/langchain/langchain-expression-language/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJd-xV-oMuhQ"
      }
    }
  ]
}