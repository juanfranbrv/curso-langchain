{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7xSm5QKN+wm/I4uM3FhEE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/LCEL%20y%20Runnables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. LCEL, LangChain Expression Language**\n",
        "---\n",
        "\n",
        "LCEL se introdujo en Langchain **a mediados de 2023**, específicamente con la **versión 0.0.142**, lanzada el **19 de julio de 2023**\n",
        "\n",
        "La introducción de **LCEL** en Langchain fue una respuesta a la necesidad de una forma más potente, flexible y fácil de usar para construir aplicaciones de lenguaje complejas. Proporcionó una sintaxis declarativa, mejoró la legibilidad, facilitó la depuración y habilitó funcionalidades avanzadas como el streaming y la ejecución asíncrona, consolidándose como una pieza fundamental del ecosistema de Langchain.\n",
        "\n",
        "**LCEL se basa en la Composición, no en Tipos Predefinidos:**\n",
        "\n",
        "-   **Operador Pipe (|):** La piedra angular de LCEL es el operador pipe. Este operador te permite encadenar componentes de forma secuencial, enviando la salida de un componente como entrada al siguiente. Esto es inherentemente secuencial, pero no se define como un tipo de cadena \"secuencial\".\n",
        "    \n",
        "-   **Primitivas Runnable:** LCEL se basa en la interfaz Runnable. Cualquier objeto que implemente esta interfaz puede ser parte de una cadena LCEL. Esto incluye modelos de lenguaje, prompts, parsers, retrievers, etc.\n",
        "    \n",
        "-   **Flexibilidad Total:** La clave es que puedes combinar estas primitivas Runnable de cualquier manera que tenga sentido para tu aplicación. No estás limitado a estructuras predefinidas.\n",
        "\n",
        "**Ventajas de este enfoque:**\n",
        "\n",
        "-   **Mayor Flexibilidad:** No estás limitado por las estructuras predefinidas. Puedes crear flujos de trabajo exactamente como los necesitas.\n",
        "    \n",
        "-   **Reutilización de Componentes:** Los componentes individuales pueden ser reutilizados en diferentes cadenas con diferentes flujos de trabajo.\n",
        "    \n",
        "-   **Claridad y Composición:** El uso del operador pipe hace que la lógica de la cadena sea más clara y fácil de entender.\n",
        "    \n",
        "-   **Optimización:** La forma en que construyes la cadena influye en cómo se puede optimizar su ejecución (por ejemplo, para paralelismo).\n",
        "    \n",
        "\n",
        "**En resumen, LCEL te proporciona las herramientas y la sintaxis para orquestar tus flujos de trabajo de manera flexible y poderosa. En lugar de imponer tipos de cadenas predefinidos, te da la libertad de construir las cadenas que mejor se adapten a tus necesidades, implementando patrones secuenciales, condicionales o paralelos según sea necesario.**\n",
        "\n",
        "Y todo esta abstración se basa en los **Runnables**"
      ],
      "metadata": {
        "id": "p9n7xOvQEQ6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Runnables**\n",
        "---\n",
        "\n",
        "Un *“Runnable”* en LangChain es una abstracción (una interfaz en código) que define un contrato para ejecutar una operación. Concretamente, un `Runnable` expone un método `invoke(input)`, el cual recibe un dato de entrada y produce un resultado de salida. Así, cualquier clase u objeto que cumpla con esta interfaz puede encadenarse o combinarse con otros `Runnables` para construir flujos de trabajo complejos (por ejemplo, secuencias o ejecuciones en paralelo).   \n",
        "\n",
        "Cada \"*Runnable*\" implementa métodos como invoke, batch, stream, y transform, lo que lo hace compatible con diferentes modos de ejecución (sincrónica, asincrónica, en lote, etc.).  \n",
        "Esto permite orquestar y reutilizar fácilmente distintas operaciones dentro de LangChain.\n",
        "\n",
        "Un poco menos técnico...\n",
        "\n",
        "Un *“Runnable”* es como una función que sabe hacer algo muy concreto: recibe cierta información y devuelve un resultado. Lo importante es que, en LangChain, todos los *“Runnables”* siguen la misma “forma de trabajar”. Gracias a esto, podemos ir uniendo varios “Runnables” uno tras otro o en paralelo para crear procesos más grandes sin tener que hacer ajustes complicados. En otras palabras, si cada bloque (*Runnable*) sabe cómo recibir datos y devolverlos transformados, podemos combinar esos bloques como si fueran piezas de LEGO para armar flujos de trabajo completos.\n",
        "\n",
        "## Conceptos clave de runnables\n",
        "-   **Modularidad**\n",
        "    \n",
        "    -   Cada Runnable representa una única tarea u operación (p.ej., ejecutar un modelo, procesar datos, encadenar operaciones).\n",
        "    -   Su diseño en “bloques” facilita la independencia y el intercambio de componentes.\n",
        "-   **Componibilidad**\n",
        "    \n",
        "    -   Varios Runnables pueden vincularse para formar canalizaciones o flujos de trabajo complejos.\n",
        "    -   Esto permite crear soluciones más grandes a partir de piezas pequeñas, flexibles y reutilizables.\n",
        "-   **Reutilizabilidad**\n",
        "    \n",
        "    -   Una vez definido, un Runnable se puede integrar en distintos proyectos sin modificaciones.\n",
        "    -   Es ideal para tareas estándar que se repiten (p.ej., preprocesamiento de datos).\n",
        "-   **Ejecución asincrónica**\n",
        "    \n",
        "    -   Los Runnables pueden ejecutarse de forma asíncrona, optimizando recursos y tiempo, especialmente cuando hay llamadas a servicios externos o E/S de por medio.\n",
        "\n",
        "-   **Ejecución paralela**\n",
        "    \n",
        "    -   Es posible configurar Runnables para que se ejecuten en paralelo, lo que mejora el rendimiento en tareas por lotes o cuando se manejan grandes volúmenes de datos.\n",
        "-   **Manejo de errores**\n",
        "    \n",
        "    -   Suelen incluir mecanismos para capturar y gestionar excepciones, reforzando la solidez del flujo de trabajo.\n",
        "-   **Registro y depuración**\n",
        "    \n",
        "    -   Admiten el registro de metadatos y eventos, lo que facilita rastrear y depurar la ejecución de principio a fin.\n"
      ],
      "metadata": {
        "id": "TXNoJRERJ5l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Chains (Cadenas)**\n",
        "---\n",
        "\n",
        "En Langchain, una Cadena (Chain) representa una secuencia orquestada de llamadas a uno o más Runnables para realizar una tarea específica. Piensa en ella como una tubería o un flujo de trabajo donde la salida de un Runnable se convierte en la entrada del siguiente.  \n",
        "\n",
        "En esencia, una Cadena combina la funcionalidad de múltiples Runnables para llevar a cabo procesos más complejos que los que un solo Runnable podría manejar individualmente. Las Cadenas son el mecanismo principal en Langchain para construir aplicaciones de lenguaje natural con lógica y pasos definidos.\n",
        "\n",
        "En LCEL no existen tipos de cadenas predefinidos como \"secuenciales\", \"condicionales\" o \"paralelas\". Sin embargo, puedes implementar estos patrones de flujo de trabajo utilizando la sintaxis de LCEL junto con funciones adicionales.  "
      ],
      "metadata": {
        "id": "dVHiL2gmcWJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. ¡Uniendo las piezas: Llegó la hora del código!**\n",
        "---\n",
        "\n",
        "En esta sección, pondremos en práctica los conceptos teóricos que hemos explorado. A través de una serie de ejemplos con código, ilustraremos cómo LCEL, los Runnables y las Cadenas se combinan en Langchain para resolver tareas concretas. Nuestro enfoque será didáctico, facilitando la comprensión a la vez que presentamos casos de uso reales.\n",
        "\n",
        "** Langchain va en este momento por la version 0.3. Dado que es una librería en evolución, es posible que se agreguen o refinen más “runnables” en versiones futuras.\n",
        "\n"
      ],
      "metadata": {
        "id": "fG7vAQWRR8pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "Uv1UDKaYSP4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "%pip install -qU langchain-together\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_together import ChatTogether\n",
        "\n",
        "llm = ChatTogether(\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=TOGETHER_API_KEY,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "llm.invoke(\"Hola. Que modelo eres ???\")"
      ],
      "metadata": {
        "id": "qFsQcCDhY_Aq",
        "outputId": "fae1e42f-98d7-42aa-ef8a-4f5c933c4bbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hola! Soy LLaMA, un modelo de lenguaje basado en inteligencia artificial desarrollado por Meta AI. No soy un modelo específico de persona o entidad, sino más bien un programa diseñado para entender y responder a preguntas y conversar de manera natural. Mi capacidad para responder se basa en el análisis de grandes cantidades de texto y datos, lo que me permite generar respuestas coherentes y relevantes. ¡Es un placer charlar contigo!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 17, 'total_tokens': 118, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3-70b-chat-hf', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fc670359-ca75-43ed-b225-9ec3c09f6123-0', usage_metadata={'input_tokens': 17, 'output_tokens': 101, 'total_tokens': 118, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Prompt + LLM**\n",
        "\n",
        "---\n",
        "Posiblemente esta es la cadena mas simple que podamos crear.  \n",
        "En este caso la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo.\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Observa que esto es lo que hemos hecho hasta ahora sin cadenas: Invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia y este enfoque puede ser más simple para tareas sencillas, carece de la flexibilidad y la extensibilidad que ofrecen las cadenas como se aprecia en cuanto la complejidad escala.\n",
        "\n",
        "Ademas LCEL y la nuevas cadenas proporcionan un nivel de abstracción mayor tanto del prompt como del proceso en general."
      ],
      "metadata": {
        "id": "3r0n1rImzRrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos el prompt template de una de esta dos formas\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¿cómo estás?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Francés\", \"texto\":\"Hola, ¿cómo estás?\"})\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "zfkcPT99zsbr",
        "outputId": "1b19a18b-7155-4def-a32a-1db4a8789bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Bonjour, comment allez-vous ?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RunnableLambda**\n",
        "\n",
        "La forma principal de añadir una función a una cadena en Langchain (especialmente en LCEL) es utilizando `RunnableLambda`. `RunnableLambda` es un Runnable que envuelve una función Python, permitiéndole integrarse perfectamente en el flujo de la cadena.\n",
        "\n",
        "(Aunque es posible, la inserción directa de la función `transformar_texto` funciona, esto se debe a que Langchain implícitamente envuelve esa función en un RunnableLambda \"por debajo del capó\" para que pueda encajar dentro del paradigma de la cadena LCEL. Es mejor ser consistente con los principios del framework y usar `RunnableLambda`)\n"
      ],
      "metadata": {
        "id": "jYlj5VuMY9jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Prompt + LLM + funcion personalizada de transformación de la salida**\n",
        "---\n",
        "\n",
        "En este caso la salida del LLM se procesa con una funcion sencilla para pasarla a mayusculas. (pero puede ser tan complicada como se necesite)\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un objeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar.\n",
        "\n"
      ],
      "metadata": {
        "id": "6KCFlW3j1zWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# # Definimos una función de transformación\n",
        "# def transformar_texto(output):\n",
        "#     return output.content.upper()\n",
        "\n",
        "\n",
        "transformar_texto=RunnableLambda(lambda output: output.content.upper())\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N-YvGoOb16Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prompt + LLM + OutputParser + funcion personalizada de transformación de la salida**\n",
        "---\n",
        "\n",
        "En este ejemplo se observa mejor la modularidad reutilizando la cadena en un bucle y usando un OutputParser.  \n",
        "\n",
        "**Los OutputParser en Langchain son Runnables**. Esto les permite ser componentes activos dentro de las cadenas LCEL, tomando la salida de Runnables precedentes y transformándola según su lógica específica. Su capacidad para ser Runnables es fundamental para la flexibilidad y el poder de composición de Langchain."
      ],
      "metadata": {
        "id": "ZdMVH9kL6uOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una función de transformación\n",
        "# Esta función toma la salida del OutPutParser (str) y la convierte a mayúsculas.\n",
        "transformar_texto=RunnableLambda(lambda output: output.upper())\n",
        "\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas en un bucle\n",
        "# Observa cómo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¿Quién fue Napoleón Bonaparte?\"},\n",
        "    {\"tema\": \"Programación\", \"pregunta\": \"¿Qué es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "VKkDds_W6ymg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Prompt + LLM + OutputParser + Guardar en fichero**\n",
        "Crea una cadena que genera una lista de elementos y la guarda en un archivo:\n",
        "\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "4yMTStcf-fPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Creamos la cadena con LCEL y RunnableLambda\n",
        "def guardar_en_archivo_y_mostrar(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Podriamos no hacer esto y meter la funcion directamente en la cadena\n",
        "# Metemos la funcion en un Runnable, reutilizamos la variable\n",
        "guardar_en_archivo_y_mostrar = RunnableLambda(guardar_en_archivo_y_mostrar)\n",
        "\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo_y_mostrar\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en 'mente'\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "aA2_DRpA9gbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + JsonOutputParser**\n",
        "---\n",
        "\n",
        "Esta vez genera una respuesta JSON.\n",
        "\n",
        "JsonOutputParser transforma la salida del modelo en un objeto JSON válido."
      ],
      "metadata": {
        "id": "-oRstVkU_SOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve directamente un array JSON. Cada elemento del array debe ser un objeto JSON con las claves 'persona' y 'aportación'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\"})\n",
        "\n",
        "result\n"
      ],
      "metadata": {
        "id": "ueUefLwJ_2Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo existe un OutputParser, PydanticOutputParser mucho mas robusto para estructurar la salida. Lo usaremos a continuación.\n",
        "\n",
        "**Consideraciones:**\n",
        "\n",
        "-   **Dependencia del Prompt:** Con JsonOutputParser, la precisión de la salida depende mucho de la claridad y especificidad del prompt. Si el modelo no sigue exactamente las instrucciones o el prompt no es absolutamente claro, el parsing podría fallar.\n",
        "    \n",
        "-   **Menos validación automática:** A diferencia de PydanticOutputParser, JsonOutputParser simplemente intenta parsear la salida como JSON. No valida si la estructura interna coincide con un esquema específico.\n",
        "    \n",
        "-   **Manejo de la salida:** Después de obtener el JSON, necesitarás acceder a la lista de personas a través de la clave \"personas\" en el diccionario resultante.\n",
        "\n",
        "Si no quieres el diccionario contenedor con la clave \"personas\", sino directamente la lista de diccionarios, necesitas modificar el prompt para que le indique al modelo de lenguaje que genere directamente un array JSON.\n",
        "\n",
        "El prompt de sistema deberia ser algo asi:\n",
        "\n",
        "*Eres un asistente conciso y directo experto. Devuelve directamente un array JSON. Cada elemento del array debe ser un objeto JSON con las claves 'persona' y 'aportación'. No añadas ningun tipo de comentario*\n",
        "    \n",
        "**El prompt tiene gran importancia**\n",
        "En resumen, si bien es posible usar JsonOutputParser para obtener una lista de objetos, requiere un prompt más específico para guiar al modelo a generar un único JSON con una lista dentro. PydanticOutputParser ofrece una solución más robusta y con validación automática cuando se trabaja con estructuras de datos definidas.\n",
        "\n"
      ],
      "metadata": {
        "id": "nLziIjL4Qp64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 6: PydanticOutputParser**\n",
        "\n",
        "PydanticOutputParser es una herramienta dentro de Langchain que facilita la estructuración de la salida de modelos de lenguaje en objetos Python definidos con Pydantic. En lugar de simplemente obtener texto plano, este parser te permite especificar un esquema de datos deseado (a través de una clase Pydantic) y automáticamente convierte la respuesta del modelo en una instancia de esa clase. Esto asegura que la información extraída tenga un formato consistente y predecible.\n",
        "\n",
        "Una de las principales ventajas de PydanticOutputParser es su capacidad de validación. Al utilizar Pydantic, se aplican automáticamente las validaciones definidas en el esquema, lo que ayuda a garantizar la calidad y la integridad de los datos extraídos. Además, simplifica el manejo de la salida del modelo, ya que puedes acceder a los datos como atributos de un objeto Python en lugar de tener que parsear manualmente cadenas de texto o diccionarios JSON.\n",
        "\n",
        "* Pero el fallo en la validacion de la respuesta arrojaria una excepcionque habria que gestionar con alguna estructura del tipo TRY-EXCEPT para que nuestra app no se detuviera !!\n",
        "\n",
        "Finalmente, PydanticOutputParser mejora la claridad y mantenibilidad del código. Al definir explícitamente la estructura de la información esperada, se facilita la comprensión del flujo de datos y se reduce la probabilidad de errores al trabajar con la salida del modelo. Además, Langchain integra funcionalidades para generar instrucciones de formato para el modelo basadas en el esquema Pydantic, lo que ayuda a guiar al modelo para que produzca la salida en el formato correcto desde el principio.\n",
        "\n"
      ],
      "metadata": {
        "id": "_OpapnJUaeHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "# Definimos nuestra estructura de datos\n",
        "class Resultado(BaseModel):\n",
        "    persona: str = Field(description=\"Nombre de la persona\")\n",
        "    aportacion: str = Field(description=\"Importe en euros\")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve el resultado en formato JSON con las claves 'persona' y 'aportación'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Usamos PydanticOutputParser y le pasamos la clase Resultado\n",
        "json_parser = PydanticOutputParser(pydantic_object=Resultado)\n",
        "\n",
        "# Incluimos las instrucciones de formato en el prompt\n",
        "prompt_con_formato = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve el resultado en formato JSON con las claves 'persona' y 'aportación'. No añadas ningun tipo de comentario. \\n{format_instructions}\\n\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_con_formato | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena y gestionamos la excepción\n",
        "try:\n",
        "    result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\", \"format_instructions\": json_parser.get_format_instructions()})\n",
        "    print(result)\n",
        "except Exception as e:\n",
        "    print(f\"Se produjo una excepción: {e}\")\n",
        "    print(\"La validación falló porque el modelo devolvió una lista de objetos, pero se esperaba un único objeto.\")"
      ],
      "metadata": {
        "id": "DN_R0XVealwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con unos cambios en el prompt y en las clases de validacion conseguimos obtener una lista JSON correcta"
      ],
      "metadata": {
        "id": "VQKA4rNFezcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "# Definimos la estructura de datos para UN resultado\n",
        "class Resultado(BaseModel):\n",
        "    persona: str = Field(description=\"Nombre de la persona\")\n",
        "    aportacion: str = Field(description=\"Importe en euros\")\n",
        "\n",
        "# Definimos una nueva estructura para una LISTA de resultados\n",
        "class ListaResultados(BaseModel):\n",
        "    resultados: List[Resultado]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve un array JSON con una lista de objetos, donde cada objeto tiene las claves 'persona' y 'aportación'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Usamos PydanticOutputParser y le pasamos la clase ListaResultados\n",
        "json_parser = PydanticOutputParser(pydantic_object=ListaResultados)\n",
        "\n",
        "# Incluimos las instrucciones de formato en el prompt\n",
        "prompt_con_formato = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve un array JSON con una lista de objetos, donde cada objeto tiene las claves 'persona' y 'aportación'. No añadas ningun tipo de comentario. \\n{format_instructions}\\n\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_con_formato | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "try:\n",
        "    result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\", \"format_instructions\": json_parser.get_format_instructions()})\n",
        "    print(result)\n",
        "    # Ahora puedes acceder a la lista de resultados así:\n",
        "    for item in result.resultados:\n",
        "        print(f\"Persona: {item.persona}, Aportación: {item.aportacion}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Se produjo una excepción: {e}\")\n",
        "    print(\"La validación falló. Asegúrate de que el modelo devuelve una lista de objetos con la estructura correcta.\")"
      ],
      "metadata": {
        "id": "K4oB0BK8e9kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 7: Cadena con dos LLMs (Generación y Resumen)**\n",
        "\n",
        "Este ejemplo nos permitir presentar algunas caracteristicas mas de LCEL.\n",
        "Presentaremos diversas variaciones del mismo.\n",
        "\n",
        "Vamos a crear una cadena donde un LLM genera contenido y otro LLM resumen ese contenido.\n",
        "\n",
        "Posiblemnte el modo mas facil, segun vamos haciendo, sea este. Pero observa que de esta forma no tenemos acceso al texto del articulo que se genera dentro de la cadena y que se pasa a resumir. Solo accedemos al resultado final de la cadena, el texto resumido."
      ],
      "metadata": {
        "id": "OIEXdOhEmn1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primer LLM: Genera contenido\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un artículo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# Segundo LLM: Resumen del contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente artículo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = generate_prompt | llm1 | summarize_prompt | llm2\n",
        "\n",
        "# Ejecutar la cadena\n",
        "salida = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "display(Markdown(salida.content))\n"
      ],
      "metadata": {
        "id": "l6-lBATTmoWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para mostrar tanto el texto extenso (artículo completo) como el texto resumido usando el enfoque moderno de cadenas en LangChain (LCEL), lo más sencillo es separar la generación en dos pasos en lugar de construir una única cadena que sobrescriba la salida intermedia. Así podemos capturar y reutilizar la respuesta completa de tu primer LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "OCOydiQXuLXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Primer LLM: genera contenido\n",
        "llm1 = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un artículo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# 2) Segundo LLM: resume el contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente artículo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# --- EJECUCIÓN EN DOS PASOS ---\n",
        "\n",
        "# Paso A: Generar el artículo\n",
        "article_chain = generate_prompt | llm1\n",
        "article_result = article_chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "\n",
        "# Paso B: Resumir el artículo\n",
        "summary_chain = summarize_prompt | llm2\n",
        "summary_result = summary_chain.invoke({\"article\": article_result.content})\n",
        "\n",
        "# Mostramos ambos resultados\n",
        "display(Markdown(\"## Texto extenso (Artículo completo)\"))\n",
        "display(Markdown(article_result.content))\n",
        "\n",
        "display(Markdown(\"## Resumen\"))\n",
        "display(Markdown(summary_result.content))"
      ],
      "metadata": {
        "id": "6ob4RIOGvlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RunnableParallel**\n",
        "\n",
        "RunnableParallel es un **Runnable** que te permite ejecutar **varios Runnables diferentes simultáneamente (en paralelo)**. Piensa en él como un coordinador que lanza varias tareas al mismo tiempo y luego reúne los resultados.\n",
        "\n",
        "**¿Cómo funciona?**\n",
        "\n",
        "- **Recibe un diccionario de Runnables:** Se le proporciona un diccionario donde las **claves son nombres descriptivos** y los **valores son los Runnables** que quieres ejecutar en paralelo.\n",
        "    \n",
        "- **Ejecuta los Runnables en paralelo:** LangChain se encarga de ejecutar todos esos Runnables al mismo tiempo (o lo más simultáneamente posible).\n",
        "    \n",
        "- **Devuelve un diccionario de resultados:** El resultado de RunnableParallel es un diccionario donde las **claves son las mismas que proporcionaste** y los **valores son los resultados de la ejecución de cada Runnable**.\n",
        "\n",
        "Es especialmente útil cuando necesitas realizar diferentes transformaciones o procesamientos **sobre los mismos datos**.\n",
        "\n",
        "1. **Input compartido:** El mismo input se pasa a todas las tareas definidas en el paralelo.  \n",
        "\n",
        "2. **Ejecución simultánea:** Todas las tareas se ejecutan al mismo tiempo.\n",
        "3. **Salida combinada:** Devuelve un diccionario donde cada clave corresponde a la tarea y su respectivo resultado."
      ],
      "metadata": {
        "id": "3gUfMx-Ya9Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo: 3 funciones en paralelo**\n",
        "\n",
        "Supongamos que queremos realizar tres trasnformaciones sobre el mismo texto.\n",
        "\n",
        "1. Contar los caracteres (`len`).\n",
        "2. Convertir el texto a mayúsculas.\n",
        "3. Invertir el texto."
      ],
      "metadata": {
        "id": "vIwa7u7UaAbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "( Observa dos cosas: Este ejemplo no implica ningun LLM, son solo funciones Python. No hacemos uso de RunnableLambda en las dos ultimas, LangChain envolverá nuestras unciones por nosotros en un RunnableLambda)"
      ],
      "metadata": {
        "id": "4ZJrE9UIadYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "\n",
        "# 1. Crear un RunnableParallel con varias tareas\n",
        "parallel = RunnableParallel({\n",
        "    \"length\": RunnableLambda(lambda x: len(x)),  # Cuenta caracteres\n",
        "    \"uppercase\": lambda x: x.upper(),  # Convierte a mayúsculas\n",
        "    \"reverse\": lambda x: x[::-1]  # Invierte el texto\n",
        "})\n",
        "\n",
        "# 2. Probar el RunnableParallel\n",
        "input_text = \"LangChain es increíble\"\n",
        "result = parallel.invoke(input_text)\n",
        "\n",
        "# 3. Imprimir el resultado\n",
        "print(result)"
      ],
      "metadata": {
        "id": "0KD1Zd0OaNNr",
        "outputId": "9963c87f-a27f-420c-cf9a-6bdf8b9e7270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'length': 22, 'uppercase': 'LANGCHAIN ES INCREÍBLE', 'reverse': 'elbíercni se niahCgnaL'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo: Traducciones en paralelo**\n",
        "\n",
        "Deseamos usar cuatro modelos de lenguje diferentes (dos de ellos el mismo con diferente temperatura) para traducir un texto\n",
        "\n"
      ],
      "metadata": {
        "id": "naeAKRyebmpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "gpt4mini_0 = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0)\n",
        "\n",
        "gpt4mini_2 = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=2)\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "llama33 = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "# Prompt para modelos que soportan mensajes de sistema\n",
        "prompt_template_con_sistema = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un experto traductor de ingles a español. Contestas siempre solo con la traducción que se te solicite\"),\n",
        "    (\"human\", \"Traduce el texto siguiente: {texto}\"),\n",
        "])\n",
        "\n",
        "# Prompt simplificado para Gemini (sin mensaje de sistema)\n",
        "prompt_template_gemini = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Traduce el texto siguiente al español: {texto}\"),\n",
        "])\n",
        "\n",
        "prompt_text = ( \"Whenever used in this Agreement, the following words and phrases,\"\n",
        "                \"unless the context otherwise requires, shall have the following meanings.\"\n",
        "                \"Such meanings shall be equally applicable to the singular and plural forms \"\n",
        "                \"of such terms, as the context may require.\" )\n",
        "\n",
        "cadena = RunnableParallel({\n",
        "    \"gpt4mini_0\": prompt_template_con_sistema | gpt4mini_0,\n",
        "    \"gpt4mini_2\": prompt_template_con_sistema | gpt4mini_2,\n",
        "    \"gemini\": prompt_template_gemini | gemini,  # Usamos el prompt simplificado para Gemini\n",
        "    \"llama33\": prompt_template_con_sistema | llama33,\n",
        "})\n",
        "\n",
        "resultado = cadena.invoke({\"texto\": prompt_text}) # Pasamos un diccionario con la clave \"texto\"\n",
        "\n",
        "for modelo, traduccion in resultado.items():\n",
        "    print(f\"{modelo}: {traduccion.content}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "Tt93rjo5cFVL",
        "outputId": "e134a890-d2c1-4d78-bb89-86bfebb97db7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt4mini_0: Siempre que se utilicen en este Acuerdo, las siguientes palabras y frases, a menos que el contexto requiera lo contrario, tendrán los siguientes significados. Dichos significados serán igualmente aplicables a las formas singular y plural de dichos términos, según lo requiera el contexto.\n",
            "---\n",
            "gpt4mini_2: Siempre que se utilicen en este Acuerdo, las siguientes palabras y frases, a menos que el contexto requiera lo contrario, tendrán los siguientes significados. Dichos significados serán aplicables igualmente a las formas singular y plural de dichos términos, según exija el contexto.\n",
            "---\n",
            "gemini: Cuando se utilizan en este Acuerdo, las siguientes palabras y frases, a menos que el contexto requiera lo contrario, tendrán los siguientes significados. Tales significados serán aplicables igualmente a las formas singulares y plurales de tales términos, según lo requiera el contexto.\n",
            "---\n",
            "llama33: Siempre que se utilicen en este Acuerdo, las siguientes palabras y frases, a menos que el contexto lo exija de otra manera, tendrán los siguientes significados. Dichos significados serán igualmente aplicables a las formas singulares y plurales de dichos términos, según lo requiera el contexto.\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtnemos un diccionario como resultado. Con claves para cada una de las tareas (o hilos) y los valores de cada una es la respuesta a esa tarea.\n",
        "\n"
      ],
      "metadata": {
        "id": "nksh2ThPaH6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "# Configuración de los modelos\n",
        "gpt4_mini = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=2)\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\", api_key=GOOGLE_API_KEY, temperature=0.7)\n",
        "\n",
        "# Prompt para traducción\n",
        "prompt_traduccion = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un traductor experto de inglés a español.\"),\n",
        "    (\"human\", \"Traduce el siguiente texto al español: {texto}\")\n",
        "])\n",
        "\n",
        "# Prompt para análisis de texto (métricas)\n",
        "prompt_analisis = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Eres un analista de texto que calcula estadísticas simples.\\n Analiza este texto y devuelve una lista con los terminos tcnicos.\\nTexto: {texto}\")\n",
        "])\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto_entrada = \"Machine learning models, such as transformers, have revolutionized natural language processing.\"\n",
        "\n",
        "# Definir tareas paralelas\n",
        "procesos_paralelos = RunnableParallel({\n",
        "    \"traduccion\": prompt_traduccion | gpt4_mini,\n",
        "    \"analisis\": prompt_analisis | gemini\n",
        "})\n",
        "\n",
        "# Ejecutar las tareas paralelas\n",
        "resultados = procesos_paralelos.invoke({\"texto\": texto_entrada})\n",
        "\n",
        "# Procesar manualmente los resultados\n",
        "traduccion = resultados[\"traduccion\"].content\n",
        "analisis = resultados[\"analisis\"].content\n",
        "\n",
        "# Combinar y mostrar los resultados\n",
        "resultado_final = f\"Traducción: {traduccion}\\n\\nAnálisis del texto original:\\n{analisis}\"\n",
        "print(resultado_final)"
      ],
      "metadata": {
        "id": "tmdrvFtqfFV7",
        "outputId": "b5cb58d2-01b6-4c63-bcd8-2aa4b8c940cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traducción: Los modelos de aprendizaje automático, como los transformers, han revolucionado el procesamiento del lenguaje natural.\n",
            "\n",
            "Análisis del texto original:\n",
            "**Número de palabras:** 12\n",
            "\n",
            "**Términos técnicos:**\n",
            "\n",
            "* Machine learning\n",
            "* Transformers\n",
            "* Natural language processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos pasar todos los resultados a otro Runnable ??\n",
        "\n",
        "Basicamente, como sabemos el resultado es un diccionario, crearemmos una funcion que tome como entrada el diccionario y haga algo con el\n",
        "\n",
        "Por ejmplo vamos a usar un LLM para producir una frase distinta de similar longitud que use los mismos terminos tenicos detectados"
      ],
      "metadata": {
        "id": "PSOWUV84gGgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "\n",
        "# Definir el modelo Pydantic para la lista de términos técnicos\n",
        "class TerminosTecnicos(BaseModel):\n",
        "    terminos: list[str] = Field(description=\"Lista de términos técnicos encontrados en el texto\")\n",
        "\n",
        "# Crear el PydanticOutputParser\n",
        "output_parser = PydanticOutputParser(pydantic_object=TerminosTecnicos)\n",
        "\n",
        "\n",
        "\n",
        "# Configuración de los modelos\n",
        "gpt4_mini_0 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "gpt4_mini_1 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\", api_key=GOOGLE_API_KEY, temperature=0.7)\n",
        "\n",
        "# Prompt para traducción\n",
        "prompt_traduccion = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un traductor experto de inglés a español.\"),\n",
        "    (\"human\", \"Traduce el siguiente texto al español: {texto}\")\n",
        "])\n",
        "\n",
        "# Prompt para análisis de texto (métricas) - Modificado para Pydantic\n",
        "prompt_analisis = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"Eres un analista de texto. Analiza este texto y devuelve una lista de los términos técnicos encontrados.\\nTexto: {texto}\\n{format_instructions}\")\n",
        "]).partial(format_instructions=output_parser.get_format_instructions())\n",
        "\n",
        "\n",
        "\n",
        "# Prompt para generacion\n",
        "prompt_generacion = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un escritor tecnico experimentado.\"),\n",
        "    (\"human\", \"Crea una frase de {longitud} usando los terminos tecnicos {lista_terminos}, pero que sea distinta a {frase_original}\")\n",
        "])\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto_entrada = \"Machine learning models, such as transformers, have revolutionized natural language processing.\"\n",
        "\n",
        "# Definir tareas paralelas\n",
        "procesos_paralelos = RunnableParallel({\n",
        "    \"traduccion\": prompt_traduccion | gpt4_mini_0,\n",
        "    \"analisis\": prompt_analisis | gemini | output_parser\n",
        "})\n",
        "\n",
        "# Ejecutar las tareas paralelas\n",
        "resultados = procesos_paralelos.invoke({\"texto\": texto_entrada})\n",
        "\n",
        "\n",
        "\n",
        "# Procesar manualmente los resultados\n",
        "traduccion = resultados[\"traduccion\"].content\n",
        "analisis = resultados[\"analisis\"].terminos\n",
        "\n",
        "\n",
        "print(analisis.terminos)"
      ],
      "metadata": {
        "id": "WxZnB_jQkNBh",
        "outputId": "ec579e2b-393e-4768-87d4-22d89f2b3de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZXsXGDXWb_Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡idea : obterner la traduccion con gtranslate y otra por un llm. Pasarsela un segundo llm para que las compare y haga una version final. Esto parerec un caso de fusionar dos ramas\n",
        "\n",
        "Otra variante, producir en paralelo varias traducciones y fusionarlas en una sola"
      ],
      "metadata": {
        "id": "0IUB9BlJFgUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://python.langchain.com/docs/concepts/runnables/\n",
        "\n",
        "2. Info oficial: https://python.langchain.com/api_reference/core/runnables.html\n",
        "3. https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
        "\n",
        "4. https://dzone.com/articles/guide-to-langchain-runnable-architecture\n",
        "\n",
        "5. https://medium.com/@danushidk507/runnables-in-langchain-e6bfb7b9c0ca\n",
        "\n",
        "6. https://www.youtube.com/watch?v=8aUYzb1aYDU\n",
        "\n",
        "7. https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac\n",
        "\n",
        "8. https://medium.com/@ulrichw/list/langchain-lcel-85af4f4ff883\n",
        "\n",
        "9. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "10. https://www.pinecone.io/learn/series/langchain/langchain-expression-language/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJd-xV-oMuhQ"
      }
    }
  ]
}