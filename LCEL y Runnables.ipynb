{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeFmUqDT2Ag2qAgo24bAzJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/LCEL%20y%20Runnables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. LCEL, LangChain Expression Language**\n",
        "---\n",
        "\n",
        "LCEL se introdujo en Langchain **a mediados de 2023**, espec√≠ficamente con la **versi√≥n 0.0.142**, lanzada el **19 de julio de 2023**\n",
        "\n",
        "La introducci√≥n de **LCEL** en Langchain fue una respuesta a la necesidad de una forma m√°s potente, flexible y f√°cil de usar para construir aplicaciones de lenguaje complejas. Proporcion√≥ una sintaxis declarativa, mejor√≥ la legibilidad, facilit√≥ la depuraci√≥n y habilit√≥ funcionalidades avanzadas como el streaming y la ejecuci√≥n as√≠ncrona, consolid√°ndose como una pieza fundamental del ecosistema de Langchain.\n",
        "\n",
        "**LCEL se basa en la Composici√≥n, no en Tipos Predefinidos:**\n",
        "\n",
        "-   **Operador Pipe (|):** La piedra angular de LCEL es el operador pipe. Este operador te permite encadenar componentes de forma secuencial, enviando la salida de un componente como entrada al siguiente. Esto es inherentemente secuencial, pero no se define como un tipo de cadena \"secuencial\".\n",
        "    \n",
        "-   **Primitivas Runnable:** LCEL se basa en la interfaz Runnable. Cualquier objeto que implemente esta interfaz puede ser parte de una cadena LCEL. Esto incluye modelos de lenguaje, prompts, parsers, retrievers, etc.\n",
        "    \n",
        "-   **Flexibilidad Total:** La clave es que puedes combinar estas primitivas Runnable de cualquier manera que tenga sentido para tu aplicaci√≥n. No est√°s limitado a estructuras predefinidas.\n",
        "\n",
        "**Ventajas de este enfoque:**\n",
        "\n",
        "-   **Mayor Flexibilidad:** No est√°s limitado por las estructuras predefinidas. Puedes crear flujos de trabajo exactamente como los necesitas.\n",
        "    \n",
        "-   **Reutilizaci√≥n de Componentes:** Los componentes individuales pueden ser reutilizados en diferentes cadenas con diferentes flujos de trabajo.\n",
        "    \n",
        "-   **Claridad y Composici√≥n:** El uso del operador pipe hace que la l√≥gica de la cadena sea m√°s clara y f√°cil de entender.\n",
        "    \n",
        "-   **Optimizaci√≥n:** La forma en que construyes la cadena influye en c√≥mo se puede optimizar su ejecuci√≥n (por ejemplo, para paralelismo).\n",
        "    \n",
        "\n",
        "**En resumen, LCEL te proporciona las herramientas y la sintaxis para orquestar tus flujos de trabajo de manera flexible y poderosa. En lugar de imponer tipos de cadenas predefinidos, te da la libertad de construir las cadenas que mejor se adapten a tus necesidades, implementando patrones secuenciales, condicionales o paralelos seg√∫n sea necesario.**\n",
        "\n",
        "Y todo esta abstraci√≥n se basa en los **Runnables**"
      ],
      "metadata": {
        "id": "p9n7xOvQEQ6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Runnables**\n",
        "---\n",
        "\n",
        "Un *‚ÄúRunnable‚Äù* en LangChain es una abstracci√≥n (una interfaz en c√≥digo) que define un contrato para ejecutar una operaci√≥n. Concretamente, un `Runnable` expone un m√©todo `invoke(input)`, el cual recibe un dato de entrada y produce un resultado de salida. As√≠, cualquier clase u objeto que cumpla con esta interfaz puede encadenarse o combinarse con otros `Runnables` para construir flujos de trabajo complejos (por ejemplo, secuencias o ejecuciones en paralelo).   \n",
        "\n",
        "Cada \"*Runnable*\" implementa m√©todos como invoke, batch, stream, y transform, lo que lo hace compatible con diferentes modos de ejecuci√≥n (sincr√≥nica, asincr√≥nica, en lote, etc.).  \n",
        "Esto permite orquestar y reutilizar f√°cilmente distintas operaciones dentro de LangChain.\n",
        "\n",
        "Un poco menos t√©cnico...\n",
        "\n",
        "Un *‚ÄúRunnable‚Äù* es como una funci√≥n que sabe hacer algo muy concreto: recibe cierta informaci√≥n y devuelve un resultado. Lo importante es que, en LangChain, todos los *‚ÄúRunnables‚Äù* siguen la misma ‚Äúforma de trabajar‚Äù. Gracias a esto, podemos ir uniendo varios ‚ÄúRunnables‚Äù uno tras otro o en paralelo para crear procesos m√°s grandes sin tener que hacer ajustes complicados. En otras palabras, si cada bloque (*Runnable*) sabe c√≥mo recibir datos y devolverlos transformados, podemos combinar esos bloques como si fueran piezas de LEGO para armar flujos de trabajo completos.\n",
        "\n",
        "## Conceptos clave de runnables\n",
        "-   **Modularidad**\n",
        "    \n",
        "    -   Cada Runnable representa una √∫nica tarea u operaci√≥n (p.ej., ejecutar un modelo, procesar datos, encadenar operaciones).\n",
        "    -   Su dise√±o en ‚Äúbloques‚Äù facilita la independencia y el intercambio de componentes.\n",
        "-   **Componibilidad**\n",
        "    \n",
        "    -   Varios Runnables pueden vincularse para formar canalizaciones o flujos de trabajo complejos.\n",
        "    -   Esto permite crear soluciones m√°s grandes a partir de piezas peque√±as, flexibles y reutilizables.\n",
        "-   **Reutilizabilidad**\n",
        "    \n",
        "    -   Una vez definido, un Runnable se puede integrar en distintos proyectos sin modificaciones.\n",
        "    -   Es ideal para tareas est√°ndar que se repiten (p.ej., preprocesamiento de datos).\n",
        "-   **Ejecuci√≥n asincr√≥nica**\n",
        "    \n",
        "    -   Los Runnables pueden ejecutarse de forma as√≠ncrona, optimizando recursos y tiempo, especialmente cuando hay llamadas a servicios externos o E/S de por medio.\n",
        "\n",
        "-   **Ejecuci√≥n paralela**\n",
        "    \n",
        "    -   Es posible configurar Runnables para que se ejecuten en paralelo, lo que mejora el rendimiento en tareas por lotes o cuando se manejan grandes vol√∫menes de datos.\n",
        "-   **Manejo de errores**\n",
        "    \n",
        "    -   Suelen incluir mecanismos para capturar y gestionar excepciones, reforzando la solidez del flujo de trabajo.\n",
        "-   **Registro y depuraci√≥n**\n",
        "    \n",
        "    -   Admiten el registro de metadatos y eventos, lo que facilita rastrear y depurar la ejecuci√≥n de principio a fin.\n"
      ],
      "metadata": {
        "id": "TXNoJRERJ5l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Chains (Cadenas)**\n",
        "---\n",
        "\n",
        "En Langchain, una Cadena (Chain) representa una secuencia orquestada de llamadas a uno o m√°s Runnables para realizar una tarea espec√≠fica. Piensa en ella como una tuber√≠a o un flujo de trabajo donde la salida de un Runnable se convierte en la entrada del siguiente.  \n",
        "\n",
        "En esencia, una Cadena combina la funcionalidad de m√∫ltiples Runnables para llevar a cabo procesos m√°s complejos que los que un solo Runnable podr√≠a manejar individualmente. Las Cadenas son el mecanismo principal en Langchain para construir aplicaciones de lenguaje natural con l√≥gica y pasos definidos.\n",
        "\n",
        "En LCEL no existen tipos de cadenas predefinidos como \"secuenciales\", \"condicionales\" o \"paralelas\". Sin embargo, puedes implementar estos patrones de flujo de trabajo utilizando la sintaxis de LCEL junto con funciones adicionales.  "
      ],
      "metadata": {
        "id": "dVHiL2gmcWJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. ¬°Uniendo las piezas: Lleg√≥ la hora del c√≥digo!**\n",
        "---\n",
        "\n",
        "En esta secci√≥n, pondremos en pr√°ctica los conceptos te√≥ricos que hemos explorado. A trav√©s de una serie de ejemplos con c√≥digo, ilustraremos c√≥mo LCEL, los Runnables y las Cadenas se combinan en Langchain para resolver tareas concretas. Nuestro enfoque ser√° did√°ctico, facilitando la comprensi√≥n a la vez que presentamos casos de uso reales.\n",
        "\n",
        "** Langchain va en este momento por la version 0.3. Dado que es una librer√≠a en evoluci√≥n, es posible que se agreguen o refinen m√°s ‚Äúrunnables‚Äù en versiones futuras.\n",
        "\n"
      ],
      "metadata": {
        "id": "fG7vAQWRR8pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "- Instalamos la librer√≠a LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases espec√≠ficas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dej√°ndolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "Uv1UDKaYSP4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librer√≠as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Prompt + LLM**\n",
        "\n",
        "---\n",
        "Posiblemente esta es la cadena mas simple que podamos crear.  \n",
        "En este caso la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo.\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Observa que esto es lo que hemos hecho hasta ahora sin cadenas: Invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia y este enfoque puede ser m√°s simple para tareas sencillas, carece de la flexibilidad y la extensibilidad que ofrecen las cadenas como se aprecia en cuanto la complejidad escala.\n",
        "\n",
        "Ademas LCEL y la nuevas cadenas proporcionan un nivel de abstracci√≥n mayor tanto del prompt como del proceso en general."
      ],
      "metadata": {
        "id": "3r0n1rImzRrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos el prompt template de una de esta dos formas\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¬øc√≥mo est√°s?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Franc√©s\", \"texto\":\"Hola, ¬øc√≥mo est√°s?\"})\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "zfkcPT99zsbr",
        "outputId": "846ce421-6f78-44af-adc6-2e14ebcf076c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hola, ¬øc√≥mo est√°s? se traduce al franc√©s como: \"Bonjour, comment √ßa va ?\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Prompt + LLM + funcion personalizada de transformaci√≥n de la salida**\n",
        "---\n",
        "La forma principal de a√±adir una funci√≥n a una cadena en Langchain (especialmente en LCEL) es utilizando el `RunnableLambda`. `RunnableLambda` es un Runnable que envuelve una funci√≥n Python, permiti√©ndole integrarse perfectamente en el flujo de la cadena.\n",
        "\n",
        "(Aunque es posible, la inserci√≥n directa de la funci√≥n `transformar_texto` funciona, esto se debe a que Langchain impl√≠citamente envuelve esa funci√≥n en un RunnableLambda \"por debajo del cap√≥\" para que pueda encajar dentro del paradigma de la cadena LCEL. Es mejor ser consistente con los principios del framework y usar `RunnableLambda`)\n",
        "\n",
        "En este caso la salida del LLM se procesa con una funcion sencilla para pasarla a mayusculas. (pero puede ser tan complicada como se necesite)\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un objeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar.\n",
        "\n"
      ],
      "metadata": {
        "id": "6KCFlW3j1zWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# # Definimos una funci√≥n de transformaci√≥n\n",
        "# def transformar_texto(output):\n",
        "#     return output.content.upper()\n",
        "\n",
        "\n",
        "transformar_texto=RunnableLambda(lambda output: output.content.upper())\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¬øQu√© es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "N-YvGoOb16Aa",
        "outputId": "e1258f3b-8b29-4645-c533-b4be8088fd7d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "EL OVERFITTING, O SOBREAJUSTE, ES UN FEN√ìMENO EN EL QUE UN MODELO DE MACHINE LEARNING SE AJUSTA DEMASIADO A LOS DATOS DE ENTRENAMIENTO, CAPTURANDO NO SOLO LAS RELACIONES SUBYACENTES, SINO TAMBI√âN EL RUIDO Y LAS VARIACIONES ALEATORIAS EN ESOS DATOS. COMO RESULTADO, AUNQUE EL MODELO PUEDE MOSTRAR UN RENDIMIENTO EXCEPCIONAL EN EL CONJUNTO DE ENTRENAMIENTO, SU CAPACIDAD PARA GENERALIZAR A DATOS NO VISTOS (COMO UN CONJUNTO DE PRUEBA O EN PRODUCCI√ìN) SE VE SERIAMENTE COMPROMETIDA, LO QUE LLEVA A UN RENDIMIENTO DEFICIENTE.\n\n### CAUSAS DEL OVERFITTING:\n1. **MODELO COMPLEJO:** UN MODELO CON DEMASIADOS PAR√ÅMETROS O UNA ARQUITECTURA MUY COMPLEJA PUEDE APRENDER PATRONES ESPEC√çFICOS DE LOS DATOS DE ENTRENAMIENTO.\n2. **DATOS INSUFICIENTES:** CON UN TAMA√ëO DE MUESTRA PEQUE√ëO, ES M√ÅS PROBABLE QUE EL MODELO CAPTE EL RUIDO EN LUGAR DE LAS TENDENCIAS GENERALES.\n3. **RUIDO EN LOS DATOS:** LA PRESENCIA DE ERRORES O VARIACIONES ALEATORIAS PUEDE LLEVAR AL MODELO A AJUSTARSE A ESTAS IRREGULARIDADES.\n\n### S√çNTOMAS DEL OVERFITTING:\n- ALTA PRECISI√ìN EN EL CONJUNTO DE ENTRENAMIENTO.\n- BAJA PRECISI√ìN EN EL CONJUNTO DE VALIDACI√ìN O PRUEBA.\n- UN MODELO QUE PARECE \"MEMORIZAR\" LOS DATOS EN LUGAR DE APRENDER PATRONES SIGNIFICATIVOS.\n\n### ESTRATEGIAS PARA MITIGAR EL OVERFITTING:\n1. **REGULARIZACI√ìN:** M√âTODOS COMO L1 (LASSO) O L2 (RIDGE) QUE A√ëADEN UN T√âRMINO DE PENALIZACI√ìN AL COSTO DEL MODELO PARA EVITAR QUE LOS PAR√ÅMETROS SE VUELVAN DEMASIADO GRANDES.\n2. **VALIDACI√ìN CRUZADA:** UTILIZAR T√âCNICAS DE VALIDACI√ìN CRUZADA PARA ASEGURARSE DE QUE EL MODELO GENERALIZA BIEN A DIFERENTES SUBCONJUNTOS DE DATOS.\n3. **REDUCCI√ìN DE COMPLEJIDAD:** OPTAR POR MODELOS M√ÅS SIMPLES O REDUCIR EL N√öMERO DE CARACTER√çSTICAS MEDIANTE T√âCNICAS DE SELECCI√ìN DE CARACTER√çSTICAS.\n4. **AUMENTO DE DATOS:** GENERAR DATOS SINT√âTICOS O UTILIZAR T√âCNICAS DE AUMENTO DE DATOS PARA INCREMENTAR EL TAMA√ëO DEL CONJUNTO DE ENTRENAMIENTO.\n5. **EARLY STOPPING:** MONITOREAR EL RENDIMIENTO DEL MODELO EN UN CONJUNTO DE VALIDACI√ìN Y DETENER EL ENTRENAMIENTO CUANDO EL RENDIMIENTO COMIENZA A DETERIORARSE.\n\nEN RESUMEN, EL OVERFITTING ES UN DESAF√çO CR√çTICO EN EL DESARROLLO DE MODELOS DE MACHINE LEARNING Y REQUIERE ATENCI√ìN CUIDADOSA PARA ASEGURAR QUE LOS MODELOS SEAN ROBUSTOS Y CAPACES DE GENERALIZAR A DATOS NO VISTOS."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prompt + LLM + OutputParser + funcion personalizada de transformaci√≥n de la salida**\n",
        "---\n",
        "\n",
        "En este ejemplo se observa mejor la modularidad reutilizando la cadena en un bucle y usando un OutputParser.  \n",
        "\n",
        "**Los OutputParser en Langchain son Runnables**. Esto les permite ser componentes activos dentro de las cadenas LCEL, tomando la salida de Runnables precedentes y transform√°ndola seg√∫n su l√≥gica espec√≠fica. Su capacidad para ser Runnables es fundamental para la flexibilidad y el poder de composici√≥n de Langchain."
      ],
      "metadata": {
        "id": "ZdMVH9kL6uOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una funci√≥n de transformaci√≥n\n",
        "# Esta funci√≥n toma la salida del OutPutParser (str) y la convierte a may√∫sculas.\n",
        "transformar_texto=RunnableLambda(lambda output: output.upper())\n",
        "\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas en un bucle\n",
        "# Observa c√≥mo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¬øQu√© es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¬øQui√©n fue Napole√≥n Bonaparte?\"},\n",
        "    {\"tema\": \"Programaci√≥n\", \"pregunta\": \"¬øQu√© es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "VKkDds_W6ymg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Prompt + LLM + OutputParser + Guardar en fichero**\n",
        "Crea una cadena que genera una lista de elementos y la guarda en un archivo:\n",
        "\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "4yMTStcf-fPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No a√±adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Creamos la cadena con LCEL y RunnableLambda\n",
        "def guardar_en_archivo_y_mostrar(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Podriamos no hacer esto y meter la funcion directamente en la cadena\n",
        "# Metemos la funcion en un Runnable, reutilizamos la variable\n",
        "guardar_en_archivo_y_mostrar = RunnableLambda(guardar_en_archivo_y_mostrar)\n",
        "\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo_y_mostrar\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en 'mente'\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "aA2_DRpA9gbe",
        "outputId": "50e408d7-ea15-41cd-8e72-fcc79f781be1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r√°pidamente  \n",
            "suavemente  \n",
            "f√°cilmente  \n",
            "silenciosamente  \n",
            "amablemente  \n",
            "eficazmente  \n",
            "despacio  \n",
            "claro  \n",
            "junto  \n",
            "bien\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Respuesta guardada en 'respuesta.txt'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + JsonOutputParser**\n",
        "---\n",
        "\n",
        "Esta vez genera una respuesta JSON.\n",
        "\n",
        "SimpleJsonOutputParser transforma la salida del modelo en un objeto JSON v√°lido."
      ],
      "metadata": {
        "id": "-oRstVkU_SOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install icecream -qU\n",
        "from icecream import ic\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Resultado(BaseModel):\n",
        "    persona: str = Field(description=\"Nombre de la persona\")\n",
        "    aportacion: str = Field(description=\"Importe en euros\")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto. Devuelve el resultado en formato JSON con las claves 'persona' y 'aportaci√≥n'. No a√±adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"pregunta\": \"Inventa 10 personas ficticias y un importe en euros para cada una de ellas\"})\n",
        "ic(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueUefLwJ_2Ki",
        "outputId": "61284536-2d55-40a8-e18d-2473679f9171"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| result: {'personas': [{'aportaci√≥n': 1500, 'persona': 'Lucas Mart√≠nez'},\n",
            "                          {'aportaci√≥n': 2000, 'persona': 'Sof√≠a Rodr√≠guez'},\n",
            "                          {'aportaci√≥n': 1200, 'persona': 'Mart√≠n G√≥mez'},\n",
            "                          {'aportaci√≥n': 1800, 'persona': 'Isabel L√≥pez'},\n",
            "                          {'aportaci√≥n': 2500, 'persona': 'Javier P√©rez'},\n",
            "                          {'aportaci√≥n': 1600, 'persona': 'Clara Herrera'},\n",
            "                          {'aportaci√≥n': 2200, 'persona': 'Diego Fern√°ndez'},\n",
            "                          {'aportaci√≥n': 1300, 'persona': 'Ana S√°nchez'},\n",
            "                          {'aportaci√≥n': 1700, 'persona': 'David Ram√≠rez'},\n",
            "                          {'aportaci√≥n': 1400, 'persona': 'Luc√≠a Torres'}]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'personas': [{'persona': 'Lucas Mart√≠nez', 'aportaci√≥n': 1500},\n",
              "  {'persona': 'Sof√≠a Rodr√≠guez', 'aportaci√≥n': 2000},\n",
              "  {'persona': 'Mart√≠n G√≥mez', 'aportaci√≥n': 1200},\n",
              "  {'persona': 'Isabel L√≥pez', 'aportaci√≥n': 1800},\n",
              "  {'persona': 'Javier P√©rez', 'aportaci√≥n': 2500},\n",
              "  {'persona': 'Clara Herrera', 'aportaci√≥n': 1600},\n",
              "  {'persona': 'Diego Fern√°ndez', 'aportaci√≥n': 2200},\n",
              "  {'persona': 'Ana S√°nchez', 'aportaci√≥n': 1300},\n",
              "  {'persona': 'David Ram√≠rez', 'aportaci√≥n': 1700},\n",
              "  {'persona': 'Luc√≠a Torres', 'aportaci√≥n': 1400}]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nDjGaPbD2NU",
        "outputId": "c2f836a2-79bd-453a-cf08-1253b1bc50c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'personas': [{'persona': 'Lucas Mart√≠nez', 'aportaci√≥n': 1500},\n",
              "  {'persona': 'Sof√≠a Rodr√≠guez', 'aportaci√≥n': 2000},\n",
              "  {'persona': 'Mart√≠n G√≥mez', 'aportaci√≥n': 1200},\n",
              "  {'persona': 'Isabel L√≥pez', 'aportaci√≥n': 1800},\n",
              "  {'persona': 'Javier P√©rez', 'aportaci√≥n': 2500},\n",
              "  {'persona': 'Clara Herrera', 'aportaci√≥n': 1600},\n",
              "  {'persona': 'Diego Fern√°ndez', 'aportaci√≥n': 2200},\n",
              "  {'persona': 'Ana S√°nchez', 'aportaci√≥n': 1300},\n",
              "  {'persona': 'David Ram√≠rez', 'aportaci√≥n': 1700},\n",
              "  {'persona': 'Luc√≠a Torres', 'aportaci√≥n': 1400}]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí°idea : obterner la traduccion con gtranslate y otra por un llm. Pasarsela un segundo llm para que las compare y haga una version final. Esto parerec un caso de fusionar dos ramas\n",
        "\n",
        "Otra variante, producir en paralelo varias traducciones y fusionarlas en una sola"
      ],
      "metadata": {
        "id": "0IUB9BlJFgUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://python.langchain.com/docs/concepts/runnables/\n",
        "\n",
        "2. Info oficial: https://python.langchain.com/api_reference/core/runnables.html\n",
        "3. https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
        "\n",
        "4. https://dzone.com/articles/guide-to-langchain-runnable-architecture\n",
        "\n",
        "5. https://medium.com/@danushidk507/runnables-in-langchain-e6bfb7b9c0ca\n",
        "\n",
        "6. https://www.youtube.com/watch?v=8aUYzb1aYDU\n",
        "\n",
        "7. https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac\n",
        "\n",
        "8. https://medium.com/@ulrichw/list/langchain-lcel-85af4f4ff883\n",
        "\n",
        "9. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "10. https://www.pinecone.io/learn/series/langchain/langchain-expression-language/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oJd-xV-oMuhQ"
      }
    }
  ]
}