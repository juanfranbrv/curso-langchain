{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQFZXzDEsrQPBVbT3hQwIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Output%20Parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output parsers en Langchain**\n",
        "---\n",
        "Imagina que le preguntas a un LLM \"¿Cuáles son los tres planetas más cercanos al sol?\" y te responde: \"Mercurio, Venus y la Tierra son los planetas más cercanos al sol\". Si bien la respuesta es correcta para un humano, para que tu programa pueda usar esa información, lo ideal sería tenerla en un formato más manejable, como una lista o un objeto JSON. Aquí es donde entran en juego los Output Parsers.  \n",
        "\n",
        "Los Output Parsers te permiten “forzar” o “guiar” al modelo para que devuelva la información según un formato deseado (por ejemplo, un JSON con campos específicos, una lista, etc).\n",
        "\n",
        "LangChain ofrece una variedad de Output Parsers preconstruidos para diferentes necesidades. Aquí te presento algunos de los más comunes:\n",
        "\n",
        "- **StrOutputParser:** Convierte la salida a string\n",
        "\n",
        "-   **StructuredOutputParser:** Analiza la salida en una estructura predefinida con campos.\n",
        "    \n",
        "-   **OutputFixingParser:** Corrige errores en el parsing estructurado usando un LLM.\n",
        "    \n",
        "-   **CommaSeparatedListOutputParser:** Convierte la salida en una lista de strings separados por comas\n",
        "    \n",
        "-   **EnumOutputParser:** Espera que el LLM elija de un conjunto de opciones.\n",
        "    \n",
        "-   **BooleanOutputParser:** Analiza la salida como Verdadero o Falso.\n",
        "    \n",
        "-   **PydanticOutputParser:** Convierte la salida a un modelo de datos Pydantic.\n",
        "    \n",
        "-   **RegexParser:** Extrae información usando expresiones regulares.\n",
        "\n",
        "\n",
        "OutputFixingParser: ?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry8P2gp7VSGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihaYmy1rgga_",
        "outputId": "07ccec8c-a4bd-43c0-a3e9-7d311b5eb958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# # Importamos las clases necesarias para trabajar con cadenas\n",
        "# from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. StructuredOutputParser**\n",
        "\n",
        "Permite analizar la salida del LLM en una estructura predefinida. Le proporcionas un esquema (generalmente una lista de ResponseSchema que definen los campos esperados) y el parser intenta extraer la información y mapearla a ese esquema.\n",
        "\n",
        "**Casos de uso:** Extraer información específica de un texto, como atributos de un producto, detalles de un evento, o datos de contacto. Es muy versátil para obtener datos estructurados.\n",
        "\n",
        "**Ventaja:** Flexible y permite definir la estructura esperada de la salida.\n",
        "\n",
        "Veamos un ejemplo en el que el resultado será un diccionario estructurado con los campos nombre, edad y email, listo para ser utilizado en tu aplicación."
      ],
      "metadata": {
        "id": "4Zlw_DQiZPI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"nombre\", description=\"El nombre del usuario\"),\n",
        "    ResponseSchema(name=\"edad\", description=\"La edad del usuario\"),\n",
        "    ResponseSchema(name=\"email\", description=\"El correo electrónico del usuario\")\n",
        "]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "F4pnLdNTZP9r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obtener instrucciones de formato (opcional pero recomendado):** Muchos parsers tienen un método get\\_format\\_instructions() que devuelve texto que puedes incluir en tu prompt para guiar al LLM sobre el formato esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDsh5c2vaw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "id": "2ov8pnCtaeFT",
        "outputId": "a682c30a-6a07-4bd9-95a3-ad4a09c48427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"nombre\": string  // El nombre del usuario\\n\\t\"edad\": string  // La edad del usuario\\n\\t\"email\": string  // El correo electrónico del usuario\\n}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera información de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "p1_T9nPdbCIu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sobre las `partial variables`\n",
        "Las **partial\\_variables** dentro de un PromptTemplate son una forma de **pre-cargar o fijar ciertos valores dentro de la plantilla del prompt antes de que se proporcionen las variables de entrada principales, es decir cuando se crea el propmpt template y de antes de que se formatee**\n",
        "\n",
        "Piénsalo de esta manera: un PromptTemplate es como una plantilla de texto con \"huecos\" que necesitas llenar para crear un prompt completo para el LLM. Hay dos formas principales de llenar estos huecos:\n",
        "\n",
        "-   **input\\_variables:** Estas son las variables que **cambian** cada vez que utilizas el prompt. Son los datos específicos que quieres que el LLM procese en cada llamada.\n",
        "    \n",
        "-   **partial\\_variables:** Estas son las variables que tienen un valor **fijo** o **predefinido** para un uso particular del PromptTemplate. No cambian con cada llamada a la cadena o LLM que usa este prompt.\n",
        "    \n",
        "\n",
        "**En el codigo anterior:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "prompt = PromptTemplate(\n",
        "            template=\"Genera información de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "            input_variables=[],\n",
        "            partial_variables={\"format_instructions\": format_instructions}\n",
        "```\n",
        "En otras palabras, son una forma de incluir en el prompt template una parte fija, pero mediante una variable, lo que proporciona mas flexibilidad programatica.\n",
        "En este caso metemos en el prompt, las instruciones que nos ha devuelto el parser."
      ],
      "metadata": {
        "id": "faC9fR3acUzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "respuesta = llm.invoke(prompt.format()) # la llamada no requiere ningun parámetro\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "respuesta_analizada\n",
        "\n"
      ],
      "metadata": {
        "id": "WrZPCFGieWkY",
        "outputId": "023a375a-32be-43bb-8337-442c7c347b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nombre': 'Juan Pérez', 'edad': '28', 'email': 'juan.perez@example.com'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. ListOutputParser:**\n",
        "\n",
        "Analiza la salida del LLM y la convierte en una lista de strings. Puedes especificar un separador para dividir el texto en elementos de la lista.\n",
        "    \n",
        "-   **Casos de uso:** Obtener listas de elementos, como nombres, ideas, pasos a seguir, o categorías.\n",
        "    \n",
        "-   **Ventaja:** Simple y efectivo para extraer listas.\n",
        "\n",
        "Vamos a hacer un ejemplo en el que queremos obtener una lista de ingredientes para hacer una pizza casera.\n",
        "\n"
      ],
      "metadata": {
        "id": "flcuOq9Nh6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera una lista de ingredientes para hacer una pizza casera. Solo lista los ingredientes, uno por línea.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "AySI2dvhi78N",
        "outputId": "7e861d71-ea2a-493b-8e63-20a2113aec32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['harina', 'agua', 'levadura', 'sal', 'aceite de oliva', 'tomate triturado', 'queso mozzarella', 'orégano', 'jamon', 'pepperoni', 'verduras (pimiento', 'cebolla', 'champiñones)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. EnumOutputParser**\n",
        "\n",
        "Es útil cuando esperas que el LLM elija entre un conjunto predefinido de opciones (un enum de Python).\n",
        "\n",
        "Casos de uso: Clasificación, selección de categorías, o elegir una opción de un menú.\n",
        "\n",
        "Ventaja: Garantiza que la salida se encuentre dentro de un conjunto de valores válidos.Imaginemos que estamos construyendo un sistema de recomendación de libros, donde el modelo debe elegir un género literario de una lista predefinida y luego recomendar un libro de ese género.\n",
        "\n",
        "❗ Este ejemplo no funciona ❓ -> porque este parser es mas bien para \"forzar\" a elegir al modelo una de las opciones. Obsrva que el \"formato\" que el parser espera (instructions) se incluye en el prompt para \"forzar\" la respuesta del modelo y luego el parser espera poder recoger la salida, pero a veces puede no estar bien"
      ],
      "metadata": {
        "id": "lj97SmlAofVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definir las opciones del enumerado\n",
        "class ColoresOjos(Enum):\n",
        "    MARRÓN = \"marrón\"\n",
        "    AVELANA = \"avellana\"\n",
        "    ÁMBAR = \"ámbar\"\n",
        "    VERDE = \"verde\"\n",
        "    AZUL = \"azul\"\n",
        "    GRIS = \"gris\"\n",
        "\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=ColoresOjos)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"De que color tenia o tiene los ojos esta persona: {persona}.\",\n",
        "    input_variables=[\"persona\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format(persona= \"Frank Sinatra\")\n",
        "output = llm.invoke(input_prompt).content.lower()\n",
        "\n",
        "parsed_output = output_parser.parse(output)\n",
        "\n",
        "# Parsear la salida\n",
        "print(f\"El género elegido es: {parsed_output}\")\n"
      ],
      "metadata": {
        "id": "tCY84L22o-Ki",
        "outputId": "20fdf0a1-8dc9-426d-cb95-0780fabb115f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Response 'azul.' is not one of the expected values: ['marrón', 'avellana', 'ámbar', 'verde', 'azul', 'gris']\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/enum.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simple value lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mve_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'azul.' is not a valid ColoresOjos",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f2d52adf4aa8>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mparsed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Parsear la salida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/enum.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             raise OutputParserException(\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0;34mf\"Response '{response}' is not one of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;34mf\"expected values: {self._valid_values}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Response 'azul.' is not one of the expected values: ['marrón', 'avellana', 'ámbar', 'verde', 'azul', 'gris']\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "# Definir los géneros literarios como un Enum\n",
        "class GenerosLiterarios(Enum):\n",
        "    FANTASÍA = \"fantasía\"\n",
        "    CIENCIA_FICCIÓN = \"ciencia ficción\"\n",
        "    MISTERIO = \"misterio\"\n",
        "    ROMANCE = \"romance\"\n",
        "    HISTÓRICA = \"novela histórica\"\n",
        "    NO_FICCIÓN = \"no ficción\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=GenerosLiterarios)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Elige un género literario de la siguiente lista: {generos_literarios}.\\nLuego, recomienda un libro de ese género.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\n",
        "        \"generos_literarios\": \", \".join([g.value for g in GenerosLiterarios]),\n",
        "        \"format_instructions\": output_parser.get_format_instructions()\n",
        "    }\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "print(input_prompt)\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "try:\n",
        "    genero_elegido = output_parser.parse(respuesta.content)\n",
        "    print(f\"El género elegido es: {genero_elegido.value}\")\n",
        "\n",
        "    # Ahora pedimos una recomendación de libro para el género elegido\n",
        "    recomendacion_prompt = PromptTemplate(\n",
        "        template=\"Recomienda un libro de género {genero}.\",\n",
        "        input_variables=[\"genero\"]\n",
        "    )\n",
        "    recomendacion_input = recomendacion_prompt.format(genero=genero_elegido.value)\n",
        "    recomendacion = llm.invoke(recomendacion_input)\n",
        "\n",
        "    print(f\"Recomendación: {recomendacion.content}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "cooXmCR3zwNO",
        "outputId": "eccf1e38-14bd-444a-bf52-21536a081ba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elige un género literario de la siguiente lista: fantasía, ciencia ficción, misterio, romance, novela histórica, no ficción.\n",
            "Luego, recomienda un libro de ese género.\n",
            "Select one of the following options: fantasía, ciencia ficción, misterio, romance, novela histórica, no ficción\n",
            "\n",
            "Error: Response 'Elijo el género de **fantasía**. \n",
            "\n",
            "Te recomiendo el libro **\"El nombre del viento\"** de Patrick Rothfuss. Es la primera entrega de la trilogía \"Crónica del asesino de reyes\" y sigue la historia de Kvothe, un joven prodigio que se convierte en un legendario héroe. La narrativa es rica y envolvente, con un mundo bien construido lleno de magia, música y aventuras. ¡Espero que lo disfrutes!' is not one of the expected values: ['fantasía', 'ciencia ficción', 'misterio', 'romance', 'novela histórica', 'no ficción']\n",
            "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import RegexParser\n",
        "\n",
        "\n",
        "# Definir la expresión regular para extraer nombre y edad\n",
        "regex_pattern = r\"Nombre: (?P<nombre>.+)\\nEdad: (?P<edad>\\d+)\"\n",
        "\n",
        "# Crear el RegexParser\n",
        "output_parser = RegexParser(regex=regex_pattern, output_keys=[\"nombre\", \"edad\"])\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera información de una persona ficticia, incluyendo su nombre y edad.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": \"\"}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "u0zW0wz-v8yf",
        "outputId": "1afa1ef7-68b9-4fe7-af9e-bdeccc4e8c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nombre': 'Valeria Mendoza  ', 'edad': '28'}\n"
          ]
        }
      ]
    }
  ]
}