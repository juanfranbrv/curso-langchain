{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3GfdndXn325UodxWGp/W+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Output%20Parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output parsers en Langchain**\n",
        "---\n",
        "Imagina que le preguntas a un LLM \"¿Cuáles son los tres planetas más cercanos al sol?\" y te responde: \"Mercurio, Venus y la Tierra son los planetas más cercanos al sol\". Si bien la respuesta es correcta para un humano, para que tu programa pueda usar esa información, lo ideal sería tenerla en un formato más manejable, como una lista o un objeto JSON. Aquí es donde entran en juego los Output Parsers.  \n",
        "\n",
        "Los Output Parsers te permiten “forzar” o “guiar” al modelo para que devuelva la información según un formato deseado (por ejemplo, un JSON con campos específicos, una lista, etc).  \n",
        "\n",
        "LangChain ofrece una variedad de Output Parsers preconstruidos para diferentes necesidades. Algunos de los mas usados son estos:\n",
        "\n",
        "- **StrOutputParser:** Convierte la salida a string\n",
        "\n",
        "-   **StructuredOutputParser:** Analiza la salida en una estructura predefinida con campos.\n",
        "    \n",
        "-   **OutputFixingParser:** Corrige errores en el parsing estructurado usando un LLM.\n",
        "\n",
        "- RetryWithError\n",
        "    \n",
        "-   **CommaSeparatedListOutputParser:** Convierte la salida en una lista de strings separados por comas\n",
        "    \n",
        "-   **EnumOutputParser:** Espera que el LLM elija de un conjunto de opciones.\n",
        "    \n",
        "-   **BooleanOutputParser:** Analiza la salida como Verdadero o Falso.\n",
        "    \n",
        "-   **PydanticOutputParser:** Convierte la salida a un modelo de datos Pydantic.\n",
        "    \n",
        "-   **RegexParser:** Extrae información usando expresiones regulares.\n",
        "\n",
        "- JSON ??\n",
        "- YAML\n",
        "- XML\n",
        "-DATETIME\n",
        "\n",
        "tODOS AQUI: https://python.langchain.com/docs/concepts/output_parsers/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "OutputFixingParser: ?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry8P2gp7VSGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**0. Preparando el entorno del cuaderno**\n",
        "\n"
      ],
      "metadata": {
        "id": "KEWPJUQdCird"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# # Importamos las clases necesarias para trabajar con cadenas\n",
        "# from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. StructuredOutputParser**\n",
        "Un analizador de salida estructurado está diseñado para extraer y estructurar información de texto no estructurado o semiestructurado. Esto es adecuado para situaciones que requieren la extracción de múltiples campos.\n",
        "\n",
        "Permite analizar la salida del LLM en una estructura predefinida. Le proporcionas un esquema (generalmente una lista de ResponseSchema que definen los campos esperados) y el parser intenta extraer la información y mapearla a ese esquema.\n",
        "\n",
        "Aunque el analizador Pydantic (explicado a continuación) ofrece capacidades más sólidas, StructuredOutputParser es ventajoso para su uso con modelos más simples.\n",
        "\n",
        "**Casos de uso:** Extraer información específica de un texto, como atributos de un producto, detalles de un evento, o datos de contacto. Es muy versátil para obtener datos estructurados.\n",
        "\n",
        "**Ventaja:** Flexible y permite definir la estructura esperada de la salida.\n",
        "\n",
        "Veamos un ejemplo en el que el resultado será un diccionario estructurado con los campos nombre, edad y email, listo para ser utilizado en la aplicación."
      ],
      "metadata": {
        "id": "4Zlw_DQiZPI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"nombre\", description=\"El nombre del usuario\"),\n",
        "    ResponseSchema(name=\"edad\", description=\"La edad del usuario\"),\n",
        "    ResponseSchema(name=\"email\", description=\"El correo electrónico del usuario\")\n",
        "]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "F4pnLdNTZP9r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obtener instrucciones de formato (opcional pero recomendado):** Muchos parsers tienen un método get\\_format\\_instructions() que devuelve texto que puedes incluir en tu prompt para guiar al LLM sobre el formato esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDsh5c2vaw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "id": "2ov8pnCtaeFT",
        "outputId": "25651e04-cd26-4623-f3cf-24e534454b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"nombre\": string  // El nombre del usuario\\n\\t\"edad\": string  // La edad del usuario\\n\\t\"email\": string  // El correo electrónico del usuario\\n}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera información de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "p1_T9nPdbCIu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sobre las `partial variables`\n",
        "Las **partial\\_variables** dentro de un PromptTemplate son una forma de **pre-cargar o fijar ciertos valores dentro de la plantilla del prompt antes de que se proporcionen las variables de entrada principales, es decir cuando se crea el propmpt template y antes de que se formatee**\n",
        "\n",
        "Piénsalo de esta manera: un PromptTemplate es como una plantilla de texto con \"huecos\" que necesitas llenar para crear un prompt completo para el LLM. Hay dos formas principales de llenar estos huecos:\n",
        "\n",
        "-   **input\\_variables:** Estas son las variables que **cambian** cada vez que utilizas el prompt. Son los datos específicos que quieres que el LLM procese en cada llamada.\n",
        "    \n",
        "-   **partial\\_variables:** Estas son las variables que tienen un valor **fijo** o **predefinido** para un uso particular del PromptTemplate. No cambian con cada llamada a la cadena o LLM que usa este prompt.\n",
        "    \n",
        "\n",
        "**En el codigo anterior:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "prompt = PromptTemplate(\n",
        "            template=\"Genera información de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "            input_variables=[],\n",
        "            partial_variables={\"format_instructions\": format_instructions}\n",
        "```\n",
        "En otras palabras, son una forma de incluir en el prompt template una parte fija, pero mediante una variable, lo que proporciona mas flexibilidad programatica.\n",
        "En este caso metemos en el prompt, las instruciones que nos ha devuelto el parser."
      ],
      "metadata": {
        "id": "faC9fR3acUzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "respuesta = llm.invoke(prompt.format()) # la llamada no requiere ningun parámetro\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "respuesta_analizada\n",
        "\n"
      ],
      "metadata": {
        "id": "WrZPCFGieWkY",
        "outputId": "bec6094c-c9b0-49ee-ed53-b6f81158cbf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nombre': 'Laura Gómez', 'edad': '28', 'email': 'laura.gomez@example.com'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AQui otro ejemplo de StructuredOutputParser (que luego e resolverá tambien con Pydantic)\n"
      ],
      "metadata": {
        "id": "4-C1hmhItipv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNxAZq7RtiNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. PydanticOutputParser: Convierte la salida a un modelo de datos Pydantic.**"
      ],
      "metadata": {
        "id": "4NbToCP9tt-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Resolver el ejercicio anterior con PydanticOutputParser"
      ],
      "metadata": {
        "id": "dEJ_Wd1Lt0Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otro ejercicio con pydantic"
      ],
      "metadata": {
        "id": "BnYJ6p9zt4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Resolver el ejercicio"
      ],
      "metadata": {
        "id": "Ba2fHvFJt4VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. JSONoutputParser**"
      ],
      "metadata": {
        "id": "626Vr1c5uzj9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wgjgzWrSvjSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bofNcvPzvhUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. CommaSeparatedListOutputParser:**\n",
        "\n",
        "Analiza la salida del LLM y la convierte en una lista de strings. Puedes especificar un separador para dividir el texto en elementos de la lista.\n",
        "    \n",
        "-   **Casos de uso:** Obtener listas de elementos, como nombres, ideas, pasos a seguir, o categorías.\n",
        "    \n",
        "-   **Ventaja:** Simple y efectivo para extraer listas.\n",
        "\n",
        "Vamos a hacer un ejemplo en el que queremos obtener una lista de ingredientes para hacer una pizza casera.\n",
        "\n"
      ],
      "metadata": {
        "id": "flcuOq9Nh6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera una lista de ingredientes para hacer una pizza casera. Solo lista los ingredientes, uno por línea.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "AySI2dvhi78N",
        "outputId": "f09142b2-d50f-4100-bd5f-38a73d98cdc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['harina', 'agua', 'levadura', 'sal', 'aceite de oliva', 'salsa de tomate', 'queso mozzarella', 'pepperoni', 'oregano', 'albahaca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. RegexOutputParser**"
      ],
      "metadata": {
        "id": "Ah3HuvErvNl3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NjKPw8TZvmdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSpQlpy-vmDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. EnumOutputParser**\n",
        "\n",
        "Es útil cuando esperas que el LLM elija entre un conjunto predefinido de opciones (un enum de Python).\n",
        "\n",
        "Casos de uso: Clasificación, selección de categorías, o elegir una opción de un menú.\n",
        "\n",
        "Ventaja: Garantiza que la salida se encuentre dentro de un conjunto de valores válidos. Imaginemos que estamos construyendo un sistema de recomendación de libros, donde el modelo debe elegir un género literario de una lista predefinida y luego recomendar un libro de ese género.\n",
        "\n",
        "❗ Este ejemplo no funciona ❓ -> porque este parser es mas bien para \"forzar\" a elegir al modelo una de las opciones. Observa que el \"formato\" que el parser espera (instructions) se incluye en el prompt para \"forzar\" la respuesta del modelo y luego el parser espera poder recoger la salida, pero a veces puede no estar bien"
      ],
      "metadata": {
        "id": "lj97SmlAofVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definir las opciones del enumerado\n",
        "class ColoresOjos(Enum):\n",
        "    MARRÓN = \"marrón\"\n",
        "    AVELANA = \"avellana\"\n",
        "    ÁMBAR = \"ámbar\"\n",
        "    VERDE = \"verde\"\n",
        "    AZUL = \"azul\"\n",
        "    GRIS = \"gris\"\n",
        "\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=ColoresOjos)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"De que color tenia o tiene los ojos esta persona: {persona}.\",\n",
        "    input_variables=[\"persona\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format(persona= \"Frank Sinatra\")\n",
        "output = llm.invoke(input_prompt).content.lower()\n",
        "\n",
        "parsed_output = output_parser.parse(output)\n",
        "\n",
        "# Parsear la salida\n",
        "print(f\"El género elegido es: {parsed_output}\")\n"
      ],
      "metadata": {
        "id": "tCY84L22o-Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo realizamos dos llamadas al modelo. En la primera le pedimos que escoja entre una opcion del Enum, y con ella realizamos la segunda invocación."
      ],
      "metadata": {
        "id": "jCgtmxHREQtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "# Definir los géneros literarios como un Enum\n",
        "class GenerosLiterarios(Enum):\n",
        "    FANTASÍA = \"fantasía\"\n",
        "    CIENCIA_FICCIÓN = \"ciencia ficción\"\n",
        "    MISTERIO = \"misterio\"\n",
        "    ROMANCE = \"romance\"\n",
        "    HISTÓRICA = \"novela histórica\"\n",
        "    NO_FICCIÓN = \"no ficción\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=GenerosLiterarios)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Elige un género literario de la siguiente lista: {generos_literarios}.\\nResponde solo con el genero escogido \\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\n",
        "        \"generos_literarios\": \", \".join([g.value for g in GenerosLiterarios]),\n",
        "        \"format_instructions\": output_parser.get_format_instructions()\n",
        "    }\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=2)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "print(input_prompt)\n",
        "respuesta = llm.invoke(input_prompt).content.lower()\n",
        "print (respuesta)\n",
        "\n"
      ],
      "metadata": {
        "id": "cooXmCR3zwNO",
        "outputId": "d21e0e09-6308-4495-b34d-b6dd3e0d2810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elige un género literario de la siguiente lista: fantasía, ciencia ficción, misterio, romance, novela histórica, no ficción.\n",
            "Responde solo con el genero escogido \n",
            "Select one of the following options: fantasía, ciencia ficción, misterio, romance, novela histórica, no ficción\n",
            "\n",
            "fantasía\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsear la salida\n",
        "try:\n",
        "    genero_elegido = output_parser.parse(respuesta)\n",
        "    print(f\"El género elegido es: {genero_elegido.value}\")\n",
        "\n",
        "    # Ahora pedimos una recomendación de libro para el género elegido\n",
        "    recomendacion_prompt = PromptTemplate(\n",
        "        template=\"Recomienda un libro de género {genero}.\",\n",
        "        input_variables=[\"genero\"]\n",
        "    )\n",
        "    recomendacion_input = recomendacion_prompt.format(genero=genero_elegido.value)\n",
        "    recomendacion = llm.invoke(recomendacion_input)\n",
        "\n",
        "    print(f\"Recomendación: {recomendacion.content}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "e9gtulAtFaSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No esta demasiado clara la conveniencia de ests OutputParser. Porque no hacer simplemente una eleccion aleatoria  ¿?¿?¿?\n",
        "\n",
        "\n",
        "```\n",
        "genero_elegido=random.choice(list(GenerosLiterarios))\n",
        "```\n",
        "En lugar de invocar al modelo ¿?¿?¿?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LDBm8EPeHcNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BooleanOutputParser**\n",
        "\n"
      ],
      "metadata": {
        "id": "lM39D_VGIglJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import BooleanOutputParser\n",
        "\n",
        "\n",
        "# Crear el BooleanOutputParser\n",
        "output_parser = BooleanOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Responde con 'Yes' o 'No' a la siguiente afirmación: 'El sol es una estrella.'\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\":\" \"}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vweQ7_PIkHL",
        "outputId": "4fdf25c2-b491-445a-98f4-ad0addee4ec5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import RegexParser\n",
        "\n",
        "\n",
        "# Definir la expresión regular para extraer nombre y edad\n",
        "regex_pattern = r\"Nombre: (?P<nombre>.+)\\nEdad: (?P<edad>\\d+)\"\n",
        "\n",
        "# Crear el RegexParser\n",
        "output_parser = RegexParser(regex=regex_pattern, output_keys=[\"nombre\", \"edad\"])\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera información de una persona ficticia, incluyendo su nombre y edad.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": \"\"}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "u0zW0wz-v8yf",
        "outputId": "c0a40fb1-b948-46fc-8da9-46b102290ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nombre': 'Laura Martínez  ', 'edad': '28'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OutputFixingParser**"
      ],
      "metadata": {
        "id": "BTfA9lJNv18c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YD59a2H4v6Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb3wSO9Dv6nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://freedium.cfd/https://python.plainenglish.io/langchain-in-chains-7-output-parsers-e1a2cdd40cd3\n",
        "\n",
        "2. https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834\n",
        "\n",
        "3. https://cobusgreyling.medium.com/langchain-structured-output-parser-using-openai-c3fe6927beb7\n",
        "\n",
        "4. https://python.langchain.com/docs/how_to/output_parser_structured/\n",
        "\n",
        "5. https://www.comet.com/site/blog/mastering-output-parsing-in-langchain/\n",
        "\n",
        "6. https://www.gettingstarted.ai/how-to-langchain-output-parsers-convert-text-to-objects/\n",
        "\n",
        "7. https://www.gettingstarted.ai/how-to-extract-metadata-from-pdf-convert-to-json-langchain/  Este es un buen reto\n",
        "\n",
        "8. https://www.analyticsvidhya.com/blog/2024/11/output-parsers/\n"
      ],
      "metadata": {
        "id": "lo85c5QHb66h"
      }
    }
  ]
}