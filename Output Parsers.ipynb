{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN1PaOuySoykOOMknu6lIWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Output%20Parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Output parsers en Langchain**\n",
        "---\n",
        "Imagina que le preguntas a un LLM \"¿Cuáles son los tres planetas más cercanos al sol?\" y te responde: \"Mercurio, Venus y la Tierra son los planetas más cercanos al sol\". Si bien la respuesta es correcta para un humano, para que tu programa pueda usar esa información, lo ideal sería tenerla en un formato más manejable, como una lista o un objeto JSON. Aquí es donde entran en juego los Output Parsers.  \n",
        "\n",
        "Los Output Parsers te permiten “forzar” o “guiar” al modelo para que devuelva la información según un formato deseado (por ejemplo, un JSON con campos específicos, una lista, etc).  \n",
        "\n",
        "LangChain ofrece una variedad de Output Parsers preconstruidos para diferentes necesidades. Algunos de los mas usados son estos:\n",
        "\n",
        "- **StrOutputParser:** Convierte la salida a string\n",
        "\n",
        "-   **StructuredOutputParser:** Analiza la salida en una estructura predefinida con campos.\n",
        "    \n",
        "-   **OutputFixingParser:** Corrige errores en el parsing estructurado usando un LLM.\n",
        "\n",
        "- RetryWithError\n",
        "    \n",
        "-   **CommaSeparatedListOutputParser:** Convierte la salida en una lista de strings separados por comas\n",
        "    \n",
        "-   **EnumOutputParser:** Espera que el LLM elija de un conjunto de opciones.\n",
        "    \n",
        "-   **BooleanOutputParser:** Analiza la salida como Verdadero o Falso.\n",
        "    \n",
        "-   **PydanticOutputParser:** Convierte la salida a un modelo de datos Pydantic.\n",
        "    \n",
        "-   **RegexParser:** Extrae información usando expresiones regulares.\n",
        "\n",
        "- JSON ??\n",
        "- YAML\n",
        "- XML\n",
        "-DATETIME\n",
        "\n",
        "tODOS AQUI: https://python.langchain.com/docs/concepts/output_parsers/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "OutputFixingParser: ?\n",
        "\n",
        "with_structured_output  solo para llm que lo soportan (openai ...)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry8P2gp7VSGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**0. Preparando el entorno del cuaderno**\n",
        "\n"
      ],
      "metadata": {
        "id": "KEWPJUQdCird"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# # Importamos las clases necesarias para trabajar con cadenas\n",
        "# from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. StructuredOutputParser**\n",
        "Un analizador de salida estructurado está diseñado para extraer y estructurar información de texto no estructurado o semiestructurado. Esto es adecuado para situaciones que requieren la extracción de múltiples campos.\n",
        "\n",
        "Permite analizar la salida del LLM en una estructura predefinida. Le proporcionas un esquema (generalmente una lista de ResponseSchema que definen los campos esperados) y el parser intenta extraer la información y mapearla a ese esquema.\n",
        "\n",
        "Aunque el analizador Pydantic (explicado a continuación) ofrece capacidades más sólidas, StructuredOutputParser es ventajoso para su uso con modelos más simples.\n",
        "\n",
        "**Casos de uso:** Extraer información específica de un texto, como atributos de un producto, detalles de un evento, o datos de contacto. Es muy versátil para obtener datos estructurados.\n",
        "\n",
        "**Ventaja:** Flexible y permite definir la estructura esperada de la salida.\n",
        "\n",
        "Veamos un ejemplo en el que el resultado será un diccionario estructurado con los campos nombre, edad y email, listo para ser utilizado en la aplicación."
      ],
      "metadata": {
        "id": "4Zlw_DQiZPI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"nombre\", description=\"El nombre del usuario\"),\n",
        "    ResponseSchema(name=\"edad\", description=\"La edad del usuario\"),\n",
        "    ResponseSchema(name=\"email\", description=\"El correo electrónico del usuario\")\n",
        "]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "F4pnLdNTZP9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obtener instrucciones de formato (opcional pero recomendado):** Muchos parsers tienen un método get\\_format\\_instructions() que devuelve texto que puedes incluir en tu prompt para guiar al LLM sobre el formato esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDsh5c2vaw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "id": "2ov8pnCtaeFT",
        "outputId": "25651e04-cd26-4623-f3cf-24e534454b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"nombre\": string  // El nombre del usuario\\n\\t\"edad\": string  // La edad del usuario\\n\\t\"email\": string  // El correo electrónico del usuario\\n}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera información de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "p1_T9nPdbCIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sobre las `partial variables`\n",
        "Las **partial\\_variables** dentro de un PromptTemplate son una forma de **pre-cargar o fijar ciertos valores dentro de la plantilla del prompt antes de que se proporcionen las variables de entrada principales, es decir cuando se crea el propmpt template y antes de que se formatee**\n",
        "\n",
        "Piénsalo de esta manera: un PromptTemplate es como una plantilla de texto con \"huecos\" que necesitas llenar para crear un prompt completo para el LLM. Hay dos formas principales de llenar estos huecos:\n",
        "\n",
        "-   **input\\_variables:** Estas son las variables que **cambian** cada vez que utilizas el prompt. Son los datos específicos que quieres que el LLM procese en cada llamada.\n",
        "    \n",
        "-   **partial\\_variables:** Estas son las variables que tienen un valor **fijo** o **predefinido** para un uso particular del PromptTemplate. No cambian con cada llamada a la cadena o LLM que usa este prompt.\n",
        "    \n",
        "\n",
        "**En el codigo anterior:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "prompt = PromptTemplate(\n",
        "            template=\"Genera información de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "            input_variables=[],\n",
        "            partial_variables={\"format_instructions\": format_instructions}\n",
        "```\n",
        "En otras palabras, son una forma de incluir en el prompt template una parte fija, pero mediante una variable, lo que proporciona mas flexibilidad programatica.\n",
        "En este caso metemos en el prompt, las instruciones que nos ha devuelto el parser."
      ],
      "metadata": {
        "id": "faC9fR3acUzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "respuesta = llm.invoke(prompt.format()) # la llamada no requiere ningun parámetro\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "respuesta_analizada\n",
        "\n"
      ],
      "metadata": {
        "id": "WrZPCFGieWkY",
        "outputId": "bec6094c-c9b0-49ee-ed53-b6f81158cbf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nombre': 'Laura Gómez', 'edad': '28', 'email': 'laura.gomez@example.com'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a diseñar una consulta para un modelo de lenguaje (LLM) que sirva como base para generar un test de preguntas sobre un tema específico. Proporcionaremos el tema y un nivel de dificultad (bajo, medio, alto), y el LLM deberá generar el texto de la pregunta, tres opciones de respuesta y el índice de la respuesta correcta. La respuesta debe estar estructurada en un diccionario con el siguiente formato:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"pregunta\": \"Texto de la pregunta\",\n",
        "    \"opciones\": [\"Opción 1\", \"Opción 2\", \"Opción 3\"],\n",
        "    \"respuesta_correcta\": índice_de_la_opción_correcta\n",
        "}\n",
        "```\n",
        "\n",
        "Este formato permitirá una fácil interpretación y uso de la pregunta generada.\n"
      ],
      "metadata": {
        "id": "4-C1hmhItipv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "response_schemas = [\n",
        "            ResponseSchema(name=\"pregunta\", description=\"Texto de la pregunta generada.\"),\n",
        "            ResponseSchema(name=\"opciones\", description=\"Lista de tres opciones de respuesta.\"),\n",
        "            ResponseSchema(name=\"respuesta_correcta\", description=\"Índice de la opción correcta (0, 1 o 2).\")\n",
        "                    ]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "        Genera una pregunta de test sobre el tema: {tema}.\n",
        "        El nivel de dificultad debe ser: {nivel}.\n",
        "        La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "respuesta_analizada"
      ],
      "metadata": {
        "id": "MNxAZq7RtiNg",
        "outputId": "56cc5bd8-86f6-4934-95eb-17879613ca74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt generado:\n",
            " \n",
            "        Genera una pregunta de test sobre el tema: LangChain.\n",
            "        El nivel de dificultad debe ser: medio.\n",
            "        La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
            "        \n",
            "        The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"pregunta\": string  // Texto de la pregunta generada.\n",
            "\t\"opciones\": string  // Lista de tres opciones de respuesta.\n",
            "\t\"respuesta_correcta\": string  // Índice de la opción correcta (0, 1 o 2).\n",
            "}\n",
            "```\n",
            "        \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pregunta': '¿Cuál de las siguientes afirmaciones sobre LangChain es correcta?',\n",
              " 'opciones': '0. LangChain solo se puede utilizar con modelos de aprendizaje profundo de código abierto.\\n1. LangChain proporciona una interfaz para interactuar con diversas fuentes de datos y modelos de lenguaje.\\n2. LangChain es exclusivo para la creación de chatbots simples.',\n",
              " 'respuesta_correcta': '1'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. PydanticOutputParser: Convierte la salida a un modelo de datos Pydantic.**\n",
        "\n",
        "Si buscas simplicidad y rapidez, StructuredOutputParser es una buena opción pero si necesitas validación de datos robusta y estás utilizando Pydantic en tu proyecto, PydanticOutputParser es la mejor alternativa.\n",
        "\n",
        "| Característica              | StructuredOutputParser               | PydanticOutputParser               |\n",
        "|-----------------------------|--------------------------------------|-------------------------------------|\n",
        "| **Definición de esquema**    | Lista de `ResponseSchema`            | Clase Pydantic (`BaseModel`)        |\n",
        "| **Validación de tipos**      | No estricta                         | Estricta (usando Pydantic)          |\n",
        "| **Uso**                     | Sencillo y rápido                   | Más verboso pero organizado         |\n",
        "| **Integración con Pydantic**| No                                  | Sí                                  |\n",
        "| **Recomendado para**         | Prototipos rápidos                  | Aplicaciones robustas y complejas   |\n",
        "\n",
        "Veamos como realizar el ejemplo anterior con PydanticOutputParser"
      ],
      "metadata": {
        "id": "4NbToCP9tt-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Definimos el modelo Pydantic para la respuesta\n",
        "class PreguntaTest(BaseModel):\n",
        "    pregunta: str = Field(description=\"Texto de la pregunta generada.\")\n",
        "    opciones: List[str] = Field(description=\"Lista de tres opciones de respuesta.\")\n",
        "    respuesta_correcta: int = Field(description=\"Índice de la opción correcta (0, 1 o 2).\")\n",
        "\n",
        "# Crear el PydanticOutputParser\n",
        "output_parser = PydanticOutputParser(pydantic_object=PreguntaTest)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "Genera una pregunta de test sobre el tema: {tema}.\n",
        "El nivel de dificultad debe ser: {nivel}.\n",
        "La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "print(\"Respuesta analizada:\\n\", respuesta_analizada)\n"
      ],
      "metadata": {
        "id": "dEJ_Wd1Lt0Ee",
        "outputId": "a44b3b93-dbda-4365-c562-a164396c5c20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt generado:\n",
            " \n",
            "Genera una pregunta de test sobre el tema: LangChain.\n",
            "El nivel de dificultad debe ser: medio.\n",
            "La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"pregunta\": {\"description\": \"Texto de la pregunta generada.\", \"title\": \"Pregunta\", \"type\": \"string\"}, \"opciones\": {\"description\": \"Lista de tres opciones de respuesta.\", \"items\": {\"type\": \"string\"}, \"title\": \"Opciones\", \"type\": \"array\"}, \"respuesta_correcta\": {\"description\": \"Índice de la opción correcta (0, 1 o 2).\", \"title\": \"Respuesta Correcta\", \"type\": \"integer\"}}, \"required\": [\"pregunta\", \"opciones\", \"respuesta_correcta\"]}\n",
            "```\n",
            "\n",
            "Respuesta analizada:\n",
            " pregunta='¿Cuál de las siguientes afirmaciones sobre LangChain es correcta?' opciones=['LangChain es una biblioteca exclusiva para procesamiento de imagen.', 'LangChain permite la integración de modelos de lenguaje y agentes para desarrollar aplicaciones más complejas.', 'LangChain no proporciona herramientas para manejar cadenas de texto.'] respuesta_correcta=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otro ejemplo con Pydantic. Deseamos obtener la bibliografia ordenada del autor indicado. deseamos una lista ordenada con el año y el titulo de cada libro.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BnYJ6p9zt4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Definimos el modelo Pydantic para la bibliografía\n",
        "class Libro(BaseModel):\n",
        "    año: int = Field(description=\"Año de publicación del libro.\")\n",
        "    título: str = Field(description=\"Título del libro.\")\n",
        "\n",
        "class BibliografiaAutor(BaseModel):\n",
        "    libros: List[Libro] = Field(description=\"Lista de libros publicados por el autor.\")\n",
        "\n",
        "# Crear el PydanticOutputParser\n",
        "output_parser = PydanticOutputParser(pydantic_object=BibliografiaAutor)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "Proporciona la bibliografía del autor: {autor}.\n",
        "La respuesta debe incluir solo el año y el título de cada libro.\n",
        "No incluyas información adicional.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"autor\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "autor=\"George R. R. Martin\"\n",
        "prompt = prompt_template.format(autor=autor)\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "display(Markdown(f\"**Respuesta del LLM sin parsear:**\\n{respuesta.content}\"))\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Imprimir la bibliografía de forma legible\n",
        "# La función enumerate toma un iterable (como una lista) y devuelve un objeto que genera pares de valores\n",
        "# start=1 , para que el 0 se considere 1\n",
        "print(f\"Bibliografía de {autor}:\\n\")\n",
        "for i, libro in enumerate(respuesta_analizada.libros, start=1):\n",
        "    print(f\"Libro {i}:\")\n",
        "    print(f\"  Año: {libro.año}\")\n",
        "    print(f\"  Título: {libro.título}\\n\")\n",
        "\n",
        "print(type(respuesta_analizada))"
      ],
      "metadata": {
        "id": "Ba2fHvFJt4VB",
        "outputId": "417d7604-b304-463c-b93c-60006aedc0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt generado:\n",
            " \n",
            "Proporciona la bibliografía del autor: George R. R. Martin.\n",
            "La respuesta debe incluir solo el año y el título de cada libro.\n",
            "No incluyas información adicional.\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"$defs\": {\"Libro\": {\"properties\": {\"año\": {\"description\": \"Año de publicación del libro.\", \"title\": \"Año\", \"type\": \"integer\"}, \"título\": {\"description\": \"Título del libro.\", \"title\": \"Título\", \"type\": \"string\"}}, \"required\": [\"año\", \"título\"], \"title\": \"Libro\", \"type\": \"object\"}}, \"properties\": {\"libros\": {\"description\": \"Lista de libros publicados por el autor.\", \"items\": {\"$ref\": \"#/$defs/Libro\"}, \"title\": \"Libros\", \"type\": \"array\"}}, \"required\": [\"libros\"]}\n",
            "```\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Respuesta del LLM sin parsear:**\n```json\n{\"libros\":[{\"año\":1977,\"título\":\"Dying of the Light\"},{\"año\":1981,\"título\":\"Windhaven\"},{\"año\":1983,\"título\":\"Fevre Dream\"},{\"año\":1996,\"título\":\"A Game of Thrones\"},{\"año\":1998,\"título\":\"A Clash of Kings\"},{\"año\":2000,\"título\":\"A Storm of Swords\"},{\"año\":2005,\"título\":\"A Feast for Crows\"},{\"año\":2011,\"título\":\"A Dance with Dragons\"},{\"año\":2020,\"título\":\"Fire & Blood\"},{\"año\":2023,\"título\":\"The Rise of the Dragon\"}]}\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliografía de George R. R. Martin:\n",
            "\n",
            "Libro 1:\n",
            "  Año: 1977\n",
            "  Título: Dying of the Light\n",
            "\n",
            "Libro 2:\n",
            "  Año: 1981\n",
            "  Título: Windhaven\n",
            "\n",
            "Libro 3:\n",
            "  Año: 1983\n",
            "  Título: Fevre Dream\n",
            "\n",
            "Libro 4:\n",
            "  Año: 1996\n",
            "  Título: A Game of Thrones\n",
            "\n",
            "Libro 5:\n",
            "  Año: 1998\n",
            "  Título: A Clash of Kings\n",
            "\n",
            "Libro 6:\n",
            "  Año: 2000\n",
            "  Título: A Storm of Swords\n",
            "\n",
            "Libro 7:\n",
            "  Año: 2005\n",
            "  Título: A Feast for Crows\n",
            "\n",
            "Libro 8:\n",
            "  Año: 2011\n",
            "  Título: A Dance with Dragons\n",
            "\n",
            "Libro 9:\n",
            "  Año: 2020\n",
            "  Título: Fire & Blood\n",
            "\n",
            "Libro 10:\n",
            "  Año: 2023\n",
            "  Título: The Rise of the Dragon\n",
            "\n",
            "<class '__main__.BibliografiaAutor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que la clase del objeto devuelto NO es un diccionario, ni un JSON sino un objeto heredado de la clase que hemos definido con Pydantic !!\n"
      ],
      "metadata": {
        "id": "dnHhQCe_Yb1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. JSONoutputParser**"
      ],
      "metadata": {
        "id": "626Vr1c5uzj9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wgjgzWrSvjSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bofNcvPzvhUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. CommaSeparatedListOutputParser:**\n",
        "\n",
        "Analiza la salida del LLM y la convierte en una lista de strings. Puedes especificar un separador para dividir el texto en elementos de la lista.\n",
        "    \n",
        "-   **Casos de uso:** Obtener listas de elementos, como nombres, ideas, pasos a seguir, o categorías.\n",
        "    \n",
        "-   **Ventaja:** Simple y efectivo para extraer listas.\n",
        "\n",
        "Vamos a hacer un ejemplo en el que queremos obtener una lista de ingredientes para hacer una pizza casera.\n",
        "\n"
      ],
      "metadata": {
        "id": "flcuOq9Nh6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera una lista de ingredientes para hacer una pizza casera. Solo lista los ingredientes, uno por línea.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "AySI2dvhi78N",
        "outputId": "f09142b2-d50f-4100-bd5f-38a73d98cdc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['harina', 'agua', 'levadura', 'sal', 'aceite de oliva', 'salsa de tomate', 'queso mozzarella', 'pepperoni', 'oregano', 'albahaca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. RegexOutputParser**"
      ],
      "metadata": {
        "id": "Ah3HuvErvNl3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NjKPw8TZvmdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSpQlpy-vmDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. EnumOutputParser**\n",
        "\n",
        "Es útil cuando esperas que el LLM elija entre un conjunto predefinido de opciones (un enum de Python).\n",
        "\n",
        "Casos de uso: Clasificación, selección de categorías, o elegir una opción de un menú.\n",
        "\n",
        "Ventaja: Garantiza que la salida se encuentre dentro de un conjunto de valores válidos. Imaginemos que estamos construyendo un sistema de recomendación de libros, donde el modelo debe elegir un género literario de una lista predefinida y luego recomendar un libro de ese género.\n",
        "\n",
        "❗ Este ejemplo no funciona ❓ -> porque este parser es mas bien para \"forzar\" a elegir al modelo una de las opciones. Observa que el \"formato\" que el parser espera (instructions) se incluye en el prompt para \"forzar\" la respuesta del modelo y luego el parser espera poder recoger la salida, pero a veces puede no estar bien"
      ],
      "metadata": {
        "id": "lj97SmlAofVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definir las opciones del enumerado\n",
        "class ColoresOjos(Enum):\n",
        "    MARRÓN = \"marrón\"\n",
        "    AVELANA = \"avellana\"\n",
        "    ÁMBAR = \"ámbar\"\n",
        "    VERDE = \"verde\"\n",
        "    AZUL = \"azul\"\n",
        "    GRIS = \"gris\"\n",
        "\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=ColoresOjos)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"De que color tenia o tiene los ojos esta persona: {persona}.\",\n",
        "    input_variables=[\"persona\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format(persona= \"Frank Sinatra\")\n",
        "output = llm.invoke(input_prompt).content.lower()\n",
        "\n",
        "parsed_output = output_parser.parse(output)\n",
        "\n",
        "# Parsear la salida\n",
        "print(f\"El género elegido es: {parsed_output}\")\n"
      ],
      "metadata": {
        "id": "tCY84L22o-Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo realizamos dos llamadas al modelo. En la primera le pedimos que escoja entre una opcion del Enum, y con ella realizamos la segunda invocación."
      ],
      "metadata": {
        "id": "jCgtmxHREQtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "# Definir los géneros literarios como un Enum\n",
        "class GenerosLiterarios(Enum):\n",
        "    FANTASÍA = \"fantasía\"\n",
        "    CIENCIA_FICCIÓN = \"ciencia ficción\"\n",
        "    MISTERIO = \"misterio\"\n",
        "    ROMANCE = \"romance\"\n",
        "    HISTÓRICA = \"novela histórica\"\n",
        "    NO_FICCIÓN = \"no ficción\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=GenerosLiterarios)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Elige un género literario de la siguiente lista: {generos_literarios}.\\nResponde solo con el genero escogido \\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\n",
        "        \"generos_literarios\": \", \".join([g.value for g in GenerosLiterarios]),\n",
        "        \"format_instructions\": output_parser.get_format_instructions()\n",
        "    }\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=2)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "print(input_prompt)\n",
        "respuesta = llm.invoke(input_prompt).content.lower()\n",
        "print (respuesta)\n",
        "\n"
      ],
      "metadata": {
        "id": "cooXmCR3zwNO",
        "outputId": "d21e0e09-6308-4495-b34d-b6dd3e0d2810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elige un género literario de la siguiente lista: fantasía, ciencia ficción, misterio, romance, novela histórica, no ficción.\n",
            "Responde solo con el genero escogido \n",
            "Select one of the following options: fantasía, ciencia ficción, misterio, romance, novela histórica, no ficción\n",
            "\n",
            "fantasía\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsear la salida\n",
        "try:\n",
        "    genero_elegido = output_parser.parse(respuesta)\n",
        "    print(f\"El género elegido es: {genero_elegido.value}\")\n",
        "\n",
        "    # Ahora pedimos una recomendación de libro para el género elegido\n",
        "    recomendacion_prompt = PromptTemplate(\n",
        "        template=\"Recomienda un libro de género {genero}.\",\n",
        "        input_variables=[\"genero\"]\n",
        "    )\n",
        "    recomendacion_input = recomendacion_prompt.format(genero=genero_elegido.value)\n",
        "    recomendacion = llm.invoke(recomendacion_input)\n",
        "\n",
        "    print(f\"Recomendación: {recomendacion.content}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "e9gtulAtFaSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No esta demasiado clara la conveniencia de ests OutputParser. Porque no hacer simplemente una eleccion aleatoria  ¿?¿?¿?\n",
        "\n",
        "\n",
        "```\n",
        "genero_elegido=random.choice(list(GenerosLiterarios))\n",
        "```\n",
        "En lugar de invocar al modelo ¿?¿?¿?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LDBm8EPeHcNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BooleanOutputParser**\n",
        "\n"
      ],
      "metadata": {
        "id": "lM39D_VGIglJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import BooleanOutputParser\n",
        "\n",
        "\n",
        "# Crear el BooleanOutputParser\n",
        "output_parser = BooleanOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Responde con 'Yes' o 'No' a la siguiente afirmación: 'El sol es una estrella.'\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\":\" \"}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vweQ7_PIkHL",
        "outputId": "4fdf25c2-b491-445a-98f4-ad0addee4ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import RegexParser\n",
        "\n",
        "\n",
        "# Definir la expresión regular para extraer nombre y edad\n",
        "regex_pattern = r\"Nombre: (?P<nombre>.+)\\nEdad: (?P<edad>\\d+)\"\n",
        "\n",
        "# Crear el RegexParser\n",
        "output_parser = RegexParser(regex=regex_pattern, output_keys=[\"nombre\", \"edad\"])\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera información de una persona ficticia, incluyendo su nombre y edad.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": \"\"}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "u0zW0wz-v8yf",
        "outputId": "c0a40fb1-b948-46fc-8da9-46b102290ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nombre': 'Laura Martínez  ', 'edad': '28'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OutputFixingParser**"
      ],
      "metadata": {
        "id": "BTfA9lJNv18c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YD59a2H4v6Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb3wSO9Dv6nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://freedium.cfd/https://python.plainenglish.io/langchain-in-chains-7-output-parsers-e1a2cdd40cd3\n",
        "\n",
        "2. https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834\n",
        "\n",
        "3. https://cobusgreyling.medium.com/langchain-structured-output-parser-using-openai-c3fe6927beb7\n",
        "\n",
        "4. https://python.langchain.com/docs/how_to/output_parser_structured/\n",
        "\n",
        "5. https://www.comet.com/site/blog/mastering-output-parsing-in-langchain/\n",
        "\n",
        "6. https://www.gettingstarted.ai/how-to-langchain-output-parsers-convert-text-to-objects/\n",
        "\n",
        "7. https://www.gettingstarted.ai/how-to-extract-metadata-from-pdf-convert-to-json-langchain/  Este es un buen reto\n",
        "\n",
        "8. https://www.analyticsvidhya.com/blog/2024/11/output-parsers/\n"
      ],
      "metadata": {
        "id": "lo85c5QHb66h"
      }
    }
  ]
}