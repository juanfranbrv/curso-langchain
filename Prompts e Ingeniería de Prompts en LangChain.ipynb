{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGPSWggrgta14Yti8SXIkx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Prompts%20e%20Ingenier%C3%ADa%20de%20Prompts%20en%20LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts e Ingeniería de Prompts dentro de LangChain  \n",
        "---\n",
        "Este cuaderno se enfoca en los **Prompts** y en la **Ingeniería de Prompts** dentro de LangChain. Abordaremos los fundamentos de los prompts, por qué son tan importantes y cómo LangChain nos ayuda a crearlos y reutilizarlos de manera efectiva con distintas clases de Templates: `PromptTemplate`, `FewShotPromptTemplate` y `ChatPromptTemplate`."
      ],
      "metadata": {
        "id": "-jfpHiwS8-O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción a los Prompts\n",
        "\n",
        "En el contexto de los grandes modelos de lenguaje (Large Language Models - LLMs), el prompt es el mensaje, instrucción o pregunta que proporcionamos para guiar la respuesta del modelo, por ejemplo:\n",
        "\n",
        "-  “Explícame en tres frases qué es el *Machine Learning*”.\n",
        "\n",
        "Cuando diseñamos prompts, buscamos que el modelo entienda el contexto y la tarea específica que queremos resolver.\n",
        "\n",
        "Un buen prompt permite orientar al LLM para que genere una respuesta más precisa y alineada con la tarea.  \n",
        "\n",
        "Mediante técnicas como *few-shot prompting*, *Chain-of-Thought (CoT) Prompting* y otras, podemos moldear la personalidad y estilo de la respuesta.  \n",
        "\n",
        "Los prompts son fundamentales en el manejo de modelos de lenguaje ya que guían al modelo para generar respuestas coherentes y útiles. Un prompt bien diseñado puede marcar la diferencia entre una salida precisa y relevante, y una respuesta confusa o fuera de contexto.  \n",
        "\n",
        "LangChain ofrece herramientas específicas para gestionar y optimizar prompts de manera eficiente. Estas herramientas permiten crear plantillas de prompts reutilizables, adaptarlas dinámicamente según el contexto y personalizarlas para diferentes aplicaciones.\n"
      ],
      "metadata": {
        "id": "-XbkJmqr9gCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuración del entorno del cuaderno\n",
        "\n",
        "Instalamos los paquetes necesarios para LangChain, incluyendo soporte para OpenAI y Groq, y recupera las claves API correspondientes desde los secretos de Colab para fines de autenticación.\n",
        "(Este codigo se explico con detalle en el cuaderno anterior)\n",
        "\n"
      ],
      "metadata": {
        "id": "CeFmOSpM_Mnb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEAvzKy0y4wf"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain -qU\n",
        "\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "# %pip install langchain-google-genai -qU\n",
        "# %pip install langchain-huggingface -qU\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. PromptTemplate\n",
        "---\n",
        "Los prompt templates en Langchain permiten crear prompts dinámicos y flexibles mediante la incorporación de variables y opciones de formato, lo que le permite personalizar los prompts en función de los datos de entrada o tareas específicas.\n",
        "\n",
        "- Ayudan en la construccion de prompts complejos.\n",
        "- Son dinamicos: tienen *placeholders* para variables {variable}  \n",
        "- Pueden reutilizarse  \n",
        "\n",
        "Imagina que quieres reutilizar la misma estructura de prompt para múltiples entradas; en lugar de escribir el prompt completo cada vez, puedes definir una “plantilla” con huecos que se llenan de forma programática.\n",
        "\n",
        "Ejemplos:\n",
        "\n",
        "- \"Explicame {tema} con nivel de dificultad {nivel}\"\n",
        "\n",
        "- \"Traduce {frase} del {idioma_entrada} a {idioma_salida}\"\n",
        "\n",
        "Prompts como los anteriores son genericos y reutilizables, pues se usan com plantillas para la construcción de prompts concretos."
      ],
      "metadata": {
        "id": "TiD0D54ZBTGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sintaxis básica\n",
        "\n",
        "`PromptTemplate` es la clase base que nos permite crear plantillas de prompts con variables dinámicas.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "prompt_template = PromptTemplate(\n",
        "            input_variables=[\"variable1\", \"variable2\"],  # Lista de variables dinámicas\n",
        "            template=\"Este es un ejemplo donde {variable1} y {variable2} son variables dinámicas.\"\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "*Observación: De momento no vamos a consultara ningún LLM. Sólo estamos construyendo un prompt que luego podriamos pasar a un modelo.*\n",
        "\n",
        "1. Definimos una cadena de texto (**`plantilla_texto`**) con dos variables dinámicas: `{tema}` y `{pregunta}`.\n",
        "\n",
        "2. Usamos el contructor de clase `PromptTemplate()` para crear un objeto PromptTemplate indicando cuáles son esas variables (input_variables) y cuál es la plantilla (template).\n",
        "\n",
        "3. Luego usamos el metodo `.format()` de la clase, para rellenar los huecos del template."
      ],
      "metadata": {
        "id": "e63XeAs-DlJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "plantilla_texto = \"\"\"\n",
        "Eres un experto en {tema}.\n",
        "Por favor, responde la siguiente pregunta con detalle:\n",
        "Pregunta: {pregunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"tema\", \"pregunta\"],\n",
        "    template=plantilla_texto\n",
        ")\n",
        "\n",
        "prompt=prompt_template.format(tema=\"programacion\", pregunta=\"Que es python?\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "3dhZ-0XZEoky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `PromptTemplate` tiene un metodo que permite instanciar objetos sin necesidad de usar el contructor de clase lo que permite que el codigo sea mas compacto y legible. Este metodo es `.from_template()`\n",
        "\n",
        "\n",
        "```\n",
        "prompt_template = \"Hola, {nombre}. Hoy es {dia}. ¿Cómo estás?\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "`PromptTemplate.from_template()` es un _método de clase_ que permite crear una instancia de `PromptTemplate` de manera directa a partir de una cadena de texto que contiene variables de relleno (por ejemplo, `{tema}` y `{pregunta}`). A diferencia de usar el constructor estándar (donde se debe especificar `input_variables` y la plantilla por separado), esta función:\n",
        "\n",
        "1. **Extrae automáticamente** los nombres de las variables desde las llaves (p. ej., `{tema}`, `{pregunta}`).\n",
        "2. **Crea y retorna** un objeto `PromptTemplate` listo para usar, lo que hace el código más compacto y legible.   \n",
        "\n",
        "Este es el metodo preferido para crear PromptTemplates"
      ],
      "metadata": {
        "id": "7BYVnqgMFu5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "#Instanciar usando .from_template es la forma preferida\n",
        "prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "Eres un experto en {tema}.\n",
        "Por favor, responde la siguiente pregunta con detalle:\n",
        "Pregunta: {pregunta}\n",
        "\"\"\")\n",
        "prompt = prompt_template.format(tema=\"programación\", pregunta=\"Que es python?\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "jsni2Y0G-djc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos ahora a crear un prompt template y enviarselo a un LLM\n"
      ],
      "metadata": {
        "id": "blp6w6QcLVS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "llm2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "plantilla_texto = \"\"\"\n",
        "Eres un experto en traducción del {idioma_origen} al inglés.\n",
        "Por favor, traduce el siguiente texto:\n",
        "---\n",
        "Texto: {texto_a_traducir}\n",
        "---\n",
        "Mantén la intención, el estilo y el tono original.\n",
        "Proporciona solo el texto traducido, sin comentarios adicionales\n",
        "\"\"\"\n",
        "\n",
        "# Creamos el PromptTemplate usando las variables definidas\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"idioma_origen\", \"texto_a_traducir\"],\n",
        "    template=plantilla_texto\n",
        ")\n",
        "\n",
        "# Ejemplo de uso\n",
        "prompt = prompt_template.format(\n",
        "    idioma_origen=\"español\",\n",
        "    texto_a_traducir=\"¡Hola! ¿Cómo te va?\"\n",
        ")\n",
        "\n",
        "\n",
        "#Invocamos los modelos pasandoles el prompt\n",
        "respuesta1 = llm1.invoke(prompt)\n",
        "respuesta2 = llm2.invoke(prompt)\n",
        "\n",
        "\n",
        "# Extraemos de la respuesta el nombre del modelo y el resultado\n",
        "print(respuesta1.response_metadata[\"model_name\"]) # imprimimos el nombre del modelo\n",
        "print(respuesta1.content)\n",
        "print(\"---\")\n",
        "print(respuesta2.response_metadata[\"model_name\"]) # imprimimos el nombre del modelo\n",
        "print(respuesta2.content)\n"
      ],
      "metadata": {
        "id": "FulupEhVHKdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reto 1: Crea tu propio PromptTemplate\n",
        "\n",
        "**Objetivo**  \n",
        "\n",
        "Diseñar un `PromptTemplate` que reciba variables para traducir un texto de un idioma a otro, especificando además el estilo de traducción. Luego, pasar este prompt a un LLM para obtener el resultado y verificar su eficacia.\n",
        "\n",
        "**Instrucciones**\n",
        "\n",
        "1. **Crea un PromptTemplate** con **tres** variables:\n",
        "    \n",
        "    - `{idioma_origen}`: El idioma original en el que está el texto.\n",
        "    - `{idioma_destino}`: El idioma al que se va a traducir.\n",
        "    - `{texto_original}`: El texto que se quiere traducir.\n",
        "2. **Incluye** en tu plantilla alguna instrucción sobre el **estilo** o **tono** de la traducción. Por ejemplo, podrías pedir que sea “formal” o “informal”.\n",
        "    \n",
        "3. **Crea** el prompt final usando `.format(...)` y pásaselo a un LLM de tu elección. Obtén la respuesta y **muéstrala** en pantalla.\n",
        "    \n",
        "4. (Opcional) **Experimenta** con distintos valores de _temperature_ o _top\\_p_ (dependiendo del LLM que uses) para comparar la **consistencia** de la traducción."
      ],
      "metadata": {
        "id": "VDv9qxAWRh4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe aquí tu solución para el reto 1"
      ],
      "metadata": {
        "id": "CQa8iipcWi8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solución Reto 1"
      ],
      "metadata": {
        "id": "SOLsFcGkVjJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realmente no es necesario importar de nuevo las librerías.\n",
        "# Solo lo hacemos con fines ilustrativos, para que el ejemplo sea autosuficiente.\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# 1. Define la plantilla con las variables {idioma_origen}, {idioma_destino}, y {texto_original}\n",
        "mi_plantilla = \"\"\"\n",
        "Eres un traductor experto. Traducirás cuidadosamente del {idioma_origen} al {idioma_destino}.\n",
        "Por favor, mantén un tono formal en la traducción.\n",
        "---\n",
        "Texto original: {texto_original}\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "# 2. Crea el PromptTemplate\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"idioma_origen\", \"idioma_destino\", \"texto_original\"],\n",
        "    template=mi_plantilla\n",
        ")\n",
        "\n",
        "# 3. Usa .format para rellenar los huecos\n",
        "prompt = prompt_template.format(\n",
        "    idioma_origen=\"español\",\n",
        "    idioma_destino=\"francés\",\n",
        "    texto_original=\"¡Hola! ¿Cómo estás hoy?\"\n",
        ")\n",
        "\n",
        "# 4. Invoca tu LLM y muestra el resultado\n",
        "# (Ejemplo con un LLM hipotético `my_llm`)\n",
        "response = llm.invoke(prompt, temperature=0.5)\n",
        "\n",
        "print(\"Prompt final:\\n\", prompt)\n",
        "print(\"\\nTraducción:\\n\", response.content)\n"
      ],
      "metadata": {
        "id": "-2Plf-wTHKaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. FewShotPromptTemplate: proporcionando ejemplos de referencia\n",
        "\n",
        "**Few-shot prompting** es una técnica donde proporcionamos ejemplos dentro del prompt para que el modelo entienda mejor el contexto y la forma de la respuesta. Con `FewShotPromptTemplate` podemos manejar varios ejemplos y luego añadir la pregunta final.\n",
        "\n",
        "### Sintaxis\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "FewShotPromptTemplate(\n",
        "    examples=ejemplos,               # Lista de ejemplos\n",
        "    example_prompt=example_prompt,   # PromptTemplate para cada ejemplo\n",
        "    prefix=prefix_text,              # Texto que aparecerá antes de los ejemplos\n",
        "    suffix=suffix_text,              # Texto que aparecerá después de los ejemplos, habitualmente la pregunta\n",
        "    input_variables=[\"nuevo_input\"], # Variables adicionales que usarás\n",
        "    example_separator=\"\\n---\\n\"      # Separador entre ejemplos\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### ¿Cómo funciona FewShotPromptTemplate?\n",
        "\n",
        "**¿Cómo funciona `FewShotPromptTemplate`?**\n",
        "\n",
        "1. **`examples`:** Una lista de diccionarios. Cada diccionario representa un ejemplo y contiene pares clave-valor con las entradas y salidas deseadas para una tarea específica.\n",
        "    \n",
        "2. **`example_prompt`:** Una instancia de `PromptTemplate` que define cómo se formatea cada ejemplo individualmente. Define las variables de entrada que se utilizarán para cada ejemplo.\n",
        "    \n",
        "3. **`prefix`:** Una cadena que va al principio del prompt completo. Suele ser una instrucción general o una descripción de la tarea.\n",
        "    \n",
        "4. **`suffix`:** Una cadena que va al final del prompt completo. Normalmente aquí es donde se introduce la pregunta al modelo, que debria ser como los ejemplos, pero sin el resultado\n",
        "    \n",
        "5. **`input_variables`:** Una lista de nombres de variables que se utilizarán en el `suffix`.\n",
        "    \n",
        "6. **`example_separator`:** Una cadena que separa cada ejemplo en el prompt. Por defecto es `\\n\\n`.\n",
        "    \n",
        "    \n",
        "\n",
        "**En resumen, `FewShotPromptTemplate` toma los ejemplos, los formatea usando `example_prompt`, los une con `example_separator`, y los coloca entre `prefix` y `suffix` para crear el prompt final. Este prompt contiene el contexto necesario (los ejemplos) para que el modelo de lenguaje pueda inferir la tarea y aplicarla a la nueva entrada proporcionada en el `suffix`.**\n",
        "\n",
        "Hay que usar la sintaxis con el constructor de clase. No hay disponible ningun metodo que simplifique la sintaxis."
      ],
      "metadata": {
        "id": "lEUBfAU2Vrrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "\n",
        "# Lista de ejemplo, donde cada ejemplo es un diccionario\n",
        "ejemplos = [\n",
        "    {\"input\": \"Hello\", \"output\": \"Bonjour\"},\n",
        "    {\"input\": \"Goodbye\", \"output\": \"Au revoir\"},\n",
        "    {\"input\": \"Thank you\", \"output\": \"Merci\"},\n",
        "]\n",
        "\n",
        "#Instancia de PromptTemplate para formatear los ejemplos\n",
        "traduccion_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Creamos el prompt pasando:\n",
        "# la lista de ejemplos\n",
        "# el formateador de los ejemplos\n",
        "# el prefix, que suele ser la descripcion general de la tarea\n",
        "# el suffix, que donde introduciomes la pregunta con la variables\n",
        "# La lista de variables\n",
        "\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,\n",
        "    example_prompt=traduccion_prompt_template,\n",
        "    prefix=\"Translate the following English words to French:\",\n",
        "    suffix=\"Input: {english_word}\\nOutput:\",\n",
        "    input_variables=[\"english_word\"],\n",
        ")\n",
        "\n",
        "print(prompt.format(english_word=\"Car\"))"
      ],
      "metadata": {
        "id": "4S8x65xPHKXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a usarlo para realizar una llamada a los modelos que estamos usando en este cuaderno"
      ],
      "metadata": {
        "id": "jVVSVmF9V6gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Creamos las instancias de cada modelo.\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "llm2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# 1) Definimos algunos ejemplos de traducción: (input → output)\n",
        "ejemplos = [\n",
        "    {\"input\": \"Buenos días\", \"output\": \"Good morning\"},\n",
        "    {\"input\": \"¿Dónde está el baño?\", \"output\": \"Where is the bathroom?\"}\n",
        "]\n",
        "\n",
        "# 2) Creamos un PromptTemplate para cada ejemplo: cómo se mostrará \"input\" y \"output\" en el prompt\n",
        "traduccion_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"\"\"\n",
        "Ejemplo de Entrada: {input}\n",
        "Ejemplo de Salida: {output}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 3) Definimos un prefijo y un sufijo para contextualizar la tarea antes y después de los ejemplos\n",
        "prefix_text = \"\"\"Eres un traductor experto del español al inglés.\n",
        "Estos son algunos ejemplos de cómo traducir oraciones al inglés:\"\"\"\n",
        "\n",
        "suffix_text = \"\"\"\n",
        "Ahora traduce la siguiente frase:\n",
        "Entrada: {nueva_frase}\n",
        "Salida:\n",
        "\"\"\"\n",
        "\n",
        "# 4) Creamos el FewShotPromptTemplate\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,                  # Nuestros ejemplos\n",
        "    example_prompt=traduccion_prompt_template,      # Formato de cada ejemplo\n",
        "    prefix=prefix_text,                 # Texto antes de los ejemplos\n",
        "    suffix=suffix_text,                 # Texto después de los ejemplos\n",
        "    input_variables=[\"nueva_frase\"],    # Variable requerida al formatear\n",
        "    example_separator=\"\\n---\\n\"         # Separador entre ejemplos\n",
        ")\n",
        "\n",
        "# 5) Generamos el prompt final para la nueva frase\n",
        "prompt = few_shot_prompt.format(nueva_frase=\"¡Hola! ¿Cómo te va?\")\n",
        "\n",
        "print(\"=== Prompt Generado ===\")\n",
        "print(prompt)\n",
        "\n",
        "# 6) Pasamos el prompt al primer modelo (ChatOpenAI - gpt-4o-mini)\n",
        "print(\"\\n=== Respuesta con ChatOpenAI (gpt-4o-mini) ===\")\n",
        "response_llm1 = llm1.invoke(prompt)\n",
        "print(response_llm1.content)\n",
        "\n",
        "# 7) Pasamos el mismo prompt al segundo modelo (ChatGroq - llama-3.3-70b-versatile)\n",
        "print(\"\\n=== Respuesta con ChatGroq (llama-3.3-70b-versatile) ===\")\n",
        "response_llm2 = llm2.invoke(prompt)\n",
        "print(response_llm2.content)\n"
      ],
      "metadata": {
        "id": "-qM0EWM2HKUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reto 2: Few-Shot Prompting para Traducción Creativa\n",
        "\n",
        "El reto consiste en usar la clase FewShotPromptTemplate para traducir oraciones del español al inglés con un estilo específico, incorporando ejemplos previos que sirvan de guía para el LLM.  \n",
        "\n",
        "### **Instrucciones**\n",
        "\n",
        "1. **Crea una lista de ejemplos** de traducción que contengan:\n",
        "    \n",
        "    - La frase de entrada en español (clave: `\"input\"`).\n",
        "    - Su traducción al inglés, **pero con cierto estilo o “sabor”** (clave: `\"output\"`).\n",
        "        - Por ejemplo, podrías elegir que la traducción sea **muy “entusiasta”** o **tenga un tono humorístico**.\n",
        "2. **Diseña un `PromptTemplate`** para mostrar cada ejemplo con un formato claro. Algo así como:\n",
        "    \n",
        "    python\n",
        "    \n",
        "    Copiar código\n",
        "    \n",
        "    `\"Oración original: {input}\\nTraducción estilizada: {output}\\n\"`\n",
        "    \n",
        "    (puedes ajustar el texto a tu preferencia).\n",
        "    \n",
        "3. **Define** el `prefix` y el `suffix` de tu `FewShotPromptTemplate`:\n",
        "    \n",
        "    - El `prefix` debe _explicar_ que el LLM es un traductor con cierto estilo (por ejemplo, “traductor humorístico” o “traductor formal”).\n",
        "    - El `suffix` debe contener la **nueva frase** a traducir (`{frase_nueva}`) y especificar que se aplique el mismo estilo.\n",
        "4. **Crea** la instancia de `FewShotPromptTemplate` con:\n",
        "    \n",
        "    - `examples`: tu lista de ejemplos.\n",
        "    - `example_prompt`: tu plantilla de ejemplo.\n",
        "    - `prefix`, `suffix`, `example_separator`, etc.\n",
        "5. **Usa `.format()`** para generar el prompt final, pasando la nueva frase a traducir en la variable `{frase_nueva}`.\n",
        "    \n",
        "6. **Envía** ese prompt a un LLM (puede ser `ChatOpenAI`, `ChatGroq` o cualquier otro). Muestra la respuesta que obtienes.\n",
        "    \n",
        "7. (Opcional) **Experimenta** con distintos estilos (más formal, más bromista, etc.) para ver cómo cambian las traducciones.\n",
        "\n",
        "8. (Opcional). Puedes incluir el estilo de traducción como una varibale dinámica del prompt ?"
      ],
      "metadata": {
        "id": "Jhd_bZrReiqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución Reto 2"
      ],
      "metadata": {
        "id": "2cR2kNLmYdwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "# 1. Ejemplos\n",
        "ejemplos = [\n",
        "    {\n",
        "        \"input\": \"Buenos días, ¿cómo amaneciste?\",\n",
        "        \"output\": \"Good morning, how did you wake up, buddy? (super friendly tone)\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"¿Quieres salir a correr más tarde?\",\n",
        "        \"output\": \"Do you want to go for a run later, pal? (energetic style)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# 2. Plantilla de ejemplo\n",
        "ejemplo_plantilla = \"\"\"\n",
        "Oración original: {input}\n",
        "Traducción con estilo: {output}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=ejemplo_plantilla\n",
        ")\n",
        "\n",
        "# 3. Prefijo y Sufijo\n",
        "prefix_text = \"\"\"Eres un traductor que siempre usa un tono muy amigable y entusiasta.\n",
        "Aquí tienes algunos ejemplos de cómo traduces del español al inglés:\n",
        "\"\"\"\n",
        "suffix_text = \"\"\"\n",
        "Ahora quiero que traduzcas la siguiente frase con el mismo tono amistoso:\n",
        "Oración: {frase_nueva}\n",
        "Traducción:\n",
        "\"\"\"\n",
        "\n",
        "# 4. FewShotPromptTemplate\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix_text,\n",
        "    suffix=suffix_text,\n",
        "    input_variables=[\"frase_nueva\"],\n",
        "    example_separator=\"\\n---\\n\"\n",
        ")\n",
        "\n",
        "# 5. Formateo del prompt\n",
        "prompt_final = few_shot_prompt.format(frase_nueva=\"¡Hola! ¿Listo para la aventura de hoy?\")\n",
        "\n",
        "# 6. (Pseudo-código) Llamada al LLM\n",
        "# response = llm.invoke(prompt_final)\n",
        "# print(response.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "xWHO02kHYnvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. ChatPromptTemplate: el modelo conversacional.\n",
        "---\n",
        "\n",
        "Los modelos de lenguaje más recientes (como `gpt-3.5-turbo` y `gpt-4`) trabajan muy bien en un _formato conversacional_. Esto significa que, en lugar de pasar un único “prompt de texto”, podemos estructurar el input como una serie de **mensajes** con distintos roles:\n",
        "\n",
        "- **system** (mensaje de sistema)\n",
        "- **user** (mensaje del usuario)\n",
        "- **assistant** (mensaje del asistente)\n",
        "\n",
        "`ChatPromptTemplate` facilita la construcción de estos mensajes, en lugar de tener que concatenarlos manualmente en un string.  \n",
        "\n",
        "### **Componentes necesarios**\n",
        "\n",
        "Para usar `ChatPromptTemplate`, necesitaremos:\n",
        "\n",
        "1. **SystemMessagePromptTemplate**: Define las instrucciones de “sistema”, es decir, el contexto o la personalidad del asistente.\n",
        "2. **HumanMessagePromptTemplate**: Representa el mensaje que proviene de un usuario.\n",
        "3. (**Opcional**) **AIMessagePromptTemplate**: Sirve para simular respuestas del asistente (en caso de que queramos “inyectar” un mensaje del asistente en el contexto, aunque no siempre es necesario).\n",
        "\n",
        "Finalmente, un **ChatPromptTemplate** agrupa todos estos “mensajes” para formar un “chat” que el modelo interpretará.\n",
        "\n",
        "**Sintaxis**\n",
        "\n",
        "Usando el contructor de clase:  \n",
        "(aquí pasas la lista de mensajes directamente al parámetro messages del constructor.)\n",
        "```\n",
        "ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\"Mensaje del sistema\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"Mensaje del humano\"),\n",
        "        AIMessagePromptTemplate.from_template(\"Mensaje de la IA\"),\n",
        "    ]\n",
        "```\n",
        "Usando el metodo `.from_messages`\n",
        "\n",
        "```\n",
        "ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"Mensaje del sistema\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"Mensaje del humano\"),\n",
        "    AIMessagePromptTemplate.from_template(\"Mensaje de la IA\"),  ])`\n",
        "```\n",
        "\n",
        "y tambien de esta forma mas legible, usando tuplas. Y nos evitamos crear instancias de SystemMessagePromptTemplate o HumanMessagePromptTemplate\n",
        "\n",
        "```\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template_sistema),\n",
        "    (\"human\", template_usuario),\n",
        "])\n",
        "```\n",
        "Además, ten en cuenta que podemos invocar un modelo de chat simplemente con una cadena. Cuando se pasa una cadena como entrada, se convierte en HumanMessage y luego se pasa al modelo subyacente.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "llm.invoke(\"hello world\")\n",
        "\n",
        "# acaba convirtiendose en\n",
        "llm.invoke(HumanMessage(content=\"hello world\"))\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "S1ewq5Rlae9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a usar un ChatPromptTemplate para eviarselo a nuestros modelos de lenguaje."
      ],
      "metadata": {
        "id": "HwcgiaErkwpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Claves de API (asegúrate de tenerlas en Secretos)\n",
        "# Configurar los modelos\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "llm2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# Definir plantillas de mensajes\n",
        "template_sistema = \"\"\"\\\n",
        "Eres un traductor profesional especializado en traducción técnica y literaria.\n",
        "Proporciona traducciones de alta calidad y analiza tus decisiones. También sugiere alternativas para frases complejas.\n",
        "\"\"\"\n",
        "\n",
        "template_usuario = \"\"\"\\\n",
        "Por favor, traduce este texto del español al inglés:\n",
        "{texto}\n",
        "\"\"\"\n",
        "\n",
        "# Texto original para traducir\n",
        "texto_original = \"\"\"\\\n",
        "La traducción técnica requiere precisión extrema, pero también la sensibilidad para adaptarse a contextos culturales variados.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el ChatPromptTemplate\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(template_sistema),\n",
        "    HumanMessagePromptTemplate.from_template(template_usuario),\n",
        "])\n",
        "\n",
        "# o de esta forma, mas legible...\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template_sistema),\n",
        "    (\"human\", template_usuario),\n",
        "])\n",
        "\n",
        "prompt = chat_prompt_template.format(texto=texto_original)\n",
        "\n",
        "# Mostramos el prompt de mensajes construido\n",
        "print(\"El prompt de mensajes enviado es:\")\n",
        "print(prompt)\n",
        "\n",
        "# Invocamos los modelos, pasandoles los mensajes\n",
        "respuesta_openai = llm1.invoke(prompt)\n",
        "respuesta_groq = llm2.invoke(prompt)\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(\"\\nTraducción y análisis con ChatOpenAI:\")\n",
        "print(respuesta_openai.content)\n",
        "\n",
        "print(\"\\nTraducción y análisis con ChatGroq:\")\n",
        "print(respuesta_groq.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "wcMivP2eHKRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reto 3: Análisis de Traducciones con Modelos de Lenguaje\n",
        "\n",
        "En este reto, tu tarea es desarrollar un programa que utilice un modelo de lenguaje (LLM) para analizar una traducción proporcionada, identificar errores comunes y sugerir correcciones. Este ejercicio se centra en el uso de herramientas como `ChatPromptTemplate` para interactuar con el modelo, configurando un contexto claro y proporcionando instrucciones específicas.\n",
        "\n",
        "#### Objetivo:\n",
        "\n",
        "El programa deberá:\n",
        "\n",
        "1. **Configurar el contexto del modelo**:\n",
        "    \n",
        "    - El modelo debe actuar como un experto en revisión de traducciones, especializado en identificar errores lingüísticos comunes.\n",
        "2. **Procesar los datos de entrada**:\n",
        "    \n",
        "    - Incluir dos textos: uno original en español y su correspondiente traducción al inglés.\n",
        "    - Los textos pueden contener errores deliberados, como falsos cognados, estructuras gramaticales incorrectas o problemas de terminología.\n",
        "3. **Realizar el análisis**:\n",
        "    \n",
        "    - Utilizar el LLM para:\n",
        "        - Detectar errores en la traducción.\n",
        "        - Sugerir correcciones claras para cada error identificado.\n",
        "        - Explicar brevemente cada decisión de corrección.\n",
        "4. **Mostrar los resultados**:\n",
        "    \n",
        "    - Presentar un informe que incluya:\n",
        "        - Los errores identificados.\n",
        "        - Las correcciones sugeridas.\n",
        "        - Una breve justificación de las decisiones tomadas.\n",
        "\n",
        "#### Requisitos Técnicos:\n",
        "\n",
        "- Utilizar `ChatPromptTemplate` para configurar los mensajes del sistema y del usuario.\n",
        "- Configurar un modelo como `ChatOpenAI` o equivalente para realizar la tarea.\n",
        "- Garantizar que el contexto y las instrucciones proporcionadas al modelo sean claras y específicas.\n",
        "\n",
        "#### Entrada del Programa:\n",
        "\n",
        "- **Texto original** (en español): Un párrafo breve que actúe como base para la traducción.\n",
        "- **Texto traducido** (en inglés): La traducción del texto original, que puede contener errores intencionados.\n",
        "\n",
        "#### Salida Esperada:\n",
        "\n",
        "- Un análisis detallado en el que se muestren:\n",
        "    - Los errores identificados (e.g., falsos cognados, problemas de fluidez o precisión).\n",
        "    - Las correcciones sugeridas para cada error.\n",
        "    - Explicaciones justificando las correcciones realizadas.\n",
        "\n",
        "#### Ejemplo:\n",
        "\n",
        "Entrada:\n",
        "\n",
        "- Texto original (Español):  \n",
        "    \"El director de la empresa estaba consternado por la situación, pero expresó su determinación de buscar soluciones.\"\n",
        "    \n",
        "- Traducción (Inglés):  \n",
        "    \"The director of the company was constipated about the situation, but expressed his determination to look for solutions.\"\n",
        "    \n",
        "\n",
        "Salida esperada:\n",
        "\n",
        "1. **Error**: \"constipated\" no es una traducción adecuada de \"consternado\".\n",
        "    \n",
        "    - **Corrección sugerida**: \"dismayed\".\n",
        "    - **Explicación**: \"Constipated\" es un falso cognado y se refiere a un problema físico, no emocional.\n",
        "2. **Error**: Problema menor en la frase \"look for solutions\" que podría ser más formal.\n",
        "    \n",
        "    - **Corrección sugerida**: \"seek solutions\".\n",
        "    - **Explicación**: \"Seek\" es un término más formal y adecuado para un contexto empresarial.\n"
      ],
      "metadata": {
        "id": "Drkkgmd4uXm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solución Reto 3: Análisis de Traducciones con Modelos de Lenguaje"
      ],
      "metadata": {
        "id": "jeQ9F-_fu9TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Claves de API (asegúrate de tenerlas en Secretos)\n",
        "# Configurar los modelos\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "llm2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# Definir el reto\n",
        "print(\"Reto de programación:\")\n",
        "print(\"Desarrolla un programa que utilice un LLM para analizar una traducción proporcionada, \"\n",
        "      \"identificar posibles errores y sugerir correcciones.\")\n",
        "\n",
        "# Definir las plantillas\n",
        "template_sistema = \"\"\"\\\n",
        "Eres un experto en traducción especializado en revisar textos traducidos.\n",
        "Identificas errores comunes como falsos cognados, estructuras gramaticales incorrectas y falta de precisión terminológica.\n",
        "Proporciona sugerencias claras para corregirlos.\n",
        "\"\"\"\n",
        "\n",
        "template_usuario = \"\"\"\\\n",
        "Aquí tienes una traducción para analizar:\n",
        "\n",
        "Texto original (Español):\n",
        "{texto_original}\n",
        "\n",
        "Traducción (Inglés):\n",
        "{texto_traducido}\n",
        "\n",
        "Por favor, analiza los errores comunes y sugiere correcciones.\n",
        "\"\"\"\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto_original = \"\"\"\\\n",
        "El director de la empresa estaba consternado por la situación, pero expresó su determinación de buscar soluciones.\n",
        "\"\"\"\n",
        "texto_traducido = \"\"\"\\\n",
        "The director of the company was constipated about the situation, but expressed his determination to look for solutions.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el ChatPromptTemplate\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(template_sistema),\n",
        "    HumanMessagePromptTemplate.from_template(template_usuario),\n",
        "])\n",
        "\n",
        "# Formatear el prompt con los textos\n",
        "prompt = chat_prompt_template.format(\n",
        "    texto_original=texto_original,\n",
        "    texto_traducido=texto_traducido\n",
        ")\n",
        "\n",
        "# Mostramos el prompt de mensajes construido\n",
        "print(\"El prompt de mensajes enviado es:\")\n",
        "print(prompt)\n",
        "\n",
        "# Invocamos los modelos, pasandoles los mensajes\n",
        "respuesta_openai = llm1.invoke(prompt)\n",
        "respuesta_groq = llm2.invoke(prompt)\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(\"\\nTraducción y análisis con ChatOpenAI:\")\n",
        "print(respuesta_openai.content)\n",
        "\n",
        "print(\"\\nTraducción y análisis con ChatGroq:\")\n",
        "print(respuesta_groq.content)\n"
      ],
      "metadata": {
        "id": "QuTmaTx2vKFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. PromptTemplate vs ChatPromptTemplate ¿ Cuando usar uno u otro ?\n",
        "---\n",
        "\n",
        "La diferencia principal entre `PromptTemplate` y `ChatPromptTemplate` en LangChain radica en el tipo de modelos con los que están diseñados para trabajar y en cómo estructuran los mensajes.\n",
        "\n",
        "### **PromptTemplate**\n",
        "\n",
        "#### Uso y Propósito:\n",
        "\n",
        "- **Diseñado para modelos tradicionales** como los basados en texto (e.g., GPT-3, `text-davinci`).\n",
        "- Se utiliza para construir un único bloque de texto a partir de una plantilla.\n",
        "- Ideal para modelos que aceptan una sola entrada textual como prompt.\n",
        "\n",
        "#### Estructura:\n",
        "\n",
        "Un `PromptTemplate` toma variables y las inserta en un solo texto formateado.\n",
        "\n",
        "### **ChatPromptTemplate**\n",
        "\n",
        "#### Uso y Propósito:\n",
        "\n",
        "- **Diseñado para modelos conversacionales** como los de la familia `chat` (e.g., GPT-4 Turbo, ChatGPT).\n",
        "- Permite estructurar mensajes en varios niveles, especificando roles como `system`, `user`, y `assistant`.\n",
        "- Ideal para contextos donde necesitas construir conversaciones completas con múltiples mensajes.\n",
        "\n",
        "#### Estructura:\n",
        "\n",
        "Un `ChatPromptTemplate` organiza varios mensajes en una lista, cada uno con un rol específico.\n",
        "\n",
        "### **Recomendación**\n",
        "\n",
        "- **Si estás trabajando con modelos de chat**, utiliza `ChatPromptTemplate` para aprovechar su capacidad de estructurar roles.\n",
        "- **Si necesitas algo rápido y sencillo**, o trabajas con modelos de texto, `PromptTemplate` será suficiente.\n",
        "\n",
        "En el caso del reto, usar `ChatPromptTemplate` sería más adecuado porque permite organizar el análisis en roles (sistema para contexto, usuario para instrucciones).  \n",
        "\n",
        "La calidad de los resultados podría verse afectada al usar un `PromptTemplate` en lugar de un `ChatPromptTemplate` si el modelo está optimizado para manejar roles (como los modelos de chat). Esto se debe a que los roles estructurados ayudan a organizar el contexto de forma más clara, lo que puede influir en cómo el modelo interpreta y responde a las instrucciones.\n",
        "\n",
        "Al estar estos modelos diseñados para trabajar con roles explícitos, como `system`, `user`, y `assistant`.\n",
        "    - Usar roles estructurados permite:\n",
        "        - Separar instrucciones contextuales (e.g., \"Eres un traductor experto\").\n",
        "        - Clarificar las interacciones del usuario (e.g., \"Traduce este texto\").\n",
        "        - Mejorar la precisión y relevancia de las respuestas.\n",
        "\n",
        "**Si usas `PromptTemplate` aquí**, podrías perder claridad en el contexto, lo que puede dar lugar a respuestas menos precisas o más genéricas.\n",
        "\n",
        "Por ejemplo el **Reto 3: Análisis de Traducciones con Modelos de Lenguaje** prodria haberse resuelto con cualquiera de ambas clases.\n",
        "\n",
        "Solo habria variado la forma de estructurar el prompt:\n",
        "\n",
        "**Con `PromptTemplate`**:\n",
        "\n",
        "- Todo el contexto y las instrucciones se construyen en un solo bloque de texto.\n",
        "- El prompt podría quedar así\n",
        "```\n",
        "template = '''\\\n",
        "Eres un traductor profesional especializado en traducción técnica y literaria.\n",
        "Identifica errores en la siguiente traducción y sugiere correcciones:\n",
        "Texto original: {texto_original}\n",
        "Traducción: {texto_traducido}\n",
        "Por favor, analiza errores y explica las decisiones tomadas.'''\n",
        "\n",
        "**Con `ChatPromptTemplate:`**\n",
        "\n",
        "- Se separan las instrucciones del sistema y las del usuario en distintos roles.\n",
        "- El codigo se veria asi:  \n",
        "\n",
        "```\n",
        "SystemMessagePromptTemplate.from_template(\n",
        "    \"Eres un traductor profesional especializado en identificar errores en traducciones.\"\n",
        ")\n",
        "HumanMessagePromptTemplate.from_template(\n",
        "    \"Texto original: {texto_original}\\nTraducción: {texto_traducido}\\nPor favor, analiza errores y explica tus decisiones.\"\n",
        ")  \n",
        "```\n",
        "\n",
        "En el Playground de OpenAI puede ver los modelos antiguos (que ya estan marcados como Legacy, GPT 3.5, Davinci-002, ... ) en el apartado Completions. https://platform.openai.com/playground/complete frente a los modelos nuevos (GPT-4o, ... ) en el apartado chat https://platform.openai.com/playground/chat"
      ],
      "metadata": {
        "id": "QNpu9G0KxZfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Agregando más interacción: AIMessagePromptTemplate**\n",
        "A veces queremos incluir un mensaje del assistant (IA) en el contexto. Esto puede ser útil para simular una conversación previa o para inyectar algún contenido que la IA haya dicho antes.  \n",
        "\n",
        "De esta forma, el modelo interpretará que ya hubo un mensaje previo de la IA y continuará la conversación.  \n",
        "\n",
        "<br><br>\n",
        "\n",
        "Este ejemplo muestra un caso práctico del uso de AIMessagePromptTemplate donde:\n",
        "\n",
        "1. Incluimos feedback de IA como parte del contexto del prompt, simulando un diálogo más natural  \n",
        "\n",
        "2. El feedback de la IA sirve como guía para mejorar la traducción original  \n",
        "\n",
        "3. Se mantiene una conversación estructurada: sistema → usuario → IA → usuario\n",
        "\n",
        "La diferencia clave con el ejemplo original es que aquí usamos AIMessagePromptTemplate para incorporar retroalimentación específica de la IA como parte del contexto, lo que ayuda a obtener una traducción más refinada y consciente de aspectos específicos a mejorar."
      ],
      "metadata": {
        "id": "IOjRh5Uw6UVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cck8dQcD-Ldu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Configurar el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Definir plantillas de mensajes\n",
        "template_sistema = \"\"\"\\\n",
        "Eres un tutor de idiomas especializado en mejorar traducciones basándote en feedback previo.\n",
        "Analiza la traducción original y el feedback proporcionado para sugerir una versión mejorada.\n",
        "\"\"\"\n",
        "\n",
        "template_usuario_inicial = \"\"\"\\\n",
        "Traduce esta frase al inglés:\n",
        "{texto}\n",
        "\"\"\"\n",
        "\n",
        "# Este es un ejemplo de feedback de IA que se utilizará como parte del contexto\n",
        "template_ai_feedback = \"\"\"\\\n",
        "La traducción es gramaticalmente correcta, pero podría mejorar en:\n",
        "1. Uso de terminología más específica del campo\n",
        "2. Mayor naturalidad en la expresión\n",
        "3. Consideración del contexto cultural\n",
        "\"\"\"\n",
        "\n",
        "template_usuario_final = \"\"\"\\\n",
        "Por favor, proporciona una versión mejorada considerando este feedback.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el prompt con múltiples mensajes incluyendo el feedback de la IA\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(template_sistema),\n",
        "    HumanMessagePromptTemplate.from_template(template_usuario_inicial),\n",
        "    AIMessagePromptTemplate.from_template(template_ai_feedback),\n",
        "    HumanMessagePromptTemplate.from_template(template_usuario_final)\n",
        "])\n",
        "\n",
        "# Texto para traducir\n",
        "texto_original = \"\"\"\\\n",
        "El software debe ser compatible con los principales sistemas operativos y\n",
        "ofrecer una interfaz intuitiva para usuarios principiantes.\n",
        "\"\"\"\n",
        "\n",
        "# Generar el prompt completo\n",
        "prompt = chat_prompt_template.format(texto=texto_original)\n",
        "\n",
        "# Mostrar el prompt construido\n",
        "print(\"El prompt de mensajes enviado es:\")\n",
        "print(prompt)\n",
        "\n",
        "# Invocar el modelo\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(\"\\nTraducción mejorada y análisis:\")\n",
        "print(respuesta.content)\n"
      ],
      "metadata": {
        "id": "xiXOXmdj7YVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En teoría sí se puede lograr algo similar incorporando el feedback directamente en el mensaje del sistema o del usuario, pero hay algunas consideraciones importantes:\n",
        "\n",
        "Las principales diferencias y consideraciones son:\n",
        "\n",
        "1. **Estructura del diálogo**:\n",
        "    - Con AIMessagePromptTemplate teníamos una estructura más natural de conversación que simula un proceso de revisión real.\n",
        "    - Sin él, tenemos que incluir todas las instrucciones de una vez, lo que puede hacer el prompt menos natural.\n",
        "2. **Flexibilidad**:\n",
        "    - La versión con AIMessagePromptTemplate permite mayor flexibilidad para modificar el feedback específico para cada caso.\n",
        "    - La versión sin él tiene los criterios fijos en el mensaje del sistema.\n",
        "3. **Contexto**:\n",
        "    - Con AIMessagePromptTemplate podemos mantener un contexto más claro de \"primera traducción → feedback → mejora\"\n",
        "    - Sin él, tenemos que pedir todo en un solo paso.\n",
        "4. **Resultados**:\n",
        "    - Los resultados pueden ser similares en calidad, pero la versión con AIMessagePromptTemplate suele producir respuestas más estructuradas y cercanas a un diálogo real de revisión.\n",
        "    - La versión sin él depende más de cómo estructuremos las instrucciones en el prompt.\n",
        "\n",
        "El AIMessagePromptTemplate es especialmente útil cuando:\n",
        "\n",
        "- Necesitas simular un diálogo real con múltiples pasos\n",
        "- Quieres mantener un contexto claro de retroalimentación"
      ],
      "metadata": {
        "id": "v5IIqb2L_bGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FALTA EL TEMA DE LOS CHATPROMPT EN FORMATO OPENAI COMO DICCIONARIOS\n",
        "{ROLE:XXX, \"CONTENT\": XXX}"
      ],
      "metadata": {
        "id": "SYhvoe10cpF_"
      }
    }
  ]
}