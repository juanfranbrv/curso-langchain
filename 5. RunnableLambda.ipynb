{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHL+Qndof/625UU246kmAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/5.%20RunnableLambda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. `RunnableLambda`**\n",
        "---\n",
        "`RunnableLambda` es un componente clave en LangChain Expression Language (LCEL) que permite\n",
        "integrar funciones de Python estándar directamente en tus cadenas y flujos de trabajo.\n",
        "En esencia, `RunnableLambda` toma una función de Python y la convierte en un objeto\n",
        "\"runnable\" que puede ser parte de una cadena de LangChain, recibiendo la entrada del\n",
        "paso anterior y pasando su salida al siguiente.\n",
        "\n",
        "**¿Por qué usar RunnableLambda?**\n",
        "\n",
        "* **Flexibilidad:** Integra lógica de Python arbitraria dentro de tus cadenas de LangChain.  \n",
        "\n",
        "* **Simplicidad:** Envuelve funciones existentes sin necesidad de crear clases complejas.\n",
        "* **Modularidad:** Permite descomponer tareas complejas en funciones más pequeñas y reutilizables.\n",
        "* **Integración:** Facilita la conexión de LangChain con herramientas y librerías de Python.\n",
        "\n",
        "Vamos a ver a continuación varios ejemplos de uso"
      ],
      "metadata": {
        "id": "qiB5bQcHn93p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together y Anthropic\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n",
        "El uso de las API de OpenAI y Anthropic es de pago. El resto son gratuitas y para usarlas basta con registrarse y generar una API Key.  \n",
        "\n",
        "En el primer cuaderno encontraras los enlaces a estos servicios y este codigo explicado"
      ],
      "metadata": {
        "id": "sTF0Mzj_pjuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "0xW-bmL_pW00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Envolviendo funciones simples**\n",
        "---\n",
        "\n",
        "La forma principal de añadir una función a una cadena en Langchain (especialmente en LCEL) es utilizando RunnableLambda. RunnableLambda es un Runnable que envuelve una función Python, permitiéndole integrarse perfectamente en el flujo de la cadena.\n",
        "\n",
        "(Aunque es posible, la inserción directa de una función en una cadena funciona, esto se debe a que Langchain implícitamente envuelve esa función en un RunnableLambda \"por debajo del capó\" para que pueda encajar dentro del paradigma de la cadena LCEL. Es mejor ser consistente con los principios del framework y usar RunnableLambda)"
      ],
      "metadata": {
        "id": "-JBgg4CxpIkR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Función simple de Python\n",
        "def duplicar(x):\n",
        "    # Duplica el valor de entrada\n",
        "    return x * 2\n",
        "\n",
        "# Convertir a Runnable\n",
        "runnable_duplicar = RunnableLambda(duplicar)\n",
        "\n",
        "# Invocar el Runnable\n",
        "resultado = runnable_duplicar.invoke(5)\n",
        "print(f\"Resultado de duplicar: {resultado}\")  # Salida: 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Usando RunnableLambda en una cadena simple**\n",
        "---\n",
        "\n",
        "Múltiples `RunnableLambda` en secuencia. Este ejemplo muestra cómo encadenar múltiples `RunnableLambda`. La salida del primero (`runnable_mayusculas`) se convierte en la entrada del segundo\n",
        "(`runnable_exclamacion`)."
      ],
      "metadata": {
        "id": "jN_e2Pa1rQe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def mayusculas(texto):\n",
        "    return texto.upper()\n",
        "\n",
        "def agregar_exclamacion(texto):\n",
        "    return texto + \"!\"\n",
        "\n",
        "runnable_mayusculas = RunnableLambda(mayusculas)\n",
        "runnable_exclamacion = RunnableLambda(agregar_exclamacion)\n",
        "\n",
        "cadena_transformacion = runnable_mayusculas | runnable_exclamacion\n",
        "\n",
        "resultado_transformacion = cadena_transformacion.invoke(\"Este es un texto cualquiera\")\n",
        "print(resultado_transformacion)"
      ],
      "metadata": {
        "id": "FgTwCRr9qod3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Preprocesamiento del prompt**\n",
        "\n",
        "Usamos un `RunnableLambda` con una función que se encargara de \"limpiar\" el texto que vamos a enviar para contruir el prompt template.\n",
        "\n"
      ],
      "metadata": {
        "id": "6kQlr_D6F70P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"Elimina espacios extra y convierte a minúsculas\"\"\"\n",
        "    return texto.strip().lower()\n",
        "\n",
        "# Crear un flujo simple de procesamiento\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Analiza el siguiente texto: {texto}\"\n",
        ")\n",
        "\n",
        "print(type(prompt_template))\n",
        "\n",
        "\n",
        "cadena_procesamiento = RunnableLambda(limpiar_texto) | prompt_template  # Primero limpia, luego aplica plantilla\n",
        "\n",
        "\n",
        "# Demostración\n",
        "print(\"\\nFlujo de procesamiento de texto:\")\n",
        "resultado = cadena_procesamiento.invoke(\"  EJEMPLO de Texto  \")\n",
        "print(resultado)"
      ],
      "metadata": {
        "id": "XyY9jG-sGypA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Transformacion de datos entre Runnables**\n",
        "\n",
        "Un caso de uso habitual de RunnableLambda es transformar los datosentre los pasos de una cadena de LangChain. Veamos cómo podemos usarlo para conseguir una lista a partir de un diccionario\n"
      ],
      "metadata": {
        "id": "f0lPe7NAfTk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Paso 1: Crear un RunnableLambda para extraer la lista de nombres.\n",
        "# Usamos list comprehension https://ellibrodepython.com/list-comprehension-python\n",
        "extract_names = RunnableLambda(\n",
        "    lambda data: [person[\"name\"] for person in data[\"people\"]]\n",
        ")\n",
        "\n",
        "# Paso 2: Probar la cadena con un diccionario de ejemplo.\n",
        "input_data = {\n",
        "    \"people\": [\n",
        "        {\"name\": \"Juan\", \"age\": 30},\n",
        "        {\"name\": \"Ana\", \"age\": 25},\n",
        "        {\"name\": \"Luis\", \"age\": 40}\n",
        "    ]\n",
        "}\n",
        "result = extract_names.invoke(input_data)\n",
        "\n",
        "# Imprimir el resultado final.\n",
        "print(result)\n",
        "print(type(result))"
      ],
      "metadata": {
        "id": "VJEuJLa7fxxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Filtrado de datos**\n",
        "\n",
        "Uno de los usos más poderosos de `RunnableLambda` es filtar los datos entre los pasos de una cadena de LangChain. Veamos cómo podemos usarlo para preparar la entrada de un `PromptTemplate`."
      ],
      "metadata": {
        "id": "gUJrx8R6I7_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginemos un caso de uso más realista donde necesitamos preprocesar datos antes de enviarlos a un LLM. Supongamos que tenemos una lista de productos y queremos filtrar aquellos que tienen un precio superior a un cierto valor.\n",
        "\n",
        "En este ejemplo, `RunnableLambda` actúa como una etapa de preprocesamiento en nuestro flujo.\n",
        "Podríamos integrar esto en una cadena más larga donde, por ejemplo, se le pide a un LLM que genere una descripción de estos productos caros."
      ],
      "metadata": {
        "id": "CIs4Lx1wQo22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "productos = [\n",
        "    {\"nombre\": \"Laptop\", \"precio\": 1200},\n",
        "    {\"nombre\": \"Teclado\", \"precio\": 75},\n",
        "    {\"nombre\": \"Monitor\", \"precio\": 300},\n",
        "    {\"nombre\": \"Ratón\", \"precio\": 25},\n",
        "]\n",
        "\n",
        "# Creamos una funcion que recibe una lista de productos y un precio minimo\n",
        "# y devuelve otra lista, filtrada\n",
        "def filtrar_productos_caros(productos: list[dict], precio_minimo: int) -> list[dict]:\n",
        "  \"\"\"Filtra productos cuyo precio es mayor o igual al precio mínimo.\"\"\"\n",
        "  return [producto for producto in productos if producto[\"precio\"] >= precio_minimo]\n",
        "\n",
        "# Construimos un runnable con nuestra funcion de filtro\n",
        "filtro_productos_runnable = RunnableLambda(\n",
        "    lambda x: filtrar_productos_caros(x, precio_minimo=100)\n",
        ")\n",
        "\n",
        "productos_caros = filtro_productos_runnable.invoke(productos)\n",
        "\n",
        "print(\"Lista de productos original:\")\n",
        "for producto in productos:\n",
        "  print(f\"- {producto['nombre']} (Precio: {producto['precio']})\")\n",
        "\n",
        "print(\"\\nProductos con precio mayor o igual a 100:\")\n",
        "for producto in productos_caros:\n",
        "  print(f\"- {producto['nombre']} (Precio: {producto['precio']})\")"
      ],
      "metadata": {
        "id": "BaxJcg-ZQs3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 6: Postprocesamiento**\n",
        "\n",
        "Otro patron de uso frecuente es validar la salida de un LLM antes de ser mostrada al usuario.\n",
        "\n",
        "Supongamos para este ejemplo que tenemos una lista de palabras prohibidas. Si la respuesta del LLM incluye alguna de ellas, la salida no debe mostrarse"
      ],
      "metadata": {
        "id": "xTQwtaJliD98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "forbidden_words = [\"prohibido\", \"secreto\", \"inadecuado\"]  # Ejemplo\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "def validate_output(aimessage):\n",
        "    text=aimessage.content\n",
        "    if any(word in text.lower() for word in forbidden_words):\n",
        "        return \"⚠️ Se ha detectado contenido prohibido. Operación detenida.\"\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "\n",
        "cadena = llm_gpt4o_mini | RunnableLambda(validate_output)\n",
        "\n",
        "# Esto no deberia producir una respuesta con palabras prohibidas\n",
        "prompt = \"Escribe una breve frase motivacional sobre el aprendizaje.\"\n",
        "respuesta = cadena.invoke(prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Respuesta:\", respuesta)\n",
        "\n",
        "# Esto SI deberia producir una respuesta con palabras prohibidas\n",
        "prompt = \"Menciona algo 'prohibido' que la gente suele hacer en secreto.\"\n",
        "respuesta = cadena.invoke(prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Respuesta:\", respuesta)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o__aiuEQirS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Desafio: Generador de TagLine**\n",
        "\n",
        "Supongamos que disponemos de una paso anterior de un diccionario con información de una pelicula con el titulo, el año y una breve descripcion.\n",
        "\n",
        "El objetivo es convertir este diccionario en un texto resumido que luego inyectaremos en un PromptTemplate. Por ejemplo, podemos combinar todos los campos y formatearlos en un breve párrafo.\n",
        "\n",
        "El prompt template tiene que consegiur que el LLM genere una Tagline llamativo"
      ],
      "metadata": {
        "id": "AtFfca6MmzaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "movie_data = {\n",
        "    \"title\": \"The Matrix\",\n",
        "    \"year\": 1999,\n",
        "    \"description\": \"Thomas Anderson lleva una doble vida: por el día es programador en una importante empresa de software, y por la noche un hacker informático llamado Neo. Su vida no volverá a ser igual cuando unos misteriosos personajes le inviten a descubrir la respuesta a la pregunta que no le deja dormir: ¿qué es Matrix?\"\n",
        "}\n",
        "\n",
        "# Esta funcion recibe el diccionario y devuleve un string\n",
        "def transform_data(data):\n",
        "    return f\"Title: {data['title']}\\nYear: {data['year']}\\nDescription: {data['description']}\"\n",
        "\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "tagline_template = PromptTemplate(\n",
        "    input_variables=[\"movie_info\"],\n",
        "    template=(\n",
        "        \"Eres un redactor creativo. Usa la información de la película a continuación para redactar 5 tagline \"\n",
        "        \"cortos, emocionantes y fáciles de recordar:\\n\\n\"\n",
        "        \"{movie_info}\\n\\n\"\n",
        "        \"Tagline:\"\n",
        "    )\n",
        ")\n",
        "\n",
        "cadena = RunnableLambda(transform_data) | tagline_template | llm_gpt4o_mini\n",
        "\n",
        "tagline = cadena.invoke(movie_data)\n",
        "print(tagline.content)\n"
      ],
      "metadata": {
        "id": "b2zi9ZuvnOBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}