{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPAAz+ap5XRsAtfKnSDjLtV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/RunnablePassthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. `RunnablePassthrouhg`**\n",
        "---\n",
        "\n",
        "Dentro de los Runnables, uno de los más sencillos —pero muy útil— es `RunnablePassthrough`. Este “pasa” sus datos de entrada directamente a la salida sin alterarlos. Puede parecer trivial, pero resulta práctico en situaciones en las que queremos que un eslabón de la cadena no modifique la información que recibe, sirviendo como “puente” para mantener la compatibilidad o facilidad de lectura en la cadena.\n",
        "\n",
        "Su mayor valor está en mantener la estructura de Runnables y cadenas, permitiéndonos insertar fácilmente lógica futura o puntos de inspección."
      ],
      "metadata": {
        "id": "uwodfkLmyG4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together y Anthropic\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n",
        "El uso de las API de OpenAI y Anthropic es de pago. El resto son gratuitas y para usarlas basta con registrarse y generar una API Key.  \n",
        "\n",
        "En el primer cuaderno encontraras los enlaces a estos servicios y este codigo explicado\n",
        "\n"
      ],
      "metadata": {
        "id": "rTQSpmGnzOgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "Nqml2kPRzN36"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: No hacer \"nada\"**\n",
        "---\n",
        "En su forma más sencilla, `RunnablePassthrough` recibe un input y **devuelve exactamente el mismo output**.\n",
        "\n",
        "#### **¿Por qué usar algo tan sencillo?**\n",
        "A veces en una Chain necesitas un paso que no modifique los datos, pero que sea compatible con la secuencia de Runnables. Por ejemplo, un eslabón que valide un formato o simplemente reenvíe la información a otro paso.\n",
        "\n"
      ],
      "metadata": {
        "id": "OzBWRHLwyrCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Creamos un RunnablePassthrough\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# Definimos un texto de ejemplo\n",
        "texto_entrada = \"Este texto será pasado sin cambios.\"\n",
        "\n",
        "# Ejecutamos el RunnablePassthrough\n",
        "resultado = passthrough.invoke(texto_entrada)\n",
        "\n",
        "print(\"Texto de entrada: \", texto_entrada)\n",
        "print(\"Resultado       : \", resultado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RPm9IHQzMPL",
        "outputId": "447fd521-e949-42a9-8a73-931fc0997087"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto de entrada :  Este texto será pasado sin cambios.\n",
            "Resultado        :  Este texto será pasado sin cambios.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Integrar RunnablePassthrough en una cadena**\n",
        "---\n",
        "Este ejempplo puede parecer un poco forzada pero trata de mostrar como se integra facilmente un RunnablePassthrough en una cadena.\n",
        "\n",
        "Pasaremos un promt a un LLM y el resultado de este pasara a traves del RunnablePassthrough a otro prompt y de ahi a otro LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "gdYmMeqN2NNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "prompt_template1 = PromptTemplate.from_template(\"Describe y condensa el siguinte tema en una sola frase: {tema}\")\n",
        "prompt_template2 = PromptTemplate.from_template(\"Describe y condensa la siguinete frase en una sola palabra: {frase}\")\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "mayusculas = RunnableLambda(lambda x: x.content.upper())\n",
        "\n",
        "chain = prompt_template1 | llm_gpt4o_mini | RunnablePassthrough() | prompt_template2 | llm_gpt4o_mini | mayusculas\n",
        "\n",
        "resultado=chain.invoke({\"tema\": \"La inteligencia artificial\"})\n",
        "\n",
        "resultado\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MRqO4TjH2Mym",
        "outputId": "3409d9a9-5119-442c-fbc5-99ac3ba37c85"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AUTOMATIZACIÓN.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te has dado cuenta de la \"dificultad\" de conocer lo que esta pasando dentro de la cadena ? Cual es la frase generada por el primer modelo ?\n",
        "\n",
        "Aqui podriamos hacer uso del RunnableLambda para imprimir el resultado intermedio en consola o cualquier otra tarea de debugger\n"
      ],
      "metadata": {
        "id": "CEEAxqDN5OwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ihaYmy1rgga_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "141a4972-fede-4f35-b7d0-88fc068eafae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La inteligencia artificial es la simulación de procesos de inteligencia humana por parte de sistemas computacionales, que buscan realizar tareas como el aprendizaje, razonamiento y autocorrección.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'INTELIGENCIA.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "prompt_template1 = PromptTemplate.from_template(\"Describe y condensa el siguinte tema en una sola frase: {tema}\")\n",
        "prompt_template2 = PromptTemplate.from_template(\"Describe y condensa la siguinete frase en una sola palabra: {frase}\")\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "mayusculas = RunnableLambda(lambda x: x.content.upper())\n",
        "\n",
        "def imprimir_log(x):\n",
        "  print(x.content)\n",
        "  return x  # es importante devolver lo mismo que recibimos\n",
        "\n",
        "imprimir_log_runnable = RunnableLambda(imprimir_log)\n",
        "\n",
        "chain = prompt_template1 | llm_gpt4o_mini | imprimir_log_runnable | RunnablePassthrough() | prompt_template2 | llm_gpt4o_mini | mayusculas\n",
        "\n",
        "resultado=chain.invoke({\"tema\": \"La inteligencia artificial\"})\n",
        "\n",
        "resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prepocesamiento y postprocesamiento de datos**\n",
        "---\n",
        "ara un caso de uso más realista, pensemos en un escenario donde:\n",
        "\n",
        "1. Tenemos un texto que **antes** de ser enviado a un LLM debe sufrir cierta limpieza o transformación (por ejemplo, eliminar caracteres especiales).\n",
        "2. Usamos el `RunnablePassthrough` como un punto de control que, en determinado momento, no hace nada, pero **podría** servir para inyectar validaciones o monitorear el flujo de datos.\n",
        "3. Tras obtener la respuesta del LLM, aplicamos algún postprocesamiento (usando un `RunnableLambda` o un parser).\n",
        "\n",
        "En un caso de producción podrías sustituir el passthrough por un paso de verificación, logging o incluso dejarlo como está si no necesitas modificar. La fortaleza de RunnablePassthrough es que no interfiere pero sí mantiene la estructura de un Runnable, lo cual facilita escalar el pipeline en el futuro."
      ],
      "metadata": {
        "id": "o00JaxaP_Sgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos que tenemos una función de preprocesamiento:\n",
        "def limpiar_texto(texto):\n",
        "    # Eliminamos caracteres no alfanuméricos (muy básico)\n",
        "    import re\n",
        "    texto_limpio = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ\\s]', '', texto)\n",
        "    return texto_limpio\n",
        "\n",
        "# Convertimos esta función en un RunnableLambda:\n",
        "preprocesador = RunnableLambda(limpiar_texto)\n",
        "\n",
        "# El RunnablePassthrough lo usaremos como \"puente\"\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# Para postprocesar, supongamos que queremos agregar metadata:\n",
        "def agregar_metadata(respuesta):\n",
        "    return {\n",
        "        \"respuesta\": respuesta,\n",
        "        \"metadata\": \"Procesado con LLMChain y validado\"\n",
        "    }\n",
        "\n",
        "postprocesador = RunnableLambda(agregar_metadata)\n",
        "\n",
        "\n",
        "\n",
        "pipeline_limpieza = preprocesador | passthrough | postprocesador\n",
        "    # Paso 1: Limpieza del texto\n",
        "    # Paso 2: Passthrough (punto de control)\n",
        "    # Paso 3: Postprocesamiento\n",
        "\n",
        "\n",
        "# Probemos con un texto con símbolos:\n",
        "texto_input = \"¡Hola! ¿Qué tal? LangChain 4ever #1\"\n",
        "\n",
        "resultado_avanzado = pipeline_limpieza.invoke(texto_input)\n",
        "print(\"Resultado avanzado:\", resultado_avanzado)"
      ],
      "metadata": {
        "id": "rnOgd8eO_sJ1",
        "outputId": "702efba7-3453-4591-9c00-c78ca646ce85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado avanzado: {'respuesta': 'Hola Qué tal LangChain ever ', 'metadata': 'Procesado con LLMChain y validado'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. `RunnablePassthrough.assign()`**\n",
        "---\n",
        "Una forma alternativa de pasar datos a través de los pasos de una cadena es dejar los valores actuales del estado de la cadena sin cambios mientras se asigna un nuevo valor bajo una clave determinada. El RunnablePassthrough.assign() método estático toma un valor de entrada y agrega los argumentos adicionales que se pasan a la función de asignación.\n",
        "\n",
        "\n",
        "Sin embargo `RunnablePassthrough` tiene un metodo especifico muy interesante: `.assign()`\n",
        "\n",
        "Este metodo resulta muy útil cuando queremos agregar o sobreescribir campos en la entrada de nuestro flujo, transformando el resultado en un diccionario y asignándole nuevas claves/valores antes de pasar al siguiente paso de la cadena.\n",
        "\n",
        "Es un atajo práctico para inyectar o complementar datos sin tener que construir manualmente un `RunnableLambda`\n",
        "\n"
      ],
      "metadata": {
        "id": "G5Ogku6JFCnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo 1:\n",
        "\n",
        "or defecto, LangChain asume que si la entrada no es un diccionario, se encapsula en un diccionario con la clave \"input\" (o \"text\" en algunos casos, dependiendo de la versión)."
      ],
      "metadata": {
        "id": "C_QY0Dj0GIRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1) Creamos un RunnablePassthrough normal\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# 2) Usar assign() con argumentos nombrados\n",
        "passthrough_asignado = passthrough.assign(lang=\"ES\")\n",
        "\n",
        "# 3) Invocamos el nuevo runnable con un string de entrada\n",
        "resultado = passthrough_asignado.invoke(\"Hola, mundo\")\n",
        "\n",
        "print(resultado)"
      ],
      "metadata": {
        "id": "zelroyC3GWKe",
        "outputId": "4a44cf69-9439-4ef3-f373-97127b689b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-006ce26397e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2) Usar assign() con argumentos nombrados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpassthrough_asignado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpassthrough\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ES\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 3) Invocamos el nuevo runnable con un string de entrada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mmapping\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mRunnableAssign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableParallel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     def invoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps__, **kwargs)\u001b[0m\n\u001b[1;32m   3535\u001b[0m         \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3536\u001b[0m         super().__init__(  # type: ignore[call-arg]\n\u001b[0;32m-> 3537\u001b[0;31m             \u001b[0msteps__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3538\u001b[0m         )\n\u001b[1;32m   3539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3535\u001b[0m         \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3536\u001b[0m         super().__init__(  # type: ignore[call-arg]\n\u001b[0;32m-> 3537\u001b[0;31m             \u001b[0msteps__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3538\u001b[0m         )\n\u001b[1;32m   3539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   5839\u001b[0m             \u001b[0;34mf\"Instead got an unsupported type: {type(thing)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5840\u001b[0m         )\n\u001b[0;32m-> 5841\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
          ]
        }
      ]
    }
  ]
}