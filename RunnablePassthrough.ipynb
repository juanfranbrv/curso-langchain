{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb+GE9owOhnPxX7KrVH7Fj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/RunnablePassthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. `RunnablePassthrouhg`**\n",
        "---\n",
        "\n",
        "Dentro de los Runnables, uno de los más sencillos —pero muy útil— es `RunnablePassthrough`. Este “pasa” sus datos de entrada directamente a la salida sin alterarlos. Puede parecer trivial, pero resulta práctico en situaciones en las que queremos que un eslabón de la cadena no modifique la información que recibe, sirviendo como “puente” para mantener la compatibilidad o facilidad de lectura en la cadena."
      ],
      "metadata": {
        "id": "uwodfkLmyG4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together y Anthropic\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n",
        "El uso de las API de OpenAI y Anthropic es de pago. El resto son gratuitas y para usarlas basta con registrarse y generar una API Key.  \n",
        "\n",
        "En el primer cuaderno encontraras los enlaces a estos servicios y este codigo explicado\n",
        "\n"
      ],
      "metadata": {
        "id": "rTQSpmGnzOgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "Nqml2kPRzN36"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: No hacer \"nada\"**\n",
        "---\n",
        "En su forma más sencilla, `RunnablePassthrough` recibe un input y **devuelve exactamente el mismo output**.\n",
        "\n",
        "#### **¿Por qué usar algo tan sencillo?**\n",
        "A veces en una Chain necesitas un paso que no modifique los datos, pero que sea compatible con la secuencia de Runnables. Por ejemplo, un eslabón que valide un formato o simplemente reenvíe la información a otro paso.\n",
        "\n"
      ],
      "metadata": {
        "id": "OzBWRHLwyrCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Creamos un RunnablePassthrough\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# Definimos un texto de ejemplo\n",
        "texto_entrada = \"Este texto será pasado sin cambios.\"\n",
        "\n",
        "# Ejecutamos el RunnablePassthrough\n",
        "resultado = passthrough.invoke(texto_entrada)\n",
        "\n",
        "print(\"Texto de entrada: \", texto_entrada)\n",
        "print(\"Resultado       : \", resultado)\n"
      ],
      "metadata": {
        "id": "7RPm9IHQzMPL",
        "outputId": "447fd521-e949-42a9-8a73-931fc0997087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto de entrada :  Este texto será pasado sin cambios.\n",
            "Resultado        :  Este texto será pasado sin cambios.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Integrar RunnablePassthrough en una cadena**\n",
        "---\n",
        "Este ejempplo puede parecer un poco forzada pero trata de mostrar como se integra facilmente un RunnablePassthrough en una cadena.\n",
        "\n",
        "Pasaremos un promt a un LLM y el resultado de este pasara a traves del RunnablePassthrough a otro prompt y de ahi a otro LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "gdYmMeqN2NNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "prompt_template1 = PromptTemplate.from_template(\"Describe y condensa el siguinte tema en una sola frase: {tema}\")\n",
        "prompt_template2 = PromptTemplate.from_template(\"Describe y condensa la siguinete frase en una sola palabra: {frase}\")\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "mayusculas = RunnableLambda(lambda x: x.content.upper())\n",
        "\n",
        "chain = prompt_template1 | llm_gpt4o_mini | RunnablePassthrough() | prompt_template2 | llm_gpt4o_mini | mayusculas\n",
        "\n",
        "resultado=chain.invoke({\"tema\": \"La inteligencia artificial\"})\n",
        "\n",
        "resultado\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MRqO4TjH2Mym",
        "outputId": "3409d9a9-5119-442c-fbc5-99ac3ba37c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AUTOMATIZACIÓN.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te has dado cuenta de la \"dificultad\" de conocer lo que esta pasando dentro de la cadena ? Cual es la frase generada por el primer modelo ?\n",
        "\n",
        "Aqui podriamos hacer uso del RunnableLambda para imprimir el resultado intermedio en consola o cualquier otra tarea de debugger\n"
      ],
      "metadata": {
        "id": "CEEAxqDN5OwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ihaYmy1rgga_",
        "outputId": "141a4972-fede-4f35-b7d0-88fc068eafae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La inteligencia artificial es la simulación de procesos de inteligencia humana por parte de sistemas computacionales, que buscan realizar tareas como el aprendizaje, razonamiento y autocorrección.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'INTELIGENCIA.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "prompt_template1 = PromptTemplate.from_template(\"Describe y condensa el siguinte tema en una sola frase: {tema}\")\n",
        "prompt_template2 = PromptTemplate.from_template(\"Describe y condensa la siguinete frase en una sola palabra: {frase}\")\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "mayusculas = RunnableLambda(lambda x: x.content.upper())\n",
        "\n",
        "def imprimir_log(x):\n",
        "  print(x.content)\n",
        "  return x  # es importante devolver lo mismo que recibimos\n",
        "\n",
        "imprimir_log_runnable = RunnableLambda(imprimir_log)\n",
        "\n",
        "chain = prompt_template1 | llm_gpt4o_mini | imprimir_log_runnable | RunnablePassthrough() | prompt_template2 | llm_gpt4o_mini | mayusculas\n",
        "\n",
        "resultado=chain.invoke({\"tema\": \"La inteligencia artificial\"})\n",
        "\n",
        "resultado"
      ]
    }
  ]
}