{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbEbo2252ceSozvIgac/uu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Splitting%20Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Splitting**\n",
        "---\n",
        "El **Text Splitting** (o división de texto) es una etapa crítica en los sistemas de **Retrieval-Augmented Generation (RAG)**. Consiste en fragmentar documentos extensos en segmentos más pequeños y manejables (_chunks_) antes de indexarlos para su recuperación posterior. Su implementación incide directamente en la calidad de la recuperación de información y, por ende, en las respuestas generadas por el modelo.\n",
        "\n",
        "* * *\n",
        "\n",
        "### **¿Por qué es importante?**\n",
        "\n",
        "1.  **Equilibrio entre contexto y precisión**:\n",
        "    \n",
        "    -   **Chunks demasiado grandes**: Pueden contener información redundante o irrelevante, lo que \"diluye\" el contexto clave y reduce la precisión del retrieval.\n",
        "        \n",
        "    -   **Chunks demasiado pequeños**: Pierden contexto necesario para entender el significado completo (p. ej., una frase sin su párrafo asociado).\n",
        "        \n",
        "    -   Un buen _splitting_ mantiene la coherencia semántica en cada segmento, facilitando que el sistema recupere los fragmentos más relevantes para una consulta.\n",
        "        \n",
        "2.  **Compatibilidad con modelos de embedding**:\n",
        "    \n",
        "    -   Los modelos de embedding (como SBERT o OpenAI) tienen límites óptimos de longitud de texto. Por ejemplo, un chunk de 512 tokens funciona bien con muchos codificadores, pero un texto más largo podría truncarse o perder información crítica.\n",
        "        \n",
        "3.  **Impacto en la generación de respuestas**:\n",
        "    \n",
        "    -   Los chunks recuperados alimentan al modelo generador (como GPT-4). Si están mal estructurados, el modelo recibirá información fragmentada o fuera de contexto, lo que generará respuestas inconsistentes o inexactas.\n",
        "        \n",
        "4.  **Eficiencia computacional**:\n",
        "    \n",
        "    -   Chunks bien dimensionados reducen costos de procesamiento y latencia, ya que evitan sobrecargar el sistema con datos innecesarios.\n",
        "\n",
        "\n",
        "### El Text Splitting no es un paso mecánico, sino una decisión estratégica que determina cómo el sistema \"ve\" la información y afectara enormemente a la calidad del sistema."
      ],
      "metadata": {
        "id": "fx7k4YhwIWp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nivel 1: División por Caracteres**  \n",
        "---\n",
        "La **división por caracteres** es la forma más básica de fragmentar texto. Consiste simplemente en dividir el texto en _chunks_ (segmentos) de un tamaño fijo de **N caracteres**, ignorando por completo el contenido o estructura del texto.\n",
        "\n",
        "Este método **no se recomienda para aplicaciones reales**, pero es un punto de partida útil para comprender los fundamentos del _text splitting_.\n",
        "\n",
        "**Ventajas**:\n",
        "\n",
        "-   Fácil y sencillo de implementar.\n",
        "    \n",
        "\n",
        "**Desventajas**:\n",
        "\n",
        "-   Muy rígido: **no considera la estructura del texto** (p. ej., separación de párrafos, puntuación o temas).\n",
        "    \n",
        "-   Puede fragmentar ideas o contextos clave a la mitad.\n",
        "    \n",
        "\n",
        "**Conceptos clave**:\n",
        "\n",
        "-   **Tamaño del chunk (_Chunk Size_)**: Número de caracteres por segmento (ej: 50, 100, 1000).\n",
        "    \n",
        "-   **Solapamiento de chunks (_Chunk Overlap_)**: Cantidad de caracteres que se superponen entre chunks consecutivos. Esto ayuda a evitar que una misma información contextual quede dividida en múltiples chunks, aunque genera duplicación de datos.\n",
        "\n"
      ],
      "metadata": {
        "id": "_nDBMJ7mKa9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list that will hold your chunks\n",
        "chunks = []\n",
        "\n",
        "chunk_size = 35 # Characters\n",
        "\n",
        "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
        "for i in range(0, len(text), chunk_size):\n",
        "    chunk = text[i:i + chunk_size]\n",
        "    chunks.append(chunk)\n",
        "chunks"
      ],
      "metadata": {
        "id": "wji55mhCK31k",
        "outputId": "7494413b-c23c-432b-89e6-6bd437242e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is the text I would like to ch',\n",
              " 'unk up. It is the example text for ',\n",
              " 'this exercise']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}