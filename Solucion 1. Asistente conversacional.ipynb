{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR5-dW-edsqa"
      },
      "source": [
        "# **Asistente conversacional con gpt-4o-mini**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Configuración del entorno del cuaderno**\n",
        "---\n",
        "En esta celda se importan las librerías necesarias y se configuran las claves API para diferentes servicios. También se instalan las dependencias de LangChain y sus integraciones con OpenAI, Groq, Google GenAI y HuggingFace.(Este código se explico con detalle en el primer cuaderno)"
      ],
      "metadata": {
        "id": "uEBllvD_4Dsg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "dUEAeIGLdsqc"
      },
      "outputs": [],
      "source": [
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Configuración del modelo**\n",
        "---\n",
        "Aquí se configura el modelo de OpenAI (`gpt-4o-mini`) con la clave API y un parámetro de temperatura para controlar la creatividad de las respuestas."
      ],
      "metadata": {
        "id": "z0f3NtKU5Krv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)"
      ],
      "metadata": {
        "id": "vtyLWCrmeaM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Preparamos las plantillas**\n",
        "---\n",
        "Se definen dos plantillas: una para el sistema (instrucciones del tutor) y otra para el usuario (mensaje del estudiante). Estas plantillas se utilizarán para generar prompts estructurados."
      ],
      "metadata": {
        "id": "R2y0zKzd5iAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las plantillas\n",
        "template_sistema = \"\"\"\\\n",
        "Eres un tutor amigable de {idioma}. Tu trabajo es:\n",
        "1. Mantener una conversación simple con el estudiante\n",
        "2. Corregir errores básicos de gramática y ortografía si los hay\n",
        "3. Ser paciente y motivador\n",
        "\"\"\"\n",
        "\n",
        "template_usuario = \"\"\"\\\n",
        "El estudiante dice: {mensaje_usuario}\n",
        "\n",
        "Por favor, responde de manera natural y si hay algún error gramatical,\n",
        "corrígelo sutilmente en tu respuesta.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WnhNMW6FepeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Creamos el ChatPromptTemplate**\n",
        "---\n",
        "Se crea un `ChatPromptTemplate` utilizando las plantillas definidas. Se muestra una alternativa para crear el mismo template, ambas formas son válidas.  \n",
        "\n",
        "Primera forma:  \n",
        "Se utilizan clases específicas (SystemMessagePromptTemplate y HumanMessagePromptTemplate) para crear los mensajes del sistema y del usuario, respectivamente.  \n",
        "\n",
        "Segunda forma:  \n",
        "Se utiliza una tupla (role, template) para definir los mensajes, donde role es el tipo de mensaje (por ejemplo, \"system\" o \"human\") y template es la plantilla de texto.\n",
        "\n",
        "Es más conciso y directo, ya que no requiere instanciar clases específicas."
      ],
      "metadata": {
        "id": "svs0EY5b51SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(template_sistema),\n",
        "    HumanMessagePromptTemplate.from_template(template_usuario),\n",
        "])\n",
        "\n",
        "# Podria ser asi ???? explicar\n",
        "\n",
        "# Crear el ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template_sistema),\n",
        "    (\"human\", template_usuario),\n",
        "])"
      ],
      "metadata": {
        "id": "wm3V7ogoewC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Función para chatear en un idioma específico**\n",
        "---\n",
        "Esta función toma un mensaje del usuario y un idioma (en principio esta fijado en español), formatea el prompt, invoca el modelo de lenguaje y devuelve la respuesta. Es el núcleo del asistente conversacional."
      ],
      "metadata": {
        "id": "SkJ4RLqv8LIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatear_en_idioma(mensaje_usuario, idioma=\"español\"):\n",
        "    \"\"\"\n",
        "    Función simple para procesar mensajes del usuario y obtener respuestas\n",
        "    1. Recibe el mensaje del usuario\n",
        "    2. Lo inyecta en el chatprompttemplate\n",
        "    3. Invoca el modelo con el\n",
        "    4. Devuelve la respuesta\n",
        "    \"\"\"\n",
        "    # Formatear el prompt con el mensaje del usuario y el idioma\n",
        "    prompt = chat_prompt.format(\n",
        "        mensaje_usuario=mensaje_usuario,\n",
        "        idioma=idioma\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del modelo\n",
        "    respuesta = llm.invoke(prompt)\n",
        "\n",
        "    return respuesta.content"
      ],
      "metadata": {
        "id": "hHmiu_rle0Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6.  Interfaz de chat interactivo.**  \n",
        "---  \n",
        "Se implementa un bucle interactivo que permite al usuario chatear con el asistente. El usuario puede escribir \"salir\" para terminar la conversación. El asistente responde corrigiendo errores y manteniendo una conversación natural."
      ],
      "metadata": {
        "id": "GpDS4PHk8lZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"¡Bienvenido al Chatbot de Aprendizaje de Idiomas!\")\n",
        "    print(\"Escribe 'salir' para terminar la conversación\")\n",
        "\n",
        "    while True:\n",
        "        # Obtener entrada del usuario\n",
        "        mensaje = input(\"\\nTú: \")\n",
        "\n",
        "        if mensaje.lower() == 'salir':\n",
        "            print(\"¡Hasta luego! Gracias por practicar.\")\n",
        "            break\n",
        "\n",
        "        # Obtener y mostrar la respuesta\n",
        "        respuesta = chatear_en_idioma(mensaje)\n",
        "        print(\"\\nTutor:\", respuesta)"
      ],
      "metadata": {
        "id": "3_jenBqIe5Gv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}