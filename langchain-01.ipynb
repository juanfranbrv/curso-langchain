{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Instalamos las bibliotecas necesarias  \n",
        " Instala las bibliotecas principales (langchain) y sus integraciones con diversos modelos de lenguaje (OpenAI, Groq, Google Gemini).  \n",
        "\n",
        " La opción -qU asegura una instalación silenciosa y que tengamos las últimas versiones disponibles."
      ],
      "metadata": {
        "id": "Mi2sqEbTVGQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -qU\n",
        "\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU"
      ],
      "metadata": {
        "id": "Mc8S-4mgj8VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de las clases necesarias\n",
        "\n",
        "Este bloque de código importa las clases necesarias de langchain para crear y utilizar los modelos de lenguaje de OpenAI, Groq y Google.  \n",
        "\n",
        "Estas clases son un envoltorio (*wrapper*) para las APIs de sus respectivos propietarios (OpenAI, Groq y Google)\n",
        "\n",
        "Tambien las clases que proporciona Langchain para la creación de mensajes de usuario y sistema."
      ],
      "metadata": {
        "id": "X9keoznMXNTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "_60Sdpp0n8Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga Segura de Claves API\n",
        "\n",
        "Utilizamos la funcionalidad de Google Colab para cargar de forma segura las claves API necesarias para acceder a los modelos de lenguaje. Las claves API de OpenAI, Groq, y Google se obtienen del sistema de configuración de Colab y se asignan a variables para su posterior uso. Este proceso evita la necesidad de codificar las claves directamente en el código, lo que representa una mejor práctica de seguridad."
      ],
      "metadata": {
        "id": "8q-cZ5A6YUXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# LANGCHAIN_TRACING_V2=True\n",
        "# LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "# LANGCHAIN_API_KEY=\"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\"\n",
        "# LANGCHAIN_PROJECT=\"pr-minty-idiom-15\""
      ],
      "metadata": {
        "id": "iQMk_1lnonK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8EkWezHj1S_"
      },
      "outputs": [],
      "source": [
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "modelo3 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=GOOGLE_API_KEY)\n",
        "\n",
        "mensajes = [\n",
        "            SystemMessage(\"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "            HumanMessage(\"Estar en la edad del pavo\")\n",
        "           ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo1.invoke(mensajes)"
      ],
      "metadata": {
        "id": "I0BBP2-zaQml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo2.invoke(mensajes)"
      ],
      "metadata": {
        "id": "YfvmxobOaQcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo3.invoke(mensajes)"
      ],
      "metadata": {
        "id": "upGJmBNPaUHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain también admite entradas de modelos de chat mediante cadenas o formato OpenAI  \n",
        "Son equivalentes los siguientes:"
      ],
      "metadata": {
        "id": "-0Ys3ydEcAiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo2.invoke(\"Traduce a un frase equivalente (idiom) en inglés: Más vale pájaro en mano que ciento volando\")"
      ],
      "metadata": {
        "id": "Deh2BAhtcDTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo2.invoke([\n",
        "    {\"role\":\"system\", \"content\": \"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"},\n",
        "    {\"role\":\"user\", \"content\": \"Más vale pájaro en mano que ciento volando\"}\n",
        "    ])"
      ],
      "metadata": {
        "id": "1uA1DVtpdMlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que la respuesta del metodo .invoque() devuelve mucha informacion util o que podria ser util en aplicaciones mas complejas.  \n",
        "\n",
        "AIMessage es una clase que contiene información sobre la respuesta de un modelo de lenguaje.\n",
        "\n",
        "Representa el resultado del modelo y consta tanto del resultado sin procesar como devuelto por el modelo junto con campos estandarizados (por ejemplo, llamadas a herramientas, metadatos de uso) agregados por el marco LangChain.\n",
        "\n",
        "- content (str): El contenido textual de la respuesta del modelo.\n",
        "\n",
        "- additional_kwargs (dict): Un diccionario para información adicional específica del proveedor del modelo (puede estar vacío en algunos casos).\n",
        "\n",
        "- response_metadata (dict): Información sobre la respuesta, como el uso de tokens, el nombre del modelo, etc.\n",
        "\n",
        "- id (str): Un identificador único para el mensaje.\n",
        "\n",
        "- usage_metadata (dict): Información de uso de tokens (entrada, salida y total)."
      ],
      "metadata": {
        "id": "MSgeyJ8Gfham"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_message = modelo2.invoke([\n",
        "    SystemMessage(\"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    HumanMessage(\"Tener la mosca detrás de la oreja\")\n",
        "            ])\n",
        "\n",
        "print(\"Contenido de la respuesta (content):\\n\", ai_message.content)\n",
        "print(\"========\")\n",
        "print(\"Metadatos de la respuesta (response_metadata): \\n\", ai_message.response_metadata)\n",
        "print(\"========\")\n",
        "print(\"Tokens utilizados (usage_metadata): \\n\", ai_message.usage_metadata)\n",
        "print(\"========\")\n",
        "print(\"Identificador de la respuesta (id):\", ai_message.id)\n",
        "print(\"========\")\n",
        "print(\"Uso de tokens detallado(response_metadata['token_usage']:\\n\", ai_message.response_metadata['token_usage'])"
      ],
      "metadata": {
        "id": "PzlKEeGndxP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming\n",
        "\n",
        "**Conceptos Clave:**\n",
        "\n",
        "- **Modelos de Chat \"Ejecutables\":**\n",
        "    \n",
        "    - Imagina que los modelos de chat (como GPT, Gemini, etc.) son como funciones que puedes \"ejecutar\" o \"llamar\".\n",
        "        \n",
        "    - En Langchain, estos modelos tienen una interfaz estándar, lo que significa que todos se comportan de manera similar cuando los usas.\n",
        "        \n",
        "    \n",
        "- **Modos de Invocación:**\n",
        "    \n",
        "    - **Invocación Normal (Síncrona):** Es como preguntar algo y esperar a que el modelo te dé la respuesta completa de una sola vez.\n",
        "        \n",
        "    - **Invocación Asíncrona:** Es como preguntar algo y dejar que el modelo trabaje en segundo plano. Puedes hacer otras cosas mientras esperas la respuesta.\n",
        "        \n",
        "    - **Transmisión (Streaming):** Es como preguntar algo y recibir la respuesta poco a poco, palabra por palabra o token por token, en lugar de esperar a que esté completa.\n",
        "        \n",
        "    \n",
        "- **Transmisión de Tokens:**\n",
        "    \n",
        "    - Los modelos de lenguaje generan texto dividiéndolo en \"tokens\". Un token puede ser una palabra, una parte de una palabra o un signo de puntuación.\n",
        "        \n",
        "    - La transmisión de tokens significa que el modelo te envía cada token a medida que lo genera, en lugar de esperar a tener toda la respuesta.\n",
        "        \n",
        "    \n",
        "\n",
        "**¿Por qué es Útil la Transmisión de Tokens?**\n",
        "\n",
        "- **Experiencia de Usuario Mejorada:**\n",
        "    \n",
        "    - En lugar de esperar un largo tiempo sin saber si el modelo está funcionando, el usuario ve la respuesta aparecer gradualmente. Esto hace que la interacción sea más fluida y natural.\n",
        "        \n",
        "    - Es como ver a alguien escribir en tiempo real en lugar de esperar a que termine de escribir todo el texto.\n",
        "        \n",
        "    \n",
        "- **Respuestas Más Rápidas (Aparentemente):**\n",
        "    \n",
        "    - Aunque el tiempo total para generar la respuesta puede ser el mismo, la transmisión hace que parezca que la respuesta llega más rápido, ya que el usuario ve el texto aparecer poco a poco.\n",
        "        \n",
        "    \n",
        "- **Procesamiento en Tiempo Real:**\n",
        "    \n",
        "    - Puedes empezar a procesar la respuesta del modelo a medida que se genera, sin tener que esperar a que esté completa. Esto es útil en aplicaciones que necesitan reaccionar a la respuesta del modelo en tiempo real."
      ],
      "metadata": {
        "id": "35GGV06vlIsK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5n1KnKDkt94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}