{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1\\. ¿Qué es LangChain y por qué es relevante en el mundo de las LLMs?\n",
        "---\n",
        "\n",
        "### 1.1. LangChain en pocas palabras\n",
        "\n",
        "**LangChain** es un framework diseñado para facilitar la construcción de aplicaciones basadas en _Large Language Models (LLMs)_. Proporciona herramientas y abstracciones que permiten **conectar** distintos modelos e integrarlos en _pipelines_ o _flujos de trabajo_ (chains) más complejos. Con LangChain podemos, por ejemplo:\n",
        "\n",
        "- **Construir** aplicaciones que combinen múltiples llamadas a modelos de lenguaje.\n",
        "- **Enriquecer** las respuestas con contexto proveniente de bases de datos o documentos.\n",
        "- **Implementar memorias** que recuerden datos de interacciones previas.\n",
        "\n",
        "Su objetivo principal es **simplificar** el desarrollo de aplicaciones que aprovechan modelos de lenguaje, a la vez que proporciona **flexibilidad** para integrar servicios de diferentes proveedores y orquestar diferentes estrategias de prompting.\n",
        "\n",
        "### 1.2. Importancia y casos de uso de las LLMs\n",
        "\n",
        "Las _Large Language Models (LLMs)_ son redes neuronales especializadas en procesar y generar lenguaje natural. Modelos como GPT-3, GPT-4 o Llama 2 han demostrado ser muy eficaces para:\n",
        "\n",
        "- **Generación de texto**: redacción de artículos, guiones de vídeo, textos creativos, etc.\n",
        "- **Traducciones y resumidos**: convertir textos entre diferentes idiomas o generar resúmenes de documentos extensos.\n",
        "- **Asistentes conversacionales**: chatbots que mantienen el contexto de la conversación y pueden llevar a cabo tareas o responder preguntas.\n",
        "- **Análisis y clasificación** de textos: etiquetar o extraer información relevante de grandes volúmenes de texto.\n",
        "- **Soporte en programación**: autocompletar código o explicar algoritmos.\n",
        "\n",
        "Dada esta variedad de posibilidades, integrar LLMs en productos o proyectos se ha convertido en una **prioridad** para muchas empresas y desarrolladores. Y es justo ahí donde **LangChain** ofrece un entorno de desarrollo potente, ágil y modular."
      ],
      "metadata": {
        "id": "Ef9QGtxV_AW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Obtener claves API\n",
        "---\n",
        "Para que la mayor parte de ejemplos de este cuaderno funcionen es necesario obtener claves API de lose servicios que vayamos a usar.  \n",
        "Para obtener las claves API que deberá luego introducir en el cuaderno debe ir a:\n",
        "\n",
        "GROQ: https://console.groq.com/keys  \n",
        "GOOGLE AI STUDIO: https://aistudio.google.com/apikey  \n",
        "HUFFING FACE: https://huggingface.co/settings/tokens  \n",
        "OPENAI: https://platform.openai.com/api-keys"
      ],
      "metadata": {
        "id": "YjL7sgN5e8wl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q-cZ5A6YUXD"
      },
      "source": [
        "## 3. Carga Segura de Claves API\n",
        "---\n",
        "\n",
        "Este código se utiliza en Google Colab para obtener claves de API (tokens de acceso) almacenadas de forma segura en la sección \"Secretos\" de Colab y se asignan a variables para su posterior uso.\n",
        "\n",
        "Estas claves son necesarias para acceder a servicios externos, como OpenAI, Groq, Google o Hugging Face, desde tu notebook.  \n",
        "\n",
        "Este proceso evita la necesidad de codificar las claves directamente en el código, lo que representa una mejor práctica de seguridad.  \n",
        "\n",
        "![Configuración de Secretos en Colab](https://github.com/juanfranbrv/curso-langchain/blob/main/images/secretos.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQMk_1lnonK3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi2sqEbTVGQg"
      },
      "source": [
        "## 4. Instalación de las bibliotecas necesarias\n",
        "---\n",
        "\n",
        "**Cambios desde Colab**  \n",
        "\n",
        " Instala las bibliotecas principales (langchain) y sus integraciones con diversos modelos de lenguaje (OpenAI, Groq, Google Gemini).  \n",
        "\n",
        " La opción -qU asegura una instalación silenciosa y que tengamos las últimas versiones disponibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc8S-4mgj8VS"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -qU\n",
        "\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9keoznMXNTx"
      },
      "source": [
        "## 5. Importación de las clases necesarias\n",
        "---\n",
        "\n",
        "Este bloque de código importa las clases necesarias de langchain para crear y utilizar los modelos de lenguaje de OpenAI, Groq y Google.  \n",
        "\n",
        "Estas clases son un envoltorio (*wrapper*) para las APIs de sus respectivos propietarios (OpenAI, Groq y Google)\n",
        "\n",
        "Tambien las clases que proporciona Langchain para la creación de mensajes de usuario y sistema, que usamos para contruir un mensaje (*prompt*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_60Sdpp0n8Pv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8EkWezHj1S_"
      },
      "outputs": [],
      "source": [
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "modelo3 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=GOOGLE_API_KEY)\n",
        "modelo4 = HuggingFaceEndpoint(repo_id=\"Qwen/Qwen2.5-72B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
        "\n",
        "prompt = [\n",
        "            SystemMessage(\"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "            HumanMessage(\"Estar en la edad del pavo\")\n",
        "           ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Invocamos los modelos\n",
        "---\n",
        "Usamos el metodo .invoke() de los modelos instanciados para llamar a los LLM pasandoles el mensaje."
      ],
      "metadata": {
        "id": "XYnbLkyihy2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0BBP2-zaQml"
      },
      "outputs": [],
      "source": [
        "modelo1.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfvmxobOaQcQ"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upGJmBNPaUHg"
      },
      "outputs": [],
      "source": [
        "modelo3.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo4.invoke(prompt)"
      ],
      "metadata": {
        "id": "hzRzGdQnZBSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Ys3ydEcAiS"
      },
      "source": [
        "### 7. Otras formas de contruir el prompt\n",
        "---\n",
        "LangChain también admite entradas de modelos de chat mediante cadenas o formato OpenAI  \n",
        "Son equivalentes los siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deh2BAhtcDTT"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(\"Traduce a un frase equivalente (idiom) en inglés: Más vale pájaro en mano que ciento volando\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uA1DVtpdMlk"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke([\n",
        "    {\"role\":\"system\", \"content\": \"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"},\n",
        "    {\"role\":\"user\", \"content\": \"Más vale pájaro en mano que ciento volando\"}\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgeyJ8Gfham"
      },
      "source": [
        "##8. Respuesta del LLM\n",
        "Observa que la respuesta del metodo `.invoke()` devuelve mucha informacion util o que podria ser util en aplicaciones mas complejas.  \n",
        "\n",
        "AIMessage es una clase que contiene información sobre la respuesta de un modelo de lenguaje.\n",
        "\n",
        "Representa el resultado del modelo y consta tanto del resultado sin procesar como devuelto por el modelo junto con campos estandarizados (por ejemplo, llamadas a herramientas, metadatos de uso) agregados por el marco LangChain.\n",
        "\n",
        "- content (str): El contenido textual de la respuesta del modelo.\n",
        "\n",
        "- additional_kwargs (dict): Un diccionario para información adicional específica del proveedor del modelo (puede estar vacío en algunos casos).\n",
        "\n",
        "- response_metadata (dict): Información sobre la respuesta, como el uso de tokens, el nombre del modelo, etc.\n",
        "\n",
        "- id (str): Un identificador único para el mensaje.\n",
        "\n",
        "- usage_metadata (dict): Información de uso de tokens (entrada, salida y total)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzlKEeGndxP0"
      },
      "outputs": [],
      "source": [
        "ai_message = modelo2.invoke([\n",
        "    SystemMessage(\"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    HumanMessage(\"Tener la mosca detrás de la oreja\")\n",
        "            ])\n",
        "\n",
        "print(\"Contenido de la respuesta (content):\\n\", ai_message.content)\n",
        "print(\"========\")\n",
        "print(\"Metadatos de la respuesta (response_metadata): \\n\", ai_message.response_metadata)\n",
        "print(\"========\")\n",
        "print(\"Tokens utilizados (usage_metadata): \\n\", ai_message.usage_metadata)\n",
        "print(\"========\")\n",
        "print(\"Identificador de la respuesta (id):\", ai_message.id)\n",
        "print(\"========\")\n",
        "print(\"Uso de tokens detallado(response_metadata['token_usage']:\\n\", ai_message.response_metadata['token_usage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GGV06vlIsK"
      },
      "source": [
        "## 9. Streaming\n",
        "---\n",
        "\n",
        "**Conceptos Clave:**\n",
        "\n",
        "- **Modelos de Chat \"Ejecutables\":**\n",
        "    \n",
        "    - Imagina que los modelos de chat (como GPT, Gemini, etc.) son como funciones que puedes \"ejecutar\" o \"llamar\".\n",
        "        \n",
        "    - En Langchain, estos modelos tienen una interfaz estándar, lo que significa que todos se comportan de manera similar cuando los usas.\n",
        "        \n",
        "    \n",
        "- **Modos de Invocación:**\n",
        "    \n",
        "    - **Invocación Normal (Síncrona):** Es como preguntar algo y esperar a que el modelo te dé la respuesta completa de una sola vez.\n",
        "        \n",
        "    - **Invocación Asíncrona:** Es como preguntar algo y dejar que el modelo trabaje en segundo plano. Puedes hacer otras cosas mientras esperas la respuesta.\n",
        "        \n",
        "    - **Transmisión (Streaming):** Es como preguntar algo y recibir la respuesta poco a poco, palabra por palabra o token por token, en lugar de esperar a que esté completa.\n",
        "        \n",
        "    \n",
        "- **Transmisión de Tokens:**\n",
        "    \n",
        "    - Los modelos de lenguaje generan texto dividiéndolo en \"tokens\". Un token puede ser una palabra, una parte de una palabra o un signo de puntuación.\n",
        "        \n",
        "    - La transmisión de tokens significa que el modelo te envía cada token a medida que lo genera, en lugar de esperar a tener toda la respuesta.\n",
        "        \n",
        "    \n",
        "\n",
        "**¿Por qué es Útil la Transmisión de Tokens?**\n",
        "\n",
        "- **Experiencia de Usuario Mejorada:**\n",
        "    \n",
        "    - En lugar de esperar un largo tiempo sin saber si el modelo está funcionando, el usuario ve la respuesta aparecer gradualmente. Esto hace que la interacción sea más fluida y natural.\n",
        "        \n",
        "    - Es como ver a alguien escribir en tiempo real en lugar de esperar a que termine de escribir todo el texto.\n",
        "        \n",
        "    \n",
        "- **Respuestas Más Rápidas (Aparentemente):**\n",
        "    \n",
        "    - Aunque el tiempo total para generar la respuesta puede ser el mismo, la transmisión hace que parezca que la respuesta llega más rápido, ya que el usuario ve el texto aparecer poco a poco.\n",
        "        \n",
        "    \n",
        "- **Procesamiento en Tiempo Real:**\n",
        "    \n",
        "    - Puedes empezar a procesar la respuesta del modelo a medida que se genera, sin tener que esperar a que esté completa. Esto es útil en aplicaciones que necesitan reaccionar a la respuesta del modelo en tiempo real."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este codigo usamos el metodo `.stream()` en lugar de `.invoke()` que devuelve un genera un flujo de tokens con la respuestas y que podemos iterar con un bucle."
      ],
      "metadata": {
        "id": "KPyImbrOcQO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5n1KnKDkt94"
      },
      "outputs": [],
      "source": [
        "for token in modelo2.stream(\"Explica los verbos modales en inglés.\"):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. Runnables\n",
        "---\n",
        "En LangChain, un Runnable es una abstracción que representa cualquier componente capaz de ejecutar una tarea, como modelos de lenguaje, cadenas de operaciones o herramientas personalizadas. Los modelos de lenguaje, como ChatGroq o OpenAI, son ejemplos clave de Runnable, ya que pueden generar texto o realizar tareas específicas al recibir una entrada. Estos componentes son fundamentales para construir flujos de trabajo, ya que permiten encadenar y combinar operaciones de manera modular y flexible.\n",
        "\n",
        "Los Runnable ofrecen métodos como `.invoke(),` .`stream()` y `.batch()` para manejar diferentes formas de ejecución. Por ejemplo, `.invoke()` devuelve el resultado completo de una tarea, mientras que `.stream()` genera un flujo incremental de resultados, ideal para respuestas largas o en tiempo real. Esta flexibilidad hace que los Runnable sean esenciales para integrar modelos de lenguaje en aplicaciones complejas, permitiendo desde simples llamadas a modelos hasta flujos de trabajo personalizados y escalables."
      ],
      "metadata": {
        "id": "EvQ18A9ub82M"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}