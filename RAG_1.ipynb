{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/juanfranbrv/curso-langchain/blob/main/RAG_1.ipynb",
      "authorship_tag": "ABX9TyNN9Jmwzo6faj3Hi4UlDbQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/RAG_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "DEEPSEEK_API_KEY=userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "\n",
        "%pip install langchain_community -qU\n",
        "%pip install langchainhub -qU\n",
        "\n",
        "%pip install pypdf -qU\n",
        "\n",
        "\n",
        "%pip install chromadb\n",
        "%pip install langchain-chroma -qU\n",
        "\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "Nqml2kPRzN36"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método .load() se llama en la instancia de PyPDFDirectoryLoader para cargar los documentos PDF desde el directorio especificado.\n",
        "\n",
        "Este método lee todos los archivos PDF en el directorio, extrae su contenido y lo convierte en un formato que puede ser utilizado por LangChain, como una lista de objetos Document.\n",
        "\n",
        "El resultado de PyPDFDirectoryLoader(\"/content//\").load() se almacena en la variable documents.\n",
        "\n",
        "documents contendrá una lista de objetos Document, donde cada objeto representa una pagina de un archivo PDF cargado.\n",
        "\n",
        "Este cargador lee cada página de cada PDF en el directorio y crea un objeto Document por página.\n",
        "\n",
        "Es decir, si tienes 8 PDFs y cada uno tiene, por ejemplo, 100 páginas, obtendrías 8 * 100 = 800 documentos (uno por página)."
      ],
      "metadata": {
        "id": "cCwcA-mUNVCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "documents = PyPDFDirectoryLoader(\"/content/pdfs\").load()"
      ],
      "metadata": {
        "id": "oJxe2NYtNVbm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))\n",
        "print(documents[500])\n",
        "print(documents[1500])\n"
      ],
      "metadata": {
        "id": "y96v6MNnnaZl",
        "outputId": "c598fb82-a581-44e5-d0b4-a533a66c8ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3591\n",
            "page_content='1 from pymongo import MongoClient\n",
            "2 import datetime\n",
            "3\n",
            "4 # de\u0000nimos los parámetros de conexión al servidor MongoDB\n",
            "5 client = MongoClient('mongodb://localhost:27017/')\n",
            "6\n",
            "7 # abrimos la base de datos \"hoteles\"\n",
            "8 db = client.hoteles\n",
            "9\n",
            "10 # accedemos a la colección \"opiniones\"\n",
            "11 empleados = db.opiniones\n",
            "12\n",
            "13 # creamos una nueva opinión\n",
            "14 opinion = {\"autor\": \"Perico Martos\",\n",
            "15 \"texto\": \"Un lugar fantástico para disfrutar en familia.\",\n",
            "16 \"hotel\": \"Parador nacional\",\n",
            "17 \"lugar\": \"Cazalla de la Sierra\",\n",
            "18 \"creacion\": datetime.datetime.utcnow()}\n",
            "19\n",
            "20 # la añadimos a la colección y obtenemos el ID asignado\n",
            "21 opinion_id = opiniones.insert_one(opinion).inserted_id\n",
            "Otras bibliotecas y herramientas\n",
            "Para finalizar este extenso apéndice no hemos querido dejar fuera algunas\n",
            "opciones difíciles de encajar en los ámbitos anteriores. No obstante,\n",
            "constituyen herramientas y bibliotecas muy conocidas y utilizados por los\n",
            "desarrolladores experimentados de Python.\n",
            "Nose\n",
            "nose.readthedocs.io/en/latest/. (Incluido en la distribución base deAnaconda)\n",
            "Nose, más que una biblioteca, es una herramienta que simplifica la\n",
            "implementación y ejecución de pruebas de unidad sobre nuestro código,\n",
            "como vimos con unittest. A diferencia de este módulo, con Nose no es\n",
            "necesario derivar clases ni importar biblioteca alguna. Podemos introducir las\n",
            "comprobaciones en el código de las pruebas directamente. Luego basta con\n",
            "lanzar esas pruebas usando la utilidad nosetests.\n",
            "1 ejemplos = [('Juan', 23), ('María', 56), ('Darío', 7)]' metadata={'source': '/content/pdfs/Curso de Programacion Python - Arturo Montejo Raez, Salud Mari.pdf', 'page': 500, 'page_label': '501'}\n",
            "page_content='[main (root-commit) 14ed9db] Starting over.\n",
            "2 files changed, 4 insertions(+)\n",
            "create mode 100644 .gitignore\n",
            "create mode 100644 hello_git.py\n",
            " git_practice$ git status\n",
            "On branch main\n",
            "nothing to commit, working tree clean\n",
            "git_practice$\n",
            "Primero, comprobamos el estado y vemos que tenemos directorio de\n",
            "trabajo limpio . A continuación usamos el comando rm -rf .git para borrar\n",
            "el directorio .git (del .git en Windows) . Cuando comprobamos el estado\n",
            "después de borrar la carpeta .git, vemos que no es un repositorio de Git .\n",
            "Toda la información que Git usa para hacer un seguimiento de un\n",
            "repositorio está en la carpeta .git, por lo que borrarla elimina todo el\n",
            "repositorio.\n",
            "Ahora somos libres de usar git init para iniciar un nuevo repositorio .\n",
            "Al comprobar el estado vemos que hemos vuelto a la fase inicial, a la espera\n",
            "de la primera confirmación . Añadimos los archivos y confirmamos .\n",
            "Ahora, al comprobar el estado, vemos que estamos en la nueva rama main sin\n",
            "nada por confirmar .\n",
            "El uso del control de versiones requiere un poco de práctica, pero, una\n",
            "vez que empiece a utilizarlo, nunca más querrá trabajar sin él.' metadata={'source': '/content/pdfs/Curso intensivo de Python. Tercera Edición (MatthesEric) (Z-Library).pdf', 'page': 724, 'page_label': '725'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bWgxG816eUw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "aUwMb1K8QjJd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY))\n",
        "retriever=vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "DUjePOruRUla"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DCWF6RhWRToJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el prompt de contexto\n",
        "contextualize_q_system_prompt = \"\"\"Dado un historial de conversación y la última pregunta del usuario\n",
        "que podría hacer referencia al contexto de la conversación, reformule la última pregunta de forma\n",
        "independiente para que sea una consulta de búsqueda independiente. NO responda a la pregunta,\n",
        "solo reformúlela si es necesario.\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Definir el prompt para la generación de respuestas\n",
        "qa_system_prompt = \"\"\"Eres un asistente para tareas de búsqueda de respuestas.\n",
        "Usa los siguientes fragmentos de contexto para responder la pregunta.\n",
        "Si no sabes la respuesta, simplemente di que no lo sabes.\n",
        "Mantén la respuesta concisa y directa.\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "1VhudLjUTwAc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BHVa84JsVE_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=0.0)\n"
      ],
      "metadata": {
        "id": "S2vVymHwVFNY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NTHlia4_Tvts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# Función para formatear los documentos recuperados\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Construir la cadena RAG usando LCEL\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Función para hacer preguntas\n",
        "def hacer_pregunta(pregunta):\n",
        "    return rag_chain.invoke(pregunta)\n",
        "\n",
        "# Ejemplo de uso\n",
        "pregunta = \"Que es el sistema solar?\"\n",
        "respuesta = hacer_pregunta(pregunta)\n",
        "display(Markdown(respuesta))\n",
        "\n",
        "# Ejemplo de seguimiento de conversación\n",
        "def conversacion_rag():\n",
        "    historial = []\n",
        "    while True:\n",
        "        pregunta = input(\"Haz una pregunta (o escribe 'salir' para terminar): \")\n",
        "\n",
        "        if pregunta.lower() == 'salir':\n",
        "            break\n",
        "\n",
        "        respuesta"
      ],
      "metadata": {
        "id": "qwh1D2NfVpwQ",
        "outputId": "0cad18a4-de58-4a6f-98fe-609b863a9fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "No lo sé."
          },
          "metadata": {}
        }
      ]
    }
  ]
}