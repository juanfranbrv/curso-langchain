{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/juanfranbrv/curso-langchain/blob/main/RAG_1.ipynb",
      "authorship_tag": "ABX9TyMiq83rCsCMreUI4aL8kHeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/RAG_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "DEEPSEEK_API_KEY=userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "\n",
        "%pip install langchain_community -qU\n",
        "%pip install langchainhub -qU\n",
        "\n",
        "%pip install pypdf -qU\n",
        "\n",
        "\n",
        "%pip install chromadb\n",
        "%pip install langchain-chroma -qU\n",
        "\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "Nqml2kPRzN36"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método .load() se llama en la instancia de PyPDFDirectoryLoader para cargar los documentos PDF desde el directorio especificado.\n",
        "\n",
        "Este método lee todos los archivos PDF en el directorio, extrae su contenido y lo convierte en un formato que puede ser utilizado por LangChain, como una lista de objetos Document.\n",
        "\n",
        "El resultado de PyPDFDirectoryLoader(\"/content//\").load() se almacena en la variable documents.\n",
        "\n",
        "documents contendrá una lista de objetos Document, donde cada objeto representa una pagina de un archivo PDF cargado.\n",
        "\n",
        "Este cargador lee cada página de cada PDF en el directorio y crea un objeto Document por página.\n",
        "\n",
        "Es decir, si tienes 8 PDFs y cada uno tiene, por ejemplo, 100 páginas, obtendrías 8 * 100 = 800 documentos (uno por página)."
      ],
      "metadata": {
        "id": "cCwcA-mUNVCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "documents = PyPDFDirectoryLoader(\"/content/pdfs\").load()"
      ],
      "metadata": {
        "id": "oJxe2NYtNVbm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))\n",
        "print(documents[10])\n",
        "print(documents[15])\n"
      ],
      "metadata": {
        "id": "y96v6MNnnaZl",
        "outputId": "432ccaef-d657-4963-8604-fa8288055075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "page_content=' \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Appendix 1 \n",
            " \n",
            " \n",
            "Source Text DeepL Proposal Google Translate \n",
            "Proposal \n",
            "GPT4o Proposal \n",
            "    \n",
            "    \n",
            "    \n",
            " \n",
            "Poner ejemplos de “four cats” \n",
            " \n",
            " \n",
            "CARLOS Input Below \n",
            " \n",
            " \n",
            "Related Work \n",
            "Despite these advances, NMT systems still have an important Achilles’ heel: MWEs. Besides \n",
            "their quintessential problematic features such as syntactic anomaly, non-compositionality, \n",
            "diasystematic variation and ambiguity, a further challenge arises for NMT: MWEs do not always \n",
            "consist of adjacent tokens (e.g., He should take those things into account.), which seriously \n",
            "hinders their automatic detection and translation (Constant et al., 2017; Corpas Pastor, 2013; \n",
            "Ramisch & Villavicencio, 2018; Rohanian et al., 2019). To overcome the challenges that \n",
            "discontinuous MWEs still pose for even the most robust NMT systems (cf. Colson, 2019; \n",
            "Zaninello & Birch, 2020), we designed two algorithms: gApp, which is able to automatically \n",
            "convert discontinuous MWEs into their continuous form (see Hidalgo-Ternero, 2021 and 2024; \n",
            "Hidalgo-Ternero and Corpas Pastor, 2020, 2024a and 2024b; Hidalgo-Ternero and Zhou-Lian, \n",
            "2022), and Paidiom, which, besides this conversion into the continuous form, is also able to \n",
            "translemmatise MWEs, i.e., to directly convert MWEs into their target-text equivalents to \n",
            "improve NMT (...). \n",
            " ' metadata={'source': '/content/pdfs/Draft.pdf', 'page': 10, 'page_label': '11'}\n",
            "page_content='T. 382 0.02   \n",
            "Table 2: appearances of the MWEs in esTenTen18 \n",
            "  \n",
            "As shown in Table 2, these four MWEs mainly appear throughout the corpus in their continuous \n",
            "forms (89.6% for haber gato encerrado, 74.8% for ser cuatro gatos, 96.2% for dormir la mona, \n",
            "and 74.3% for ganar/costar/pagar… cuatro perras). Continuous occurrences are 3 times more \n",
            "frequent than discontinuous ones. In this context, in this study we intend to test our main \n",
            "hypothesis: that MT performance is better when MWEs appear in their canonical state (i.e., their \n",
            "continuous form). \n",
            "Against this background, The MT outputs for these different scenarios were manually assessed \n",
            "with an MT evaluation method based on directly expressed judgements (DEJ-based evaluation \n",
            "method, cf. Chatzikoumi, 2020). We decided to use human evaluation for our study given the \n",
            "obstacles that automatic metrics present for specifically evaluating the phenomenon of idiom \n",
            "translation: \n",
            "Global metrics, such as BLEU (Papineni et al., 2002), consider the full translation, and \n",
            "thus, the effects of idiom translation are overshadowed. Previous efforts on targeted \n",
            "evaluation isolated the idiom translation using word alignments (Fadaee et al., 2018) or \n",
            "word edit distance (Zaninello and Birch, 2020). These approaches measure the \n",
            "accuracy of the idiom translation but do not account for literal translation errors. Shao et \n",
            "al. (2018) proposed a method for estimating the frequency of such errors, but this \n",
            "requires the creation of language-specific handcrafted lists (i.e., blocklists) with words \n",
            "that correspond to literal translation errors. (Baziotis et al., 2022: 1) \n",
            "  \n",
            "For this reason, in our study three professional ES/EN translators, with between 4 and 10 years \n",
            "of experience, were selected as annotators to directly express judgement on the translation \n",
            "quality of the different MT outputs using a binary scale: 1 (good) or 0 (bad). After they submitted \n",
            "their judgements, final decisions on the acceptability (or not) of each specific target text were \n",
            "made on a majority basis: for instance, if 2 or 3 of the translators had judged an MT output as \n",
            "good, then this output was also finally categorised as good for our study, and vice versa. When \n",
            "judging MT quality, they were specifically instructed to focus exclusively on the phenomenon of \n",
            "MWE discontinuity for the MWEs under study, i.e., whether the ST idiom was accurately \n",
            "conveyed in the TT and, where applicable, whether the element causing the discontinuity was \n",
            "still appropriately rendered in the TT. As the ST was altered by gApp (joining the discontinuous \n",
            "MWE) and by Paidiom (partially translemmatising the ST idiom), they were also instructed to \n",
            "consider whether this alteration in the structure of the input sentence caused any additional \n",
            "error in the rest of the text that was not already present in the TT of the original (discontinuous) ' metadata={'source': '/content/pdfs/Draft.pdf', 'page': 15, 'page_label': '16'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bWgxG816eUw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "aUwMb1K8QjJd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY))\n",
        "retriever=vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "DUjePOruRUla"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DCWF6RhWRToJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el prompt de contexto\n",
        "contextualize_q_system_prompt = \"\"\"Dado un historial de conversación y la última pregunta del usuario\n",
        "que podría hacer referencia al contexto de la conversación, reformule la última pregunta de forma\n",
        "independiente para que sea una consulta de búsqueda independiente. NO responda a la pregunta,\n",
        "solo reformúlela si es necesario.\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Definir el prompt para la generación de respuestas\n",
        "qa_system_prompt = \"\"\"Eres un asistente para tareas de búsqueda de respuestas.\n",
        "Usa los siguientes fragmentos de contexto para responder la pregunta.\n",
        "Si no sabes la respuesta, simplemente di que no lo sabes.\n",
        "Mantén la respuesta concisa y directa.\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "1VhudLjUTwAc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BHVa84JsVE_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=0.0)\n"
      ],
      "metadata": {
        "id": "S2vVymHwVFNY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NTHlia4_Tvts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# Función para formatear los documentos recuperados\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Construir la cadena RAG usando LCEL\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Función para hacer preguntas\n",
        "def hacer_pregunta(pregunta):\n",
        "    return rag_chain.invoke(pregunta)\n",
        "\n",
        "# Ejemplo de uso\n",
        "pregunta = \"De que va este texto ?\"\n",
        "respuesta = hacer_pregunta(pregunta)\n",
        "display(Markdown(respuesta))\n",
        "\n",
        "# Ejemplo de seguimiento de conversación\n",
        "def conversacion_rag():\n",
        "    historial = []\n",
        "    while True:\n",
        "        pregunta = input(\"Haz una pregunta (o escribe 'salir' para terminar): \")\n",
        "\n",
        "        if pregunta.lower() == 'salir':\n",
        "            break\n",
        "\n",
        "        respuesta"
      ],
      "metadata": {
        "id": "qwh1D2NfVpwQ",
        "outputId": "19ef8bb8-a875-4490-e5de-7d4880f18e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "El texto aborda un estudio sobre expresiones idiomáticas en español, específicamente sobre su traducción y los desafíos que presentan en sistemas de traducción automática. Se mencionan ejemplos de expresiones idiomáticas y se discuten aspectos metodológicos del análisis, incluyendo la selección de expresiones y la evaluación de sistemas de traducción."
          },
          "metadata": {}
        }
      ]
    }
  ]
}