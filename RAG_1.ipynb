{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2ORBcgtafR8q2c8LvcnvL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/RAG_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. `RunnablePassthrouhg`**\n",
        "---\n",
        "\n",
        "Dentro de los Runnables, uno de los más sencillos —pero muy útil— es `RunnablePassthrough`. Este “pasa” sus datos de entrada directamente a la salida sin alterarlos. Puede parecer trivial, pero resulta práctico en situaciones en las que queremos que un eslabón de la cadena no modifique la información que recibe, sirviendo como “puente” para mantener la compatibilidad o facilidad de lectura en la cadena.\n",
        "\n",
        "Su mayor valor está en mantener la estructura de Runnables y cadenas, permitiéndonos insertar fácilmente lógica futura o puntos de inspección."
      ],
      "metadata": {
        "id": "uwodfkLmyG4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparando el entorno del cuaderno**\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google Hugging Face, Mistral, Together, Anthropic, Deepseek\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n",
        "El uso de las API de OpenAI y Anthropic es de pago. El resto son gratuitas y para usarlas basta con registrarse y generar una API Key.  \n",
        "\n",
        "En el primer cuaderno encontraras los enlaces a estos servicios y este codigo explicado\n",
        "\n"
      ],
      "metadata": {
        "id": "rTQSpmGnzOgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "DEEPSEEK_API_KEY=userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "Nqml2kPRzN36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: No hacer \"nada\"**\n",
        "---\n",
        "En su forma más sencilla, `RunnablePassthrough` recibe un input y **devuelve exactamente el mismo output**.\n",
        "\n",
        "#### **¿Por qué usar algo tan sencillo?**\n",
        "A veces en una Chain necesitas un paso que no modifique los datos, pero que sea compatible con la secuencia de Runnables. Por ejemplo, un eslabón que valide un formato o simplemente reenvíe la información a otro paso.\n",
        "\n"
      ],
      "metadata": {
        "id": "OzBWRHLwyrCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Creamos un RunnablePassthrough\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# Definimos un texto de ejemplo\n",
        "texto_entrada = \"Este texto será pasado sin cambios.\"\n",
        "\n",
        "# Ejecutamos el RunnablePassthrough\n",
        "resultado = passthrough.invoke(texto_entrada)\n",
        "\n",
        "print(\"Texto de entrada: \", texto_entrada)\n",
        "print(\"Resultado       : \", resultado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RPm9IHQzMPL",
        "outputId": "447fd521-e949-42a9-8a73-931fc0997087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto de entrada :  Este texto será pasado sin cambios.\n",
            "Resultado        :  Este texto será pasado sin cambios.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Integrar RunnablePassthrough en una cadena**\n",
        "---\n",
        "Este ejempplo puede parecer un poco forzada pero trata de mostrar como se integra facilmente un RunnablePassthrough en una cadena.\n",
        "\n",
        "Pasaremos un promt a un LLM y el resultado de este pasara a traves del RunnablePassthrough a otro prompt y de ahi a otro LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "gdYmMeqN2NNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "prompt_template1 = PromptTemplate.from_template(\"Describe y condensa el siguinte tema en una sola frase: {tema}\")\n",
        "prompt_template2 = PromptTemplate.from_template(\"Describe y condensa la siguinete frase en una sola palabra: {frase}\")\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "mayusculas = RunnableLambda(lambda x: x.content.upper())\n",
        "\n",
        "chain = prompt_template1 | llm_gpt4o_mini | RunnablePassthrough() | prompt_template2 | llm_gpt4o_mini | mayusculas\n",
        "\n",
        "resultado=chain.invoke({\"tema\": \"La inteligencia artificial\"})\n",
        "\n",
        "resultado\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MRqO4TjH2Mym",
        "outputId": "3409d9a9-5119-442c-fbc5-99ac3ba37c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AUTOMATIZACIÓN.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te has dado cuenta de la \"dificultad\" de conocer lo que esta pasando dentro de la cadena ? Cual es la frase generada por el primer modelo ?\n",
        "\n",
        "Aqui podriamos hacer uso del RunnableLambda para imprimir el resultado intermedio en consola o cualquier otra tarea de debugger\n"
      ],
      "metadata": {
        "id": "CEEAxqDN5OwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihaYmy1rgga_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "141a4972-fede-4f35-b7d0-88fc068eafae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La inteligencia artificial es la simulación de procesos de inteligencia humana por parte de sistemas computacionales, que buscan realizar tareas como el aprendizaje, razonamiento y autocorrección.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'INTELIGENCIA.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "prompt_template1 = PromptTemplate.from_template(\"Describe y condensa el siguinte tema en una sola frase: {tema}\")\n",
        "prompt_template2 = PromptTemplate.from_template(\"Describe y condensa la siguinete frase en una sola palabra: {frase}\")\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "mayusculas = RunnableLambda(lambda x: x.content.upper())\n",
        "\n",
        "def imprimir_log(x):\n",
        "  print(x.content)\n",
        "  return x  # es importante devolver lo mismo que recibimos\n",
        "\n",
        "imprimir_log_runnable = RunnableLambda(imprimir_log)\n",
        "\n",
        "chain = prompt_template1 | llm_gpt4o_mini | imprimir_log_runnable | RunnablePassthrough() | prompt_template2 | llm_gpt4o_mini | mayusculas\n",
        "\n",
        "resultado=chain.invoke({\"tema\": \"La inteligencia artificial\"})\n",
        "\n",
        "resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prepocesamiento y postprocesamiento de datos**\n",
        "---\n",
        "ara un caso de uso más realista, pensemos en un escenario donde:\n",
        "\n",
        "1. Tenemos un texto que **antes** de ser enviado a un LLM debe sufrir cierta limpieza o transformación (por ejemplo, eliminar caracteres especiales).\n",
        "2. Usamos el `RunnablePassthrough` como un punto de control que, en determinado momento, no hace nada, pero **podría** servir para inyectar validaciones o monitorear el flujo de datos.\n",
        "3. Tras obtener la respuesta del LLM, aplicamos algún postprocesamiento (usando un `RunnableLambda` o un parser).\n",
        "\n",
        "En un caso de producción podrías sustituir el passthrough por un paso de verificación, logging o incluso dejarlo como está si no necesitas modificar. La fortaleza de RunnablePassthrough es que no interfiere pero sí mantiene la estructura de un Runnable, lo cual facilita escalar el pipeline en el futuro."
      ],
      "metadata": {
        "id": "o00JaxaP_Sgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# Supongamos que tenemos una función de preprocesamiento:\n",
        "def limpiar_texto(texto):\n",
        "    # Eliminamos caracteres no alfanuméricos (muy básico)\n",
        "    import re\n",
        "    texto_limpio = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ\\s]', '', texto)\n",
        "    return texto_limpio\n",
        "\n",
        "# Convertimos esta función en un RunnableLambda:\n",
        "preprocesador = RunnableLambda(limpiar_texto)\n",
        "\n",
        "# El RunnablePassthrough lo usaremos como \"puente\"\n",
        "passthrough = RunnablePassthrough()\n",
        "\n",
        "# Para postprocesar, supongamos que queremos agregar metadata:\n",
        "def agregar_metadata(respuesta):\n",
        "    return {\n",
        "        \"respuesta\": respuesta,\n",
        "        \"metadata\": \"Procesado con LLMChain y validado\"\n",
        "    }\n",
        "\n",
        "postprocesador = RunnableLambda(agregar_metadata)\n",
        "\n",
        "\n",
        "\n",
        "pipeline_limpieza = preprocesador | passthrough | postprocesador\n",
        "    # Paso 1: Limpieza del texto\n",
        "    # Paso 2: Passthrough (punto de control)\n",
        "    # Paso 3: Postprocesamiento\n",
        "\n",
        "\n",
        "# Probemos con un texto con símbolos:\n",
        "texto_input = \"¡Hola! ¿Qué tal? LangChain 4ever #1\"\n",
        "\n",
        "resultado_avanzado = pipeline_limpieza.invoke(texto_input)\n",
        "print(\"Resultado avanzado:\", resultado_avanzado)"
      ],
      "metadata": {
        "id": "rnOgd8eO_sJ1",
        "outputId": "4a9763c1-48d3-4f13-930d-bb2a866d93ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado avanzado: {'respuesta': 'Hola Qué tal LangChain ever ', 'metadata': 'Procesado con LLMChain y validado'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. `RunnablePassthrough.assign()`**\n",
        "---\n",
        "El método `.assign()` de `RunnablePassthrough` ofrece un mecanismo para **extender el estado actual de una cadena** sin modificar sus valores existentes. Su funcionamiento clave es:\n",
        "\n",
        "1.  **Preserva** todos los datos originales del flujo\n",
        "    \n",
        "2.  **Añade nuevos campos** mediante transformaciones programáticas\n",
        "    \n",
        "3.  **Estructura automática** en diccionario para su uso en pasos posteriores\n",
        "\n",
        "\n",
        "Es un atajo práctico para inyectar o complementar datos sin tener que construir manualmente un `RunnableLambda`\n",
        "\n",
        "La entrada de `RunnablePassthrough.assign()` debe ser un diccionario"
      ],
      "metadata": {
        "id": "G5Ogku6JFCnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Añadimos dos claves al diccionario**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "C_QY0Dj0GIRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "cadena_enriquecida = RunnablePassthrough().assign(\n",
        "    longitud=lambda x: len(x['input']),     # Nuevo campo calculado\n",
        "    formato=lambda x: type(x['input'])      # Metadato adicional\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ejecución con entrada de texto\n",
        "resultado = cadena_enriquecida.invoke({\"input\": \"LangChain\"})\n",
        "print(resultado)"
      ],
      "metadata": {
        "id": "zelroyC3GWKe",
        "outputId": "eb9f26e1-88aa-4825-b839-796246fbef46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'LangChain', 'longitud': 9, 'formato': <class 'str'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo 2: Enriquecer datos\n",
        "\n",
        "Estás procesando textos para incluir una marca de tiempo simple"
      ],
      "metadata": {
        "id": "iNIq5TsKZGPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from datetime import datetime\n",
        "\n",
        "# Crear un objeto de RunnablePassthrough\n",
        "add_timestamp = RunnablePassthrough().assign(timestamp=lambda x: datetime.now().isoformat())\n",
        "\n",
        "# Procesar un texto\n",
        "output = add_timestamp.invoke({\"text\": \"Hola, mundo\"})\n",
        "print(output)"
      ],
      "metadata": {
        "id": "I48Zx4f6ZJ6k",
        "outputId": "cc8ec822-3a8e-4696-8f7f-e6879d0b3c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Hola, mundo', 'timestamp': '2025-01-21T19:45:12.244390'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "from os import X_OK\n",
        "%pip install langdetect -qU\n",
        "\n",
        "from langdetect import detect\n",
        "\n",
        "# Crear un objeto de RunnablePassthrough\n",
        "add_language_and_unique_words = RunnablePassthrough().assign(\n",
        "    language=lambda x: detect(x[\"text\"]),\n",
        "    unique_words=lambda x: len(set(x[\"text\"].split()))\n",
        ")\n",
        "\n",
        "# Procesar un texto\n",
        "output = add_language_and_unique_words.invoke({\n",
        "    \"text\": \"Hola, este es un texto en español. ¡Hola de nuevo!\"\n",
        "})\n",
        "print(output)"
      ],
      "metadata": {
        "id": "DbP8nNOKZ6VL",
        "outputId": "20de6c63-89c5-4b21-ec4d-2a87388446b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Hola, este es un texto en español. ¡Hola de nuevo!', 'language': 'es', 'unique_words': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gvcovPYWbBgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from datetime import datetime\n",
        "\n",
        "# Función para enriquecer perfil de usuario\n",
        "usuario_enriquecido = RunnablePassthrough().assign(\n",
        "    edad_categoria=lambda x: (\n",
        "        \"Niño\" if x['edad'] < 12 else\n",
        "        \"Adolescente\" if x['edad'] < 18 else\n",
        "        \"Adulto\" if x['edad'] < 60 else\n",
        "        \"Adulto Mayor\"\n",
        "    ),\n",
        "    fecha_registro=lambda x: datetime.now().strftime(\"%Y-%m-%d\")\n",
        ")\n",
        "\n",
        "# Caso de uso: Clasificación de usuario\n",
        "resultado = usuario_enriquecido.invoke({\n",
        "    \"nombre\": \"Carlos Mendez\",\n",
        "    \"edad\": 25\n",
        "})\n",
        "print(resultado)"
      ],
      "metadata": {
        "id": "2nvgRZjCbCG6",
        "outputId": "7b89be0e-8057-45b0-e92c-8c2feed860df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nombre': 'Carlos Mendez', 'edad': 25, 'edad_categoria': 'Adulto', 'fecha_registro': '2025-01-21'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pkq-RY8WdLOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install textblob -qU\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    # Eliminar caracteres especiales y convertir a minúsculas\n",
        "    texto_limpio = re.sub(r'[^a-zA-Z\\s]', '', texto.lower())\n",
        "    return texto_limpio\n",
        "\n",
        "analisis_texto = RunnablePassthrough().assign(\n",
        "    texto_limpio=lambda x: limpiar_texto(x['texto']),\n",
        "    longitud=lambda x: len(x['texto'].split()),\n",
        "    sentimiento=lambda x: TextBlob(x['texto']).sentiment.polarity,\n",
        "    sentimiento_categoria=lambda x: (\n",
        "        \"Positivo\" if TextBlob(x['texto']).sentiment.polarity > 0.2 else\n",
        "        \"Negativo\" if TextBlob(x['texto']).sentiment.polarity < -0.2 else\n",
        "        \"Neutral\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Caso de uso: Análisis de reseña de producto\n",
        "resultado = analisis_texto.invoke({\n",
        "    \"texto\": \"This product is amazing! Highly recommended. 5 stars ⭐\"})\n",
        "print(resultado)"
      ],
      "metadata": {
        "id": "f8iOBqgfdLnz",
        "outputId": "fe0e6ba7-c6b5-4f24-8ce6-b16fdbb9dbf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'texto': 'This product is amazing! Highly recommended. 5 stars ⭐', 'texto_limpio': 'this product is amazing highly recommended  stars ', 'longitud': 9, 'sentimiento': 0.45500000000000007, 'sentimiento_categoria': 'Positivo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desafio:\n",
        "\n",
        "Eres un desarrollador encargado de diseñar un sistema de análisis para procesar datos de usuarios en un flujo de trabajo. Cada entrada contiene el nombre del usuario y un mensaje escrito por él. Tu tarea es implementar una solución utilizando el método `assign()` de `RunnablePassthrough` que realice las siguientes operaciones:\n",
        "\n",
        "1.  **Determinar la longitud del mensaje del usuario:** Clasificar como:\n",
        "    \n",
        "    -   `\"corto\"` si tiene menos de 20 caracteres.\n",
        "    -   `\"mediano\"` si tiene entre 20 y 50 caracteres.\n",
        "    -   `\"largo\"` si tiene más de 50 caracteres.\n",
        "2.  **Contar las palabras únicas del mensaje.**\n",
        "    \n",
        "3.  **Generar un identificador único (ID) para cada usuario:** Usa el formato `user_<número>` donde el número sea un contador incremental comenzando desde 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "yaqJSTwleWIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contador = 0\n",
        "\n",
        "def incrementar_contador(_:int) -> str:\n",
        "    global contador\n",
        "    contador += 1\n",
        "    return f\"user_{contador}\"\n",
        "\n",
        "datos = [\n",
        "    {\"usuario\": \"Ana\", \"mensaje\": \"Hola, ¿cómo estás?\"},\n",
        "    {\"usuario\": \"Luis\", \"mensaje\": \"Este es un mensaje de prueba para verificar la longitud.\"},\n",
        "    {\"usuario\": \"Carla\", \"mensaje\": \"¡Wow! Esto es impresionante, realmente útil.\"}\n",
        "]\n",
        "\n",
        "clasificador_mensaje = RunnablePassthrough().assign(\n",
        "    longitud=lambda x: (\n",
        "        \"corto\" if len(x[\"mensaje\"]) < 20 else\n",
        "        \"mediano\" if 20 <= len(x[\"mensaje\"]) <= 50 else\n",
        "        \"largo\"\n",
        "    ),\n",
        "    palabras_unicas=lambda x: len(set(x[\"mensaje\"].split())),\n",
        "    id=lambda x:   (incrementar_contador(x))\n",
        ")\n",
        "\n",
        "for dato in datos:\n",
        "    resultado = clasificador_mensaje.invoke(dato)\n",
        "    print(resultado)"
      ],
      "metadata": {
        "id": "mLN_G7RZeVEn",
        "outputId": "048c426b-c204-4507-fa0c-94ac44fc439c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'usuario': 'Ana', 'mensaje': 'Hola, ¿cómo estás?', 'longitud': 'corto', 'palabras_unicas': 3, 'id': 'user_1'}\n",
            "{'usuario': 'Luis', 'mensaje': 'Este es un mensaje de prueba para verificar la longitud.', 'longitud': 'largo', 'palabras_unicas': 10, 'id': 'user_2'}\n",
            "{'usuario': 'Carla', 'mensaje': '¡Wow! Esto es impresionante, realmente útil.', 'longitud': 'mediano', 'palabras_unicas': 6, 'id': 'user_3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bWgxG816eUw0"
      }
    }
  ]
}