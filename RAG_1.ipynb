{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/juanfranbrv/curso-langchain/blob/main/RAG_1.ipynb",
      "authorship_tag": "ABX9TyMZhPCCCUTABjUmebMZ1aDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/RAG_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "DEEPSEEK_API_KEY=userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "\n",
        "%pip install langchain_community -qU\n",
        "%pip install langchainhub -qU\n",
        "%pip install langchain-chromadb -qU\n",
        "%pip install pypdf -qU\n",
        "\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "Nqml2kPRzN36"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cCwcA-mUNVCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "documents = PyPDFDirectoryLoader(\"/content/data/\").load()"
      ],
      "metadata": {
        "id": "oJxe2NYtNVbm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bWgxG816eUw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "aUwMb1K8QjJd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY))\n",
        "retriever=vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "DUjePOruRUla"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DCWF6RhWRToJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el prompt de contexto\n",
        "contextualize_q_system_prompt = \"\"\"Dado un historial de conversación y la última pregunta del usuario\n",
        "que podría hacer referencia al contexto de la conversación, reformule la última pregunta de forma\n",
        "independiente para que sea una consulta de búsqueda independiente. NO responda a la pregunta,\n",
        "solo reformúlela si es necesario.\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Definir el prompt para la generación de respuestas\n",
        "qa_system_prompt = \"\"\"Eres un asistente para tareas de búsqueda de respuestas.\n",
        "Usa los siguientes fragmentos de contexto para responder la pregunta.\n",
        "Si no sabes la respuesta, simplemente di que no lo sabes.\n",
        "Mantén la respuesta concisa y directa.\n",
        "\n",
        "{context}\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "1VhudLjUTwAc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BHVa84JsVE_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY, temperature=0.7)\n"
      ],
      "metadata": {
        "id": "S2vVymHwVFNY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NTHlia4_Tvts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# Función para formatear los documentos recuperados\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Construir la cadena RAG usando LCEL\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Función para hacer preguntas\n",
        "def hacer_pregunta(pregunta):\n",
        "    return rag_chain.invoke(pregunta)\n",
        "\n",
        "# Ejemplo de uso\n",
        "pregunta = \"Quien es Gerard Bentley ?\"\n",
        "respuesta = hacer_pregunta(pregunta)\n",
        "display(Markdown(respuesta))\n",
        "\n",
        "# Ejemplo de seguimiento de conversación\n",
        "def conversacion_rag():\n",
        "    historial = []\n",
        "    while True:\n",
        "        pregunta = input(\"Haz una pregunta (o escribe 'salir' para terminar): \")\n",
        "\n",
        "        if pregunta.lower() == 'salir':\n",
        "            break\n",
        "\n",
        "        respuesta"
      ],
      "metadata": {
        "id": "qwh1D2NfVpwQ",
        "outputId": "8f414907-2e6c-491e-a140-213e2e4573c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Gerard Bentley es un creador de Streamlit y ingeniero de software que actualmente trabaja en servicios web backend en una startup llamada Sensible Weather, que ofrece garantías de seguros contra mal tiempo."
          },
          "metadata": {}
        }
      ]
    }
  ]
}