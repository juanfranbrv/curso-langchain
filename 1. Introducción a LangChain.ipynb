{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/1.%20Introducci%C3%B3n%20a%20LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. ¬øQu√© es LangChain y por qu√© es relevante en el mundo de los LLMs?**\n",
        "---\n",
        "\n",
        "## **1.1. LangChain en pocas palabras**\n",
        "---\n",
        "\n",
        "**LangChain** es un framework dise√±ado para facilitar la construcci√≥n de aplicaciones basadas en _Large Language Models (LLMs)_. Proporciona herramientas y abstracciones que permiten **conectar** distintos modelos e integrarlos en _pipelines_ o _flujos de trabajo_ (chains) m√°s complejos. Con LangChain podemos, por ejemplo:\n",
        "\n",
        "- **Construir** aplicaciones que combinen m√∫ltiples llamadas a modelos de lenguaje.\n",
        "- **Enriquecer** las respuestas con contexto proveniente de bases de datos o documentos.\n",
        "- **Implementar memorias** que recuerden datos de interacciones previas.\n",
        "\n",
        "Su objetivo principal es **simplificar** el desarrollo de aplicaciones que aprovechan modelos de lenguaje, a la vez que proporciona **flexibilidad** para integrar servicios de diferentes proveedores y orquestar diferentes estrategias de prompting.\n",
        "\n",
        "Se habla de Langchain como un **software de orquestaci√≥n** ya que su fortaleza reside en la capacidad de conectar y coordinar diversos componentes, como modelos de lenguaje, prompts, bases de datos y herramientas externas, permitiendo definir flujos de trabajo complejos (cadenas o chains) donde la salida de un componente alimenta la entrada del siguiente. Esta orquestaci√≥n facilita la construcci√≥n de aplicaciones sofisticadas.  \n",
        "\n",
        "![https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png?token=GHSAT0AAAAAAC5HTYEE4WSTQKG224SMQHZMZ4FOHNA](https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png?token=GHSAT0AAAAAAC5HTYEE4WSTQKG224SMQHZMZ4FOHNA)\n",
        "\n",
        "## **1.2. Importancia y casos de uso de los LLMs**\n",
        "---\n",
        "\n",
        "Los _Large Language Models (LLMs)_ son redes neuronales especializadas en procesar y generar lenguaje natural. Modelos como GPT-3, GPT-4 o Llama 2 han demostrado ser muy eficaces para:\n",
        "\n",
        "- **Generaci√≥n de texto**: redacci√≥n de art√≠culos, guiones de v√≠deo, textos creativos, etc.\n",
        "- **Traducciones y resumidos**: convertir textos entre diferentes idiomas o generar res√∫menes de documentos extensos.\n",
        "- **Asistentes conversacionales**: chatbots que mantienen el contexto de la conversaci√≥n y pueden llevar a cabo tareas o responder preguntas.\n",
        "- **An√°lisis y clasificaci√≥n** de textos: etiquetar o extraer informaci√≥n relevante de grandes vol√∫menes de texto.\n",
        "- **Soporte en programaci√≥n**: autocompletar c√≥digo o explicar algoritmos.\n",
        "\n",
        "Dada esta variedad de posibilidades, integrar LLMs en productos o proyectos se ha convertido en una **prioridad** para muchas empresas y desarrolladores. Y es justo ah√≠ donde **LangChain** ofrece un entorno de desarrollo potente, √°gil y modular."
      ],
      "metadata": {
        "id": "Ef9QGtxV_AW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Configuracion del entorno del cuaderno**\n",
        "---"
      ],
      "metadata": {
        "id": "Usx9MmcgPiwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Obtener claves API y incorporarlas al cuaderno**\n",
        "---\n",
        "Para que la mayor√≠a de los ejemplos de este cuaderno funcionen, es necesario contar con las claves API de los servicios que vamos a usar. Estas claves funcionan como ‚Äúcontrase√±as‚Äù o ‚Äútokens‚Äù que nos permiten autenticar nuestro c√≥digo en cada servicio. Si no tienes una cuenta en cada plataforma, deber√°s crearla primero y luego generar tu clave API en el panel correspondiente.  \n",
        "Apuntalas en un fichero a medida que las obtengas. Las necesitaras en el proximo paso.\n",
        "\n",
        "GROQ: https://console.groq.com/keys  \n",
        "GOOGLE AI STUDIO: https://aistudio.google.com/apikey  \n",
        "HUFFING FACE: https://huggingface.co/settings/tokens  \n",
        "OPENAI: https://platform.openai.com/api-keys  (**de pago!**)\n",
        "\n",
        "No es necesario disponer de todas. Si decides por ejemplo no usar la API de OPENAI (que es de pago) simplemente ignorala en este momento. Y  mas adelante comenta en el codigo todas las referencias a OPENAI\n",
        "\n",
        "Ve a la barra lateral y haz clic en icono de una llave (\"Secretos\".)\n",
        "Haz clic en \"A√±adir secreto nuevo\".\n",
        "En \"Nombre\", escribe el nombre de tu secreto.  \n",
        "\n",
        "üëâüèª Los nombres importan para que funcione el codigo que sigue. Debes usar los siguientes:\n",
        "\n",
        "OPENAI_API_KEY  \n",
        "GROQ_API_KEY  \n",
        "GOOGLE_API_KEY  \n",
        "HUGGINGFACEHUB_API_TOKEN  \n",
        "\n",
        "En \"Value\", pega el valor de la clave API.\n",
        "\n",
        "![Configuraci√≥n de Secretos en Colab](https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/secretos.png?token=GHSAT0AAAAAAC5HTYEEDLGANEQ7BARWGCGEZ4FPBXA)"
      ],
      "metadata": {
        "id": "YjL7sgN5e8wl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q-cZ5A6YUXD"
      },
      "source": [
        "##**2.2. Carga Segura de Claves API**\n",
        "---\n",
        "\n",
        "Este c√≥digo se utiliza en Google Colab para obtener claves de API (tokens de acceso) almacenadas de forma segura en la secci√≥n \"Secretos\" de Colab y se asignan a variables para su posterior uso.\n",
        "\n",
        "Estas claves son necesarias para acceder a servicios externos, como OpenAI, Groq, Google o Hugging Face, desde tu notebook.  \n",
        "\n",
        "Este proceso evita la necesidad de codificar las claves directamente en el c√≥digo, lo que representa una mejor pr√°ctica de seguridad.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQMk_1lnonK3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi2sqEbTVGQg"
      },
      "source": [
        "## **2.3. Instalaci√≥n de las bibliotecas necesarias**\n",
        "---\n",
        "\n",
        " Instala las bibliotecas principales (langchain) y sus integraciones con diversos modelos de lenguaje (OpenAI, Groq, Google Gemini).  \n",
        "\n",
        " La opci√≥n -qU asegura una instalaci√≥n silenciosa y que tengamos las √∫ltimas versiones disponibles.\n",
        "\n",
        " %%capture... captura los mensajes de instalaci√≥n para evitar que salgan a pantalla (salvo si hay errores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mc8S-4mgj8VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643352b5-f944-4ec5-d529-84dbc83f16b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/54.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Instalamos el paquete principal de Langchain\n",
        "%pip install langchain -qU\n",
        "\n",
        "# Instalamos sus integraiones con otros servicios\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9keoznMXNTx"
      },
      "source": [
        "## **2.4. Importaci√≥n de las clases necesarias**\n",
        "---\n",
        "\n",
        "Este bloque de c√≥digo importa las clases necesarias de langchain para crear y utilizar los modelos de lenguaje de OpenAI, Groq y Google.  \n",
        "\n",
        "Estas clases son un envoltorio (*wrapper*) para las APIs de sus respectivos propietarios (OpenAI, Groq y Google)\n",
        "\n",
        "Tambien las clases que proporciona Langchain para la creaci√≥n de mensajes de usuario y sistema, que usamos para contruir un mensaje (*prompt*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_60Sdpp0n8Pv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. El momento de la verdad: Invocando nuestros LLMs**\n",
        "---\n",
        "Una vez que hemos instalado los componentes requeridos, importado las librer√≠as necesarias y configurado las claves API, estamos listos para la acci√≥n. Realicemos ahora nuestra primera llamada, aunque sea una sencilla, a un modelo de lenguaje (LLM)\n"
      ],
      "metadata": {
        "id": "XYnbLkyihy2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a8EkWezHj1S_"
      },
      "outputs": [],
      "source": [
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "modelo3 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=GOOGLE_API_KEY)\n",
        "modelo4 = HuggingFaceEndpoint(repo_id=\"Qwen/Qwen2.5-72B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
        "\n",
        "prompt = [\n",
        "            SystemMessage(\"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "            HumanMessage(\"Estar en la edad del pavo\")\n",
        "           ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos el metodo .invoke() de los modelos instanciados para llamar a los LLM pasandoles el mensaje."
      ],
      "metadata": {
        "id": "Qjc6oLbWQcAd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I0BBP2-zaQml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d55700-8e94-4f77-a8db-3422a2e01d61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='To be at a awkward age.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 52, 'total_tokens': 59, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-78e972e1-ab4d-490d-8d29-a748a26d67bb-0', usage_metadata={'input_tokens': 52, 'output_tokens': 7, 'total_tokens': 59, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "modelo1.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfvmxobOaQcQ"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upGJmBNPaUHg"
      },
      "outputs": [],
      "source": [
        "modelo3.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo4.invoke(prompt)"
      ],
      "metadata": {
        "id": "hzRzGdQnZBSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Ys3ydEcAiS"
      },
      "source": [
        "###**Otras formas de contruir el prompt**\n",
        "\n",
        "LangChain tambi√©n admite entradas de modelos de chat mediante cadenas o formato OpenAI  \n",
        "Son equivalentes los siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deh2BAhtcDTT"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(\"Traduce a un frase equivalente (idiom) en ingl√©s: M√°s vale p√°jaro en mano que ciento volando\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1uA1DVtpdMlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d280de-9f6c-497d-b611-e72b6263e8ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='A bird in the hand is worth two in the bush.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 85, 'total_tokens': 98, 'completion_time': 0.047272727, 'prompt_time': 0.01323484, 'queue_time': 0.107212177, 'total_time': 0.060507567}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_4196e754db', 'finish_reason': 'stop', 'logprobs': None}, id='run-cb4dc4a5-494b-413b-979e-3b4e433c9f5d-0', usage_metadata={'input_tokens': 85, 'output_tokens': 13, 'total_tokens': 98})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "modelo2.invoke([\n",
        "    {\"role\":\"system\", \"content\": \"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"},\n",
        "    {\"role\":\"user\", \"content\": \"M√°s vale p√°jaro en mano que ciento volando\"}\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_GXFAVcSFlN",
        "outputId": "b458dacf-295e-4c19-be79-abae3557525c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='A bird in the hand is worth two in the bush.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 85, 'total_tokens': 98, 'completion_time': 0.048048413, 'prompt_time': 0.016265944, 'queue_time': 0.0393188, 'total_time': 0.064314357}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_4e32347616', 'finish_reason': 'stop', 'logprobs': None}, id='run-de4a5ffc-7802-439c-8f9c-48f365d40cc8-0', usage_metadata={'input_tokens': 85, 'output_tokens': 13, 'total_tokens': 98})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "modelo2.invoke([\n",
        "    (\"system\", \"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    (\"user\", \"M√°s vale p√°jaro en mano que ciento volando\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgeyJ8Gfham"
      },
      "source": [
        "###**Respuesta del LLM**\n",
        "\n",
        "Observa que la respuesta del metodo `.invoke()` devuelve mucha informacion util o que podria ser util en aplicaciones mas complejas.  \n",
        "\n",
        "La respuesta es un **objeto de la clase `AIMessage`** (definida dentro de LangChain). Dicho objeto act√∫a de forma similar a una ‚Äúestructura de datos‚Äù que contiene varios atributos o _campos_, entre ellos:\n",
        "\n",
        "- `content`: el texto generado por el modelo.\n",
        "- `additional_kwargs`: un diccionario con posibles argumentos adicionales.\n",
        "- `response_metadata`: un diccionario donde viene, entre otras cosas, el nombre del modelo (`model_name`).\n",
        "- `usage_metadata`: estad√≠sticas del uso de tokens.\n",
        "- `id`: un identificador √∫nico de la ejecuci√≥n.una clase que contiene informaci√≥n sobre la respuesta de un modelo de lenguaje.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PzlKEeGndxP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279b186f-aabf-41cb-bb54-2829c66da09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenido de la respuesta (content):\n",
            " To have a bee in one's bonnet.\n",
            "========\n",
            "Metadatos de la respuesta (response_metadata): \n",
            " {'token_usage': {'completion_tokens': 11, 'prompt_tokens': 83, 'total_tokens': 94, 'completion_time': 0.04, 'prompt_time': 0.014568136, 'queue_time': 0.358517083, 'total_time': 0.054568136}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_4e32347616', 'finish_reason': 'stop', 'logprobs': None}\n",
            "========\n",
            "Tokens utilizados (usage_metadata): \n",
            " {'input_tokens': 83, 'output_tokens': 11, 'total_tokens': 94}\n",
            "========\n",
            "Identificador de la respuesta (id): run-96a2f28a-5b43-45eb-8e75-80959e314674-0\n",
            "========\n",
            "Uso de tokens detallado(response_metadata['token_usage']:\n",
            " {'completion_tokens': 11, 'prompt_tokens': 83, 'total_tokens': 94, 'completion_time': 0.04, 'prompt_time': 0.014568136, 'queue_time': 0.358517083, 'total_time': 0.054568136}\n"
          ]
        }
      ],
      "source": [
        "ai_message = modelo2.invoke([\n",
        "    SystemMessage(\"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    HumanMessage(\"Tener la mosca detr√°s de la oreja\")\n",
        "            ])\n",
        "\n",
        "print(\"Contenido de la respuesta (content):\\n\", ai_message.content)\n",
        "print(\"========\")\n",
        "print(\"Metadatos de la respuesta (response_metadata): \\n\", ai_message.response_metadata)\n",
        "print(\"========\")\n",
        "print(\"Tokens utilizados (usage_metadata): \\n\", ai_message.usage_metadata)\n",
        "print(\"========\")\n",
        "print(\"Identificador de la respuesta (id):\", ai_message.id)\n",
        "print(\"========\")\n",
        "print(\"Uso de tokens detallado(response_metadata['token_usage']:\\n\", ai_message.response_metadata['token_usage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GGV06vlIsK"
      },
      "source": [
        "# **4. Streaming**\n",
        "---\n",
        "En la interacci√≥n con modelos de lenguaje como GPT, el streaming ofrece una alternativa a la espera de la respuesta completa. En lugar de una entrega √∫nica, el modelo emite la respuesta progresivamente, token por token (fragmentos de palabras o caracteres). Esta transmisi√≥n gradual mejora la experiencia del usuario, quien percibe una respuesta m√°s r√°pida y natural al ver el texto aparecer en tiempo real. Adem√°s, el streaming permite iniciar el procesamiento de la respuesta en tiempo real, sin esperar a que el modelo termine de generar toda la salida. En esencia, el streaming transforma la interacci√≥n con el LLM de una espera prolongada a una recepci√≥n continua y din√°mica.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este codigo usamos el metodo `.stream()` en lugar de `.invoke()` que devuelve un genera un flujo de tokens con la respuestas y que podemos iterar con un bucle."
      ],
      "metadata": {
        "id": "KPyImbrOcQO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5n1KnKDkt94"
      },
      "outputs": [],
      "source": [
        "for token in modelo2.stream(\"Explica los verbos modales en ingl√©s.\"):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Runnables**\n",
        "---\n",
        "En Langchain, piensa en un Runnable como un bloque de construcci√≥n fundamental: cualquier componente que puede realizar una tarea. Modelos de lenguaje como ChatGroq o OpenAI son excelentes ejemplos, ya que pueden generar texto o hacer cosas espec√≠ficas cuando les das una entrada. Estos bloques son cruciales porque te permiten unir y combinar diferentes tareas para crear flujos de trabajo. Los Runnables tienen funciones clave como .invoke() (para obtener el resultado completo de una vez) y .stream() (para recibir la respuesta poco a poco, ideal para textos largos o ver la respuesta en tiempo real).  \n",
        "\n",
        "Profundizaremos en los Runnables m√°s adelante. Por ahora, es fundamental saber que una amplia gama de componentes en Langchain se implementan como Runnables o pueden ser adaptados para serlo. Esto incluye elementos centrales como los modelos de lenguaje (LLMs) y los parsers de salida (OutputParsers). Si bien los DataLoaders en s√≠ mismos no son directamente Runnables, a menudo se utilizan para cargar datos que luego se procesan dentro de Runnables. Incluso las funciones Python pueden integrarse en cadenas LCEL al ser envueltas como Runnables"
      ],
      "metadata": {
        "id": "EvQ18A9ub82M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Referencias**\n",
        "---\n",
        "\n",
        "1. https://www.langchain.com/  \n",
        "2. https://python.langchain.com/docs/introduction/"
      ],
      "metadata": {
        "id": "RwKXFzC_RbBG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}