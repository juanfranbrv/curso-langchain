{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/1.%20Introducci%C3%B3n%20a%20LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. ¬øQu√© es LangChain y por qu√© es relevante en el mundo de los LLMs?**\n",
        "---\n",
        "\n",
        "## 1.1. LangChain en pocas palabras\n",
        "\n",
        "**LangChain** es un framework dise√±ado para facilitar la construcci√≥n de aplicaciones basadas en _Large Language Models (LLMs)_. Proporciona herramientas y abstracciones que permiten **conectar** distintos modelos e integrarlos en _pipelines_ o _flujos de trabajo_ (chains) m√°s complejos. Con LangChain podemos, por ejemplo:\n",
        "\n",
        "- **Construir** aplicaciones que combinen m√∫ltiples llamadas a modelos de lenguaje.\n",
        "- **Enriquecer** las respuestas con contexto proveniente de bases de datos o documentos.\n",
        "- **Implementar memorias** que recuerden datos de interacciones previas.\n",
        "\n",
        "Su objetivo principal es **simplificar** el desarrollo de aplicaciones que aprovechan modelos de lenguaje, a la vez que proporciona **flexibilidad** para integrar servicios de diferentes proveedores y orquestar diferentes estrategias de prompting.\n",
        "\n",
        "## 1.2. Importancia y casos de uso de las LLMs\n",
        "\n",
        "Las _Large Language Models (LLMs)_ son redes neuronales especializadas en procesar y generar lenguaje natural. Modelos como GPT-3, GPT-4 o Llama 2 han demostrado ser muy eficaces para:\n",
        "\n",
        "- **Generaci√≥n de texto**: redacci√≥n de art√≠culos, guiones de v√≠deo, textos creativos, etc.\n",
        "- **Traducciones y resumidos**: convertir textos entre diferentes idiomas o generar res√∫menes de documentos extensos.\n",
        "- **Asistentes conversacionales**: chatbots que mantienen el contexto de la conversaci√≥n y pueden llevar a cabo tareas o responder preguntas.\n",
        "- **An√°lisis y clasificaci√≥n** de textos: etiquetar o extraer informaci√≥n relevante de grandes vol√∫menes de texto.\n",
        "- **Soporte en programaci√≥n**: autocompletar c√≥digo o explicar algoritmos.\n",
        "\n",
        "Dada esta variedad de posibilidades, integrar LLMs en productos o proyectos se ha convertido en una **prioridad** para muchas empresas y desarrolladores. Y es justo ah√≠ donde **LangChain** ofrece un entorno de desarrollo potente, √°gil y modular.\n",
        "\n",
        "Tambien se habla de Langchain como un **software de orquestaci√≥n** ya que su fortaleza reside en la capacidad de conectar y coordinar diversos componentes, como modelos de lenguaje, prompts, bases de datos y herramientas externas, permitiendo definir flujos de trabajo complejos (cadenas o chains) donde la salida de un componente alimenta la entrada del siguiente. Esta orquestaci√≥n facilita la construcci√≥n de aplicaciones sofisticadas.\n",
        "![https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png?token=GHSAT0AAAAAAC5HTYEE4WSTQKG224SMQHZMZ4FOHNA](https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png?token=GHSAT0AAAAAAC5HTYEE4WSTQKG224SMQHZMZ4FOHNA)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ef9QGtxV_AW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Configuracion del entorno del cuaderno**\n",
        "---"
      ],
      "metadata": {
        "id": "Usx9MmcgPiwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Obtener claves API y incorporarlas al cuaderno**\n",
        "---\n",
        "Para que la mayor√≠a de los ejemplos de este cuaderno funcionen, es necesario contar con las claves API de los servicios que vamos a usar. Estas claves funcionan como ‚Äúcontrase√±as‚Äù o ‚Äútokens‚Äù que nos permiten autenticar nuestro c√≥digo en cada servicio. Si no tienes una cuenta en cada plataforma, deber√°s crearla primero y luego generar tu clave API en el panel correspondiente.  \n",
        "Apuntalas en un fichero a medida que las obtengas. Las necesitaras en el proximo paso.\n",
        "\n",
        "GROQ: https://console.groq.com/keys  \n",
        "GOOGLE AI STUDIO: https://aistudio.google.com/apikey  \n",
        "HUFFING FACE: https://huggingface.co/settings/tokens  \n",
        "OPENAI: https://platform.openai.com/api-keys  (**de pago!**)\n",
        "\n",
        "No es necesario disponer de todas. Si decides por ejemplo no usar la API de OPENAI (que es de pago) simplemente ignorala en este momento. Y  mas adelante comenta en el codigo todas las referencias a OPENAI\n",
        "\n",
        "Ve a la barra lateral y haz clic en icono de una llave (\"Secretos\".)\n",
        "Haz clic en \"A√±adir secreto nuevo\".\n",
        "En \"Nombre\", escribe el nombre de tu secreto. üü° Los nombres importan para que funcione el codigo que sigue. Debes usar los siguientes:\n",
        "\n",
        "OPENAI_API_KEY\n",
        "GROQ_API_KEY\n",
        "GOOGLE_API_KEY\n",
        "HUGGINGFACEHUB_API_TOKEN\n",
        "\n",
        "En \"Value\", pega el valor de la clave API.\n",
        "\n",
        "![Configuraci√≥n de Secretos en Colab](https://github.com/juanfranbrv/curso-langchain/blob/main/images/secretos.png?raw=true)"
      ],
      "metadata": {
        "id": "YjL7sgN5e8wl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q-cZ5A6YUXD"
      },
      "source": [
        "##**2.2. Carga Segura de Claves API**\n",
        "---\n",
        "\n",
        "Este c√≥digo se utiliza en Google Colab para obtener claves de API (tokens de acceso) almacenadas de forma segura en la secci√≥n \"Secretos\" de Colab y se asignan a variables para su posterior uso.\n",
        "\n",
        "Estas claves son necesarias para acceder a servicios externos, como OpenAI, Groq, Google o Hugging Face, desde tu notebook.  \n",
        "\n",
        "Este proceso evita la necesidad de codificar las claves directamente en el c√≥digo, lo que representa una mejor pr√°ctica de seguridad.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQMk_1lnonK3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi2sqEbTVGQg"
      },
      "source": [
        "## **2.3. Instalaci√≥n de las bibliotecas necesarias**\n",
        "---\n",
        "\n",
        " Instala las bibliotecas principales (langchain) y sus integraciones con diversos modelos de lenguaje (OpenAI, Groq, Google Gemini).  \n",
        "\n",
        " La opci√≥n -qU asegura una instalaci√≥n silenciosa y que tengamos las √∫ltimas versiones disponibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc8S-4mgj8VS"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -qU\n",
        "\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9keoznMXNTx"
      },
      "source": [
        "## **2.4. Importaci√≥n de las clases necesarias**\n",
        "---\n",
        "\n",
        "Este bloque de c√≥digo importa las clases necesarias de langchain para crear y utilizar los modelos de lenguaje de OpenAI, Groq y Google.  \n",
        "\n",
        "Estas clases son un envoltorio (*wrapper*) para las APIs de sus respectivos propietarios (OpenAI, Groq y Google)\n",
        "\n",
        "Tambien las clases que proporciona Langchain para la creaci√≥n de mensajes de usuario y sistema, que usamos para contruir un mensaje (*prompt*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_60Sdpp0n8Pv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. El momento de la verdad: Invocando nuestros LLMs**\n",
        "---\n",
        "Una vez que hemos instalado los componentes requeridos, importado las librer√≠as necesarias y configurado las claves API, estamos listos para la acci√≥n. Realicemos ahora nuestra primera llamada, aunque sea una sencilla, a un modelo de lenguaje (LLM)\n"
      ],
      "metadata": {
        "id": "XYnbLkyihy2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8EkWezHj1S_"
      },
      "outputs": [],
      "source": [
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "modelo3 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=GOOGLE_API_KEY)\n",
        "modelo4 = HuggingFaceEndpoint(repo_id=\"Qwen/Qwen2.5-72B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
        "\n",
        "prompt = [\n",
        "            SystemMessage(\"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "            HumanMessage(\"Estar en la edad del pavo\")\n",
        "           ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos el metodo .invoke() de los modelos instanciados para llamar a los LLM pasandoles el mensaje."
      ],
      "metadata": {
        "id": "Qjc6oLbWQcAd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0BBP2-zaQml"
      },
      "outputs": [],
      "source": [
        "modelo1.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfvmxobOaQcQ"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upGJmBNPaUHg"
      },
      "outputs": [],
      "source": [
        "modelo3.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo4.invoke(prompt)"
      ],
      "metadata": {
        "id": "hzRzGdQnZBSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Ys3ydEcAiS"
      },
      "source": [
        "###**Otras formas de contruir el prompt**\n",
        "\n",
        "LangChain tambi√©n admite entradas de modelos de chat mediante cadenas o formato OpenAI  \n",
        "Son equivalentes los siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deh2BAhtcDTT"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(\"Traduce a un frase equivalente (idiom) en ingl√©s: M√°s vale p√°jaro en mano que ciento volando\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uA1DVtpdMlk"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke([\n",
        "    {\"role\":\"system\", \"content\": \"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"},\n",
        "    {\"role\":\"user\", \"content\": \"M√°s vale p√°jaro en mano que ciento volando\"}\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_GXFAVcSFlN"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke([\n",
        "    (\"system\", \"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    (\"user\", \"M√°s vale p√°jaro en mano que ciento volando\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgeyJ8Gfham"
      },
      "source": [
        "###**Respuesta del LLM**\n",
        "\n",
        "Observa que la respuesta del metodo `.invoke()` devuelve mucha informacion util o que podria ser util en aplicaciones mas complejas.  \n",
        "\n",
        "La respuesta es un **objeto de la clase `AIMessage`** (definida dentro de LangChain). Dicho objeto act√∫a de forma similar a una ‚Äúestructura de datos‚Äù que contiene varios atributos o _campos_, entre ellos:\n",
        "\n",
        "- `content`: el texto generado por el modelo.\n",
        "- `additional_kwargs`: un diccionario con posibles argumentos adicionales.\n",
        "- `response_metadata`: un diccionario donde viene, entre otras cosas, el nombre del modelo (`model_name`).\n",
        "- `usage_metadata`: estad√≠sticas del uso de tokens.\n",
        "- `id`: un identificador √∫nico de la ejecuci√≥n.una clase que contiene informaci√≥n sobre la respuesta de un modelo de lenguaje.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzlKEeGndxP0"
      },
      "outputs": [],
      "source": [
        "ai_message = modelo2.invoke([\n",
        "    SystemMessage(\"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    HumanMessage(\"Tener la mosca detr√°s de la oreja\")\n",
        "            ])\n",
        "\n",
        "print(\"Contenido de la respuesta (content):\\n\", ai_message.content)\n",
        "print(\"========\")\n",
        "print(\"Metadatos de la respuesta (response_metadata): \\n\", ai_message.response_metadata)\n",
        "print(\"========\")\n",
        "print(\"Tokens utilizados (usage_metadata): \\n\", ai_message.usage_metadata)\n",
        "print(\"========\")\n",
        "print(\"Identificador de la respuesta (id):\", ai_message.id)\n",
        "print(\"========\")\n",
        "print(\"Uso de tokens detallado(response_metadata['token_usage']:\\n\", ai_message.response_metadata['token_usage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GGV06vlIsK"
      },
      "source": [
        "# **4. Streaming**\n",
        "---\n",
        "üî∫Redactar/reorgnizar todo este apartado\n",
        "\n",
        "**Conceptos Clave:**\n",
        "\n",
        "- **Modelos de Chat \"Ejecutables\":**\n",
        "    \n",
        "    - Imagina que los modelos de chat (como GPT, Gemini, etc.) son como funciones que puedes \"ejecutar\" o \"llamar\".\n",
        "        \n",
        "    - En Langchain, estos modelos tienen una interfaz est√°ndar, lo que significa que todos se comportan de manera similar cuando los usas.\n",
        "        \n",
        "    \n",
        "- **Modos de Invocaci√≥n:**\n",
        "    \n",
        "    - **Invocaci√≥n Normal (S√≠ncrona):** Es como preguntar algo y esperar a que el modelo te d√© la respuesta completa de una sola vez.\n",
        "        \n",
        "    - **Invocaci√≥n As√≠ncrona:** Es como preguntar algo y dejar que el modelo trabaje en segundo plano. Puedes hacer otras cosas mientras esperas la respuesta.\n",
        "        \n",
        "    - **Transmisi√≥n (Streaming):** Es como preguntar algo y recibir la respuesta poco a poco, palabra por palabra o token por token, en lugar de esperar a que est√© completa.\n",
        "        \n",
        "    \n",
        "- **Transmisi√≥n de Tokens:**\n",
        "    \n",
        "    - Los modelos de lenguaje generan texto dividi√©ndolo en \"tokens\". Un token puede ser una palabra, una parte de una palabra o un signo de puntuaci√≥n.\n",
        "        \n",
        "    - La transmisi√≥n de tokens significa que el modelo te env√≠a cada token a medida que lo genera, en lugar de esperar a tener toda la respuesta.\n",
        "        \n",
        "    \n",
        "\n",
        "**¬øPor qu√© es √ötil la Transmisi√≥n de Tokens?**\n",
        "\n",
        "- **Experiencia de Usuario Mejorada:**\n",
        "    \n",
        "    - En lugar de esperar un largo tiempo sin saber si el modelo est√° funcionando, el usuario ve la respuesta aparecer gradualmente. Esto hace que la interacci√≥n sea m√°s fluida y natural.\n",
        "        \n",
        "    - Es como ver a alguien escribir en tiempo real en lugar de esperar a que termine de escribir todo el texto.\n",
        "        \n",
        "    \n",
        "- **Respuestas M√°s R√°pidas (Aparentemente):**\n",
        "    \n",
        "    - Aunque el tiempo total para generar la respuesta puede ser el mismo, la transmisi√≥n hace que parezca que la respuesta llega m√°s r√°pido, ya que el usuario ve el texto aparecer poco a poco.\n",
        "        \n",
        "    \n",
        "- **Procesamiento en Tiempo Real:**\n",
        "    \n",
        "    - Puedes empezar a procesar la respuesta del modelo a medida que se genera, sin tener que esperar a que est√© completa. Esto es √∫til en aplicaciones que necesitan reaccionar a la respuesta del modelo en tiempo real."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este codigo usamos el metodo `.stream()` en lugar de `.invoke()` que devuelve un genera un flujo de tokens con la respuestas y que podemos iterar con un bucle."
      ],
      "metadata": {
        "id": "KPyImbrOcQO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5n1KnKDkt94"
      },
      "outputs": [],
      "source": [
        "for token in modelo2.stream(\"Explica los verbos modales en ingl√©s.\"):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Runnables**\n",
        "---\n",
        "En LangChain, un Runnable es una abstracci√≥n que representa cualquier componente capaz de ejecutar una tarea, como modelos de lenguaje, cadenas de operaciones o herramientas personalizadas. Los modelos de lenguaje, como ChatGroq o OpenAI, son ejemplos clave de Runnable, ya que pueden generar texto o realizar tareas espec√≠ficas al recibir una entrada. Estos componentes son fundamentales para construir flujos de trabajo, ya que permiten encadenar y combinar operaciones de manera modular y flexible.\n",
        "\n",
        "Los Runnable ofrecen m√©todos como `.invoke(),` .`stream()` y `.batch()` para manejar diferentes formas de ejecuci√≥n. Por ejemplo, `.invoke()` devuelve el resultado completo de una tarea, mientras que `.stream()` genera un flujo incremental de resultados, ideal para respuestas largas o en tiempo real. Esta flexibilidad hace que los Runnable sean esenciales para integrar modelos de lenguaje en aplicaciones complejas, permitiendo desde simples llamadas a modelos hasta flujos de trabajo personalizados y escalables.\n",
        "\n",
        "Profundizaremos en los Runnables m√°s adelante. Por ahora, es fundamental saber que una amplia gama de componentes en Langchain se implementan como Runnables o pueden ser adaptados para serlo. Esto incluye elementos centrales como los modelos de lenguaje (LLMs) y los parsers de salida (OutputParsers). Si bien los DataLoaders en s√≠ mismos no son directamente Runnables, a menudo se utilizan para cargar datos que luego se procesan dentro de Runnables. Incluso las funciones Python pueden integrarse en cadenas LCEL al ser envueltas como Runnables"
      ],
      "metadata": {
        "id": "EvQ18A9ub82M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Referencias**\n",
        "---\n",
        "\n",
        "1. https://www.langchain.com/  \n",
        "2. https://python.langchain.com/docs/introduction/"
      ],
      "metadata": {
        "id": "RwKXFzC_RbBG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}