{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/1.%20Introducci%C3%B3n%20a%20LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. ¬øQu√© es LangChain y por qu√© es relevante en el mundo de los LLMs?**\n",
        "---\n",
        "\n",
        "## **1.1. LangChain en pocas palabras**\n",
        "---\n",
        "\n",
        "**LangChain** es un framework dise√±ado para facilitar la construcci√≥n de aplicaciones basadas en _Large Language Models (LLMs)_. Proporciona herramientas y abstracciones que permiten **conectar** distintos modelos e integrarlos en _pipelines_ o _flujos de trabajo_ (chains) m√°s complejos. Con LangChain podemos, por ejemplo:\n",
        "\n",
        "- **Construir** aplicaciones que combinen m√∫ltiples llamadas a modelos de lenguaje.\n",
        "- **Enriquecer** las respuestas con contexto proveniente de bases de datos o documentos.\n",
        "- **Implementar memorias** que recuerden datos de interacciones previas.\n",
        "\n",
        "Su objetivo principal es **simplificar** el desarrollo de aplicaciones que aprovechan modelos de lenguaje, a la vez que proporciona **flexibilidad** para integrar servicios de diferentes proveedores y orquestar diferentes estrategias de prompting.\n",
        "\n",
        "Se habla de Langchain como un **software de orquestaci√≥n** ya que su fortaleza reside en la capacidad de conectar y coordinar diversos componentes, como modelos de lenguaje, prompts, bases de datos y herramientas externas, permitiendo definir flujos de trabajo complejos (cadenas) donde la salida de un componente alimenta la entrada del siguiente. Esta orquestaci√≥n facilita la construcci√≥n de aplicaciones sofisticadas.  \n",
        "\n",
        "![https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png](https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png)\n",
        "\n",
        "## **1.2. Importancia y casos de uso de los LLMs**\n",
        "---\n",
        "\n",
        "Los _Large Language Models (LLMs)_ son redes neuronales especializadas en procesar y generar lenguaje natural. Modelos como GPT-3, GPT-4 o Llama 2 han demostrado ser muy eficaces para:\n",
        "\n",
        "- **Generaci√≥n de texto**: redacci√≥n de art√≠culos, guiones de v√≠deo, textos creativos, etc.\n",
        "- **Traducciones y resumidos**: convertir textos entre diferentes idiomas o generar res√∫menes de documentos extensos.\n",
        "- **Asistentes conversacionales**: chatbots que mantienen el contexto de la conversaci√≥n y pueden llevar a cabo tareas o responder preguntas.\n",
        "- **An√°lisis y clasificaci√≥n** de textos: etiquetar o extraer informaci√≥n relevante de grandes vol√∫menes de texto.\n",
        "- **Soporte en programaci√≥n**: autocompletar c√≥digo o explicar algoritmos.\n",
        "\n",
        "Dada esta variedad de posibilidades, integrar LLMs en productos o proyectos se ha convertido en una **prioridad** para muchas empresas y desarrolladores. Y es justo ah√≠ donde **LangChain** ofrece un entorno de desarrollo potente, √°gil y modular."
      ],
      "metadata": {
        "id": "Ef9QGtxV_AW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Configuracion del entorno del cuaderno**\n",
        "---"
      ],
      "metadata": {
        "id": "Usx9MmcgPiwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Obtener claves API y incorporarlas al cuaderno**\n",
        "---\n",
        "Para que la mayor√≠a de los ejemplos de este cuaderno funcionen, es necesario contar con las claves API de los servicios que vamos a usar. Estas claves funcionan como ‚Äúcontrase√±as‚Äù o ‚Äútokens‚Äù que nos permiten autenticar nuestro c√≥digo en cada servicio. Si no tienes una cuenta en cada plataforma, deber√°s crearla primero y luego generar tu clave API en el panel correspondiente.  \n",
        "Apuntalas en un fichero a medida que las obtengas. Las necesitaras en el proximo paso.\n",
        "\n",
        "GROQ: https://console.groq.com/keys  \n",
        "GOOGLE AI STUDIO: https://aistudio.google.com/apikey  \n",
        "HUFFING FACE: https://huggingface.co/settings/tokens  \n",
        "OPENAI: https://platform.openai.com/api-keys  (**de pago!**)\n",
        "MISTRAL: https://console.mistral.ai/api-keys/  \n",
        "TOGETHER: https://api.together.ai/\n",
        "ANTHROPIC: https://console.anthropic.com/ (**de pago!**)\n",
        "\n",
        "\n",
        "No es necesario disponer de todas. Si decides por ejemplo no usar la API de OPENAI (que es de pago) simplemente ignorala en este momento. Y  mas adelante comenta en el codigo todas las referencias a OPENAI\n",
        "\n",
        "Ve a la barra lateral y haz clic en icono de una llave (\"Secretos\".)\n",
        "Haz clic en \"A√±adir secreto nuevo\".\n",
        "En \"Nombre\", escribe el nombre de tu secreto.  \n",
        "\n",
        "üëâüèª Los nombres importan para que funcione el codigo que sigue. Debes usar los siguientes:\n",
        "\n",
        "OPENAI_API_KEY  \n",
        "GROQ_API_KEY  \n",
        "GOOGLE_API_KEY  \n",
        "HUGGINGFACEHUB_API_TOKEN  \n",
        "MISTRAL_API_KEY  \n",
        "TOGETHER_API_KEY  \n",
        "ANTHROPIC_API_KEY  \n",
        "\n",
        "\n",
        "En \"Value\", pega el valor de la clave API.\n",
        "\n",
        "![Configuraci√≥n de Secretos en Colab](https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/secretos.png)"
      ],
      "metadata": {
        "id": "YjL7sgN5e8wl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q-cZ5A6YUXD"
      },
      "source": [
        "##**2.2. Carga Segura de Claves API**\n",
        "---\n",
        "\n",
        "Este c√≥digo se utiliza en Google Colab para obtener claves de API (tokens de acceso) almacenadas de forma segura en la secci√≥n \"Secretos\" de Colab y se asignan a variables para su posterior uso.\n",
        "\n",
        "Estas claves son necesarias para acceder a servicios externos, como OpenAI, Groq, Google o Hugging Face, desde tu notebook.  \n",
        "\n",
        "Este proceso evita la necesidad de codificar las claves directamente en el c√≥digo, lo que representa una mejor pr√°ctica de seguridad.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQMk_1lnonK3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi2sqEbTVGQg"
      },
      "source": [
        "## **2.3. Instalaci√≥n de las bibliotecas necesarias**\n",
        "---\n",
        "\n",
        " Instala las bibliotecas principales (langchain) y sus integraciones con diversos modelos de lenguaje (OpenAI, Groq, Google Gemini).  \n",
        "\n",
        " La opci√≥n -qU asegura una instalaci√≥n silenciosa y que tengamos las √∫ltimas versiones disponibles.\n",
        "\n",
        " %%capture... captura los mensajes de instalaci√≥n para evitar que salgan a pantalla (salvo si hay errores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc8S-4mgj8VS"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Instalamos el paquete principal de Langchain\n",
        "%pip install langchain -qU\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9keoznMXNTx"
      },
      "source": [
        "## **2.4. Importaci√≥n de las clases necesarias**\n",
        "---\n",
        "\n",
        "Este bloque de c√≥digo importa las clases necesarias de langchain para crear y utilizar los modelos de lenguaje de OpenAI, Groq y Google.  \n",
        "\n",
        "Estas clases son un envoltorio (*wrapper*) para las APIs de sus respectivos propietarios (OpenAI, Groq y Google)\n",
        "\n",
        "Tambien las clases que proporciona Langchain para la creaci√≥n de mensajes de usuario y sistema, que usamos para contruir un mensaje (*prompt*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_60Sdpp0n8Pv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. El momento de la verdad: Invocando nuestros LLMs**\n",
        "---\n",
        "Una vez que hemos instalado los componentes requeridos, importado las librer√≠as necesarias y configurado las claves API, estamos listos para la acci√≥n. Realicemos ahora nuestra primera llamada, aunque sea una sencilla, a un modelo de lenguaje (LLM)\n"
      ],
      "metadata": {
        "id": "XYnbLkyihy2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8EkWezHj1S_"
      },
      "outputs": [],
      "source": [
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "modelo3 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=GOOGLE_API_KEY)\n",
        "modelo4 = HuggingFaceEndpoint(repo_id=\"Qwen/Qwen2.5-72B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
        "\n",
        "prompt = [\n",
        "            SystemMessage(\"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "            HumanMessage(\"Estar en la edad del pavo\")\n",
        "           ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos el metodo .invoke() de los modelos instanciados para llamar a los LLM pasandoles el mensaje."
      ],
      "metadata": {
        "id": "Qjc6oLbWQcAd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0BBP2-zaQml"
      },
      "outputs": [],
      "source": [
        "modelo1.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfvmxobOaQcQ"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upGJmBNPaUHg"
      },
      "outputs": [],
      "source": [
        "modelo3.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo4.invoke(prompt)"
      ],
      "metadata": {
        "id": "hzRzGdQnZBSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Ys3ydEcAiS"
      },
      "source": [
        "###**Otras formas de contruir el prompt**\n",
        "\n",
        "LangChain tambi√©n admite entradas de modelos de chat mediante cadenas o formato OpenAI  \n",
        "Son equivalentes los siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deh2BAhtcDTT"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(\"Traduce a un frase equivalente (idiom) en ingl√©s: M√°s vale p√°jaro en mano que ciento volando\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uA1DVtpdMlk"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke([\n",
        "    {\"role\":\"system\", \"content\": \"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"},\n",
        "    {\"role\":\"user\", \"content\": \"M√°s vale p√°jaro en mano que ciento volando\"}\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_GXFAVcSFlN"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke([\n",
        "    (\"system\", \"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    (\"user\", \"M√°s vale p√°jaro en mano que ciento volando\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgeyJ8Gfham"
      },
      "source": [
        "###**Respuesta del LLM**\n",
        "\n",
        "Observa que la respuesta del metodo `.invoke()` devuelve mucha informacion util o que podria ser util en aplicaciones mas complejas.  \n",
        "\n",
        "La respuesta es un **objeto de la clase `AIMessage`** (definida dentro de LangChain). Dicho objeto act√∫a de forma similar a una ‚Äúestructura de datos‚Äù que contiene varios atributos o _campos_, entre ellos:\n",
        "\n",
        "- `content`: el texto generado por el modelo.\n",
        "- `additional_kwargs`: un diccionario con posibles argumentos adicionales.\n",
        "- `response_metadata`: un diccionario donde viene, entre otras cosas, el nombre del modelo (`model_name`).\n",
        "- `usage_metadata`: estad√≠sticas del uso de tokens.\n",
        "- `id`: un identificador √∫nico de la ejecuci√≥n.una clase que contiene informaci√≥n sobre la respuesta de un modelo de lenguaje.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzlKEeGndxP0"
      },
      "outputs": [],
      "source": [
        "ai_message = modelo2.invoke([\n",
        "    SystemMessage(\"Proporciona una frase hecha (idiom) en ingl√©s, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    HumanMessage(\"Tener la mosca detr√°s de la oreja\")\n",
        "            ])\n",
        "\n",
        "print(\"Contenido de la respuesta (content):\\n\", ai_message.content)\n",
        "print(\"========\")\n",
        "print(\"Metadatos de la respuesta (response_metadata): \\n\", ai_message.response_metadata)\n",
        "print(\"========\")\n",
        "print(\"Tokens utilizados (usage_metadata): \\n\", ai_message.usage_metadata)\n",
        "print(\"========\")\n",
        "print(\"Identificador de la respuesta (id):\", ai_message.id)\n",
        "print(\"========\")\n",
        "print(\"Uso de tokens detallado(response_metadata['token_usage']:\\n\", ai_message.response_metadata['token_usage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GGV06vlIsK"
      },
      "source": [
        "# **4. Streaming**\n",
        "---\n",
        "En la interacci√≥n con modelos de lenguaje como GPT, el streaming ofrece una alternativa a la espera de la respuesta completa. En lugar de una entrega √∫nica, el modelo emite la respuesta progresivamente, token por token (fragmentos de palabras o caracteres). Esta transmisi√≥n gradual mejora la experiencia del usuario, quien percibe una respuesta m√°s r√°pida y natural al ver el texto aparecer en tiempo real. Adem√°s, el streaming permite iniciar el procesamiento de la respuesta en tiempo real, sin esperar a que el modelo termine de generar toda la salida. En esencia, el streaming transforma la interacci√≥n con el LLM de una espera prolongada a una recepci√≥n continua y din√°mica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este codigo usamos el metodo `.stream()` en lugar de `.invoke()` que devuelve un genera un flujo de tokens con la respuestas y que podemos iterar con un bucle."
      ],
      "metadata": {
        "id": "KPyImbrOcQO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5n1KnKDkt94"
      },
      "outputs": [],
      "source": [
        "for token in modelo2.stream(\"Explica los verbos modales en ingl√©s.\"):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Runnables**\n",
        "---\n",
        "En Langchain, piensa en un Runnable como un bloque de construcci√≥n fundamental: cualquier componente que puede realizar una tarea. Modelos de lenguaje como ChatGroq o OpenAI son excelentes ejemplos, ya que pueden generar texto o hacer cosas espec√≠ficas cuando les das una entrada. Estos bloques son cruciales porque te permiten unir y combinar diferentes tareas para crear flujos de trabajo. Los Runnables tienen funciones clave como .invoke() (para obtener el resultado completo de una vez) y .stream() (para recibir la respuesta poco a poco, ideal para textos largos o ver la respuesta en tiempo real).  \n",
        "\n",
        "Profundizaremos en los Runnables m√°s adelante. Por ahora, es fundamental saber que una amplia gama de componentes en Langchain se implementan como Runnables o pueden ser adaptados para serlo. Esto incluye elementos centrales como los modelos de lenguaje (LLMs) y los parsers de salida (OutputParsers). Si bien los DataLoaders en s√≠ mismos no son directamente Runnables, a menudo se utilizan para cargar datos que luego se procesan dentro de Runnables. Incluso las funciones Python pueden integrarse en cadenas LCEL al ser envueltas como Runnables"
      ],
      "metadata": {
        "id": "EvQ18A9ub82M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Referencias**\n",
        "---\n",
        "\n",
        "1. https://www.langchain.com/  \n",
        "2. https://python.langchain.com/docs/introduction/"
      ],
      "metadata": {
        "id": "RwKXFzC_RbBG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}