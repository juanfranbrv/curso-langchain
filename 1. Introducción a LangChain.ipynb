{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/1.%20Introducci%C3%B3n%20a%20LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. ¿Qué es LangChain y por qué es relevante en el mundo de los LLMs?**\n",
        "---\n",
        "\n",
        "## **1.1. LangChain en pocas palabras**\n",
        "---\n",
        "\n",
        "**LangChain** es un framework diseñado para facilitar la construcción de aplicaciones basadas en _Large Language Models (LLMs)_. Proporciona herramientas y abstracciones que permiten **conectar** distintos modelos e integrarlos en _pipelines_ o _flujos de trabajo_ (chains) más complejos. Con LangChain podemos, por ejemplo:\n",
        "\n",
        "- **Construir** aplicaciones que combinen múltiples llamadas a modelos de lenguaje.\n",
        "- **Enriquecer** las respuestas con contexto proveniente de bases de datos o documentos.\n",
        "- **Implementar memorias** que recuerden datos de interacciones previas.\n",
        "\n",
        "Su objetivo principal es **simplificar** el desarrollo de aplicaciones que aprovechan modelos de lenguaje, a la vez que proporciona **flexibilidad** para integrar servicios de diferentes proveedores y orquestar diferentes estrategias de prompting.\n",
        "\n",
        "Se habla de Langchain como un **software de orquestación** ya que su fortaleza reside en la capacidad de conectar y coordinar diversos componentes, como modelos de lenguaje, prompts, bases de datos y herramientas externas, permitiendo definir flujos de trabajo complejos (cadenas o chains) donde la salida de un componente alimenta la entrada del siguiente. Esta orquestación facilita la construcción de aplicaciones sofisticadas.  \n",
        "\n",
        "![https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png?token=GHSAT0AAAAAAC5HTYEE4WSTQKG224SMQHZMZ4FOHNA](https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/langchain.png?token=GHSAT0AAAAAAC5HTYEE4WSTQKG224SMQHZMZ4FOHNA)\n",
        "\n",
        "## **1.2. Importancia y casos de uso de los LLMs**\n",
        "---\n",
        "\n",
        "Los _Large Language Models (LLMs)_ son redes neuronales especializadas en procesar y generar lenguaje natural. Modelos como GPT-3, GPT-4 o Llama 2 han demostrado ser muy eficaces para:\n",
        "\n",
        "- **Generación de texto**: redacción de artículos, guiones de vídeo, textos creativos, etc.\n",
        "- **Traducciones y resumidos**: convertir textos entre diferentes idiomas o generar resúmenes de documentos extensos.\n",
        "- **Asistentes conversacionales**: chatbots que mantienen el contexto de la conversación y pueden llevar a cabo tareas o responder preguntas.\n",
        "- **Análisis y clasificación** de textos: etiquetar o extraer información relevante de grandes volúmenes de texto.\n",
        "- **Soporte en programación**: autocompletar código o explicar algoritmos.\n",
        "\n",
        "Dada esta variedad de posibilidades, integrar LLMs en productos o proyectos se ha convertido en una **prioridad** para muchas empresas y desarrolladores. Y es justo ahí donde **LangChain** ofrece un entorno de desarrollo potente, ágil y modular."
      ],
      "metadata": {
        "id": "Ef9QGtxV_AW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Configuracion del entorno del cuaderno**\n",
        "---"
      ],
      "metadata": {
        "id": "Usx9MmcgPiwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Obtener claves API y incorporarlas al cuaderno**\n",
        "---\n",
        "Para que la mayoría de los ejemplos de este cuaderno funcionen, es necesario contar con las claves API de los servicios que vamos a usar. Estas claves funcionan como “contraseñas” o “tokens” que nos permiten autenticar nuestro código en cada servicio. Si no tienes una cuenta en cada plataforma, deberás crearla primero y luego generar tu clave API en el panel correspondiente.  \n",
        "Apuntalas en un fichero a medida que las obtengas. Las necesitaras en el proximo paso.\n",
        "\n",
        "GROQ: https://console.groq.com/keys  \n",
        "GOOGLE AI STUDIO: https://aistudio.google.com/apikey  \n",
        "HUFFING FACE: https://huggingface.co/settings/tokens  \n",
        "OPENAI: https://platform.openai.com/api-keys  (**de pago!**)\n",
        "\n",
        "No es necesario disponer de todas. Si decides por ejemplo no usar la API de OPENAI (que es de pago) simplemente ignorala en este momento. Y  mas adelante comenta en el codigo todas las referencias a OPENAI\n",
        "\n",
        "Ve a la barra lateral y haz clic en icono de una llave (\"Secretos\".)\n",
        "Haz clic en \"Añadir secreto nuevo\".\n",
        "En \"Nombre\", escribe el nombre de tu secreto.  \n",
        "\n",
        "👉🏻 Los nombres importan para que funcione el codigo que sigue. Debes usar los siguientes:\n",
        "\n",
        "OPENAI_API_KEY  \n",
        "GROQ_API_KEY  \n",
        "GOOGLE_API_KEY  \n",
        "HUGGINGFACEHUB_API_TOKEN  \n",
        "\n",
        "En \"Value\", pega el valor de la clave API.\n",
        "\n",
        "![Configuración de Secretos en Colab](https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/images/secretos.png?token=GHSAT0AAAAAAC5HTYEEDLGANEQ7BARWGCGEZ4FPBXA)"
      ],
      "metadata": {
        "id": "YjL7sgN5e8wl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q-cZ5A6YUXD"
      },
      "source": [
        "##**2.2. Carga Segura de Claves API**\n",
        "---\n",
        "\n",
        "Este código se utiliza en Google Colab para obtener claves de API (tokens de acceso) almacenadas de forma segura en la sección \"Secretos\" de Colab y se asignan a variables para su posterior uso.\n",
        "\n",
        "Estas claves son necesarias para acceder a servicios externos, como OpenAI, Groq, Google o Hugging Face, desde tu notebook.  \n",
        "\n",
        "Este proceso evita la necesidad de codificar las claves directamente en el código, lo que representa una mejor práctica de seguridad.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQMk_1lnonK3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi2sqEbTVGQg"
      },
      "source": [
        "## **2.3. Instalación de las bibliotecas necesarias**\n",
        "---\n",
        "\n",
        " Instala las bibliotecas principales (langchain) y sus integraciones con diversos modelos de lenguaje (OpenAI, Groq, Google Gemini).  \n",
        "\n",
        " La opción -qU asegura una instalación silenciosa y que tengamos las últimas versiones disponibles.\n",
        "\n",
        " %%capture... captura los mensajes de instalación para evitar que salgan a pantalla (salvo si hay errores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mc8S-4mgj8VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643352b5-f944-4ec5-d529-84dbc83f16b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Instalamos el paquete principal de Langchain\n",
        "%pip install langchain -qU\n",
        "\n",
        "# Instalamos sus integraiones con otros servicios\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9keoznMXNTx"
      },
      "source": [
        "## **2.4. Importación de las clases necesarias**\n",
        "---\n",
        "\n",
        "Este bloque de código importa las clases necesarias de langchain para crear y utilizar los modelos de lenguaje de OpenAI, Groq y Google.  \n",
        "\n",
        "Estas clases son un envoltorio (*wrapper*) para las APIs de sus respectivos propietarios (OpenAI, Groq y Google)\n",
        "\n",
        "Tambien las clases que proporciona Langchain para la creación de mensajes de usuario y sistema, que usamos para contruir un mensaje (*prompt*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_60Sdpp0n8Pv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. El momento de la verdad: Invocando nuestros LLMs**\n",
        "---\n",
        "Una vez que hemos instalado los componentes requeridos, importado las librerías necesarias y configurado las claves API, estamos listos para la acción. Realicemos ahora nuestra primera llamada, aunque sea una sencilla, a un modelo de lenguaje (LLM)\n"
      ],
      "metadata": {
        "id": "XYnbLkyihy2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a8EkWezHj1S_"
      },
      "outputs": [],
      "source": [
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "modelo3 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=GOOGLE_API_KEY)\n",
        "modelo4 = HuggingFaceEndpoint(repo_id=\"Qwen/Qwen2.5-72B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
        "\n",
        "prompt = [\n",
        "            SystemMessage(\"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "            HumanMessage(\"Estar en la edad del pavo\")\n",
        "           ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos el metodo .invoke() de los modelos instanciados para llamar a los LLM pasandoles el mensaje."
      ],
      "metadata": {
        "id": "Qjc6oLbWQcAd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I0BBP2-zaQml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d55700-8e94-4f77-a8db-3422a2e01d61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='To be at a awkward age.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 52, 'total_tokens': 59, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-78e972e1-ab4d-490d-8d29-a748a26d67bb-0', usage_metadata={'input_tokens': 52, 'output_tokens': 7, 'total_tokens': 59, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "modelo1.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfvmxobOaQcQ"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upGJmBNPaUHg"
      },
      "outputs": [],
      "source": [
        "modelo3.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo4.invoke(prompt)"
      ],
      "metadata": {
        "id": "hzRzGdQnZBSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Ys3ydEcAiS"
      },
      "source": [
        "###**Otras formas de contruir el prompt**\n",
        "\n",
        "LangChain también admite entradas de modelos de chat mediante cadenas o formato OpenAI  \n",
        "Son equivalentes los siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deh2BAhtcDTT"
      },
      "outputs": [],
      "source": [
        "modelo2.invoke(\"Traduce a un frase equivalente (idiom) en inglés: Más vale pájaro en mano que ciento volando\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1uA1DVtpdMlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d280de-9f6c-497d-b611-e72b6263e8ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='A bird in the hand is worth two in the bush.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 85, 'total_tokens': 98, 'completion_time': 0.047272727, 'prompt_time': 0.01323484, 'queue_time': 0.107212177, 'total_time': 0.060507567}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_4196e754db', 'finish_reason': 'stop', 'logprobs': None}, id='run-cb4dc4a5-494b-413b-979e-3b4e433c9f5d-0', usage_metadata={'input_tokens': 85, 'output_tokens': 13, 'total_tokens': 98})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "modelo2.invoke([\n",
        "    {\"role\":\"system\", \"content\": \"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"},\n",
        "    {\"role\":\"user\", \"content\": \"Más vale pájaro en mano que ciento volando\"}\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_GXFAVcSFlN",
        "outputId": "b458dacf-295e-4c19-be79-abae3557525c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='A bird in the hand is worth two in the bush.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 85, 'total_tokens': 98, 'completion_time': 0.048048413, 'prompt_time': 0.016265944, 'queue_time': 0.0393188, 'total_time': 0.064314357}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_4e32347616', 'finish_reason': 'stop', 'logprobs': None}, id='run-de4a5ffc-7802-439c-8f9c-48f365d40cc8-0', usage_metadata={'input_tokens': 85, 'output_tokens': 13, 'total_tokens': 98})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "modelo2.invoke([\n",
        "    (\"system\", \"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    (\"user\", \"Más vale pájaro en mano que ciento volando\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgeyJ8Gfham"
      },
      "source": [
        "###**Respuesta del LLM**\n",
        "\n",
        "Observa que la respuesta del metodo `.invoke()` devuelve mucha informacion util o que podria ser util en aplicaciones mas complejas.  \n",
        "\n",
        "La respuesta es un **objeto de la clase `AIMessage`** (definida dentro de LangChain). Dicho objeto actúa de forma similar a una “estructura de datos” que contiene varios atributos o _campos_, entre ellos:\n",
        "\n",
        "- `content`: el texto generado por el modelo.\n",
        "- `additional_kwargs`: un diccionario con posibles argumentos adicionales.\n",
        "- `response_metadata`: un diccionario donde viene, entre otras cosas, el nombre del modelo (`model_name`).\n",
        "- `usage_metadata`: estadísticas del uso de tokens.\n",
        "- `id`: un identificador único de la ejecución.una clase que contiene información sobre la respuesta de un modelo de lenguaje.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PzlKEeGndxP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279b186f-aabf-41cb-bb54-2829c66da09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenido de la respuesta (content):\n",
            " To have a bee in one's bonnet.\n",
            "========\n",
            "Metadatos de la respuesta (response_metadata): \n",
            " {'token_usage': {'completion_tokens': 11, 'prompt_tokens': 83, 'total_tokens': 94, 'completion_time': 0.04, 'prompt_time': 0.014568136, 'queue_time': 0.358517083, 'total_time': 0.054568136}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_4e32347616', 'finish_reason': 'stop', 'logprobs': None}\n",
            "========\n",
            "Tokens utilizados (usage_metadata): \n",
            " {'input_tokens': 83, 'output_tokens': 11, 'total_tokens': 94}\n",
            "========\n",
            "Identificador de la respuesta (id): run-96a2f28a-5b43-45eb-8e75-80959e314674-0\n",
            "========\n",
            "Uso de tokens detallado(response_metadata['token_usage']:\n",
            " {'completion_tokens': 11, 'prompt_tokens': 83, 'total_tokens': 94, 'completion_time': 0.04, 'prompt_time': 0.014568136, 'queue_time': 0.358517083, 'total_time': 0.054568136}\n"
          ]
        }
      ],
      "source": [
        "ai_message = modelo2.invoke([\n",
        "    SystemMessage(\"Proporciona una frase hecha (idiom) en inglés, equivalentes a la que te proporcione. Responde con un frase y solo con la frase.\"),\n",
        "    HumanMessage(\"Tener la mosca detrás de la oreja\")\n",
        "            ])\n",
        "\n",
        "print(\"Contenido de la respuesta (content):\\n\", ai_message.content)\n",
        "print(\"========\")\n",
        "print(\"Metadatos de la respuesta (response_metadata): \\n\", ai_message.response_metadata)\n",
        "print(\"========\")\n",
        "print(\"Tokens utilizados (usage_metadata): \\n\", ai_message.usage_metadata)\n",
        "print(\"========\")\n",
        "print(\"Identificador de la respuesta (id):\", ai_message.id)\n",
        "print(\"========\")\n",
        "print(\"Uso de tokens detallado(response_metadata['token_usage']:\\n\", ai_message.response_metadata['token_usage'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35GGV06vlIsK"
      },
      "source": [
        "# **4. Streaming**\n",
        "---\n",
        "En la interacción con modelos de lenguaje como GPT, el streaming ofrece una alternativa a la espera de la respuesta completa. En lugar de una entrega única, el modelo emite la respuesta progresivamente, token por token (fragmentos de palabras o caracteres). Esta transmisión gradual mejora la experiencia del usuario, quien percibe una respuesta más rápida y natural al ver el texto aparecer en tiempo real. Además, el streaming permite iniciar el procesamiento de la respuesta en tiempo real, sin esperar a que el modelo termine de generar toda la salida. En esencia, el streaming transforma la interacción con el LLM de una espera prolongada a una recepción continua y dinámica.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este codigo usamos el metodo `.stream()` en lugar de `.invoke()` que devuelve un genera un flujo de tokens con la respuestas y que podemos iterar con un bucle."
      ],
      "metadata": {
        "id": "KPyImbrOcQO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5n1KnKDkt94"
      },
      "outputs": [],
      "source": [
        "for token in modelo2.stream(\"Explica los verbos modales en inglés.\"):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Runnables**\n",
        "---\n",
        "En Langchain, piensa en un Runnable como un bloque de construcción fundamental: cualquier componente que puede realizar una tarea. Modelos de lenguaje como ChatGroq o OpenAI son excelentes ejemplos, ya que pueden generar texto o hacer cosas específicas cuando les das una entrada. Estos bloques son cruciales porque te permiten unir y combinar diferentes tareas para crear flujos de trabajo. Los Runnables tienen funciones clave como .invoke() (para obtener el resultado completo de una vez) y .stream() (para recibir la respuesta poco a poco, ideal para textos largos o ver la respuesta en tiempo real).  \n",
        "\n",
        "Profundizaremos en los Runnables más adelante. Por ahora, es fundamental saber que una amplia gama de componentes en Langchain se implementan como Runnables o pueden ser adaptados para serlo. Esto incluye elementos centrales como los modelos de lenguaje (LLMs) y los parsers de salida (OutputParsers). Si bien los DataLoaders en sí mismos no son directamente Runnables, a menudo se utilizan para cargar datos que luego se procesan dentro de Runnables. Incluso las funciones Python pueden integrarse en cadenas LCEL al ser envueltas como Runnables"
      ],
      "metadata": {
        "id": "EvQ18A9ub82M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Referencias**\n",
        "---\n",
        "\n",
        "1. https://www.langchain.com/  \n",
        "2. https://python.langchain.com/docs/introduction/"
      ],
      "metadata": {
        "id": "RwKXFzC_RbBG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}