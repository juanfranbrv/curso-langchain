{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/5.%20Cargadores%20de%20datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pw6o3FaExjg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XVSljdc2xjJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Preparando el entorno del cuaderno\n",
        "---\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "- Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "cvW7RJJjdTi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "%pip install langchain-community  -qU # Instalar el paquete langchain-community, necesario para los Loaders\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "3sPMCzq6dtA2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/datasets/fichero.txt?token=GHSAT0AAAAAAC46W6R3UHHBKLMKSDHHNI6UZ4EIF4Q   -O fichero.txt\n",
        "!wget https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/datasets/datos.csv?token=GHSAT0AAAAAAC46W6R3YIVQHNWARDGD4G2CZ4EIY5A -O datos.csv"
      ],
      "metadata": {
        "id": "2AJqP39FmkCv",
        "outputId": "bd6d3b0b-a547-45d5-d585-802fbef1cfc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-12 22:47:39--  https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/datasets/fichero.txt?token=GHSAT0AAAAAAC46W6R3UHHBKLMKSDHHNI6UZ4EIF4Q\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-01-12 22:47:39 ERROR 404: Not Found.\n",
            "\n",
            "--2025-01-12 22:47:39--  https://raw.githubusercontent.com/juanfranbrv/curso-langchain/refs/heads/main/datasets/datos.csv?token=GHSAT0AAAAAAC46W6R3YIVQHNWARDGD4G2CZ4EIY5A\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-01-12 22:47:39 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1\\. Introducción a los cargadores de documentos (Document Loaders)\n",
        "---\n",
        "\n",
        "Los _Document Loaders_ en LangChain son componentes que se encargan de localizar y extraer la información de una fuente determinada (archivos locales, webs, bases de datos, etc.) para luego transformarla en un formato que pueda ser consumido por modelos de lenguaje. Piensa en ellos como “puentes” entre tus datos en bruto y las herramientas de procesamiento de texto que ofrece LangChain.\n",
        "\n",
        "Son importantes porque constituyen el primer eslabón del proceso de análisis: seleccionan qué información se utiliza y la preparan para que tus cadenas (chains) y modelos funcionen correctamente. Con una buena elección y configuración de los loaders, se facilita la posterior segmentación (chunking) y optimización de consultas, reduciendo así el costo computacional y maximizando la relevancia de las respuestas.\n",
        "\n"
      ],
      "metadata": {
        "id": "UJuGPKevamum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2\\. Tipos de Document Loaders en LangChain\n",
        "---\n",
        "\n",
        "LangChain ofrece una variedad de loaders para trabajar con diferentes tipos de fuentes de datos (archivos locales, páginas web, servicios en la nube, etc.). Cada uno tiene sus particularidades y ventajas, permitiéndote elegir el mejor para tu proyecto y asegurando que solo la información más relevante alimente tus flujos de trabajo.\n",
        "\n",
        "Algunos importantes son los siguientes:\n",
        "\n",
        "-   **Cargadores para archivos locales**\n",
        "    -   _TextLoader_: para archivos .txt.\n",
        "    -   _CSVLoader_: para archivos .csv.\n",
        "    -   _PDFLoader_: para documentos en PDF (con dependencias como `PyPDF2` o `pdfminer`).\n",
        "    -   _UnstructuredFileLoader_: para distintos tipos de archivos basados en librerías “unstructured”.\n",
        "-   **Cargadores para fuentes web**\n",
        "    -   _WebBaseLoader_: para extraer texto desde páginas web.\n",
        "    -   _URLsLoader_: para múltiples URLs de forma simultánea.\n",
        "    -   Consideraciones sobre rate-limiting, HTML parsing, etc.\n",
        "-   **Cargadores para APIs o servicios en la nube**\n",
        "    -   _GoogleDriveLoader_: lectura de documentos en Google Drive.\n",
        "    -   _S3Loader_ (Amazon): acceso a buckets S3.\n",
        "    -   _OneDriveLoader_: conexión con Microsoft OneDrive.\n",
        "    -   Factores de autenticación y permisos.\n",
        "-   **Cargadores especializados**\n",
        "    -   _YouTubeLoader_: extracción de transcripciones de videos de YouTube.\n",
        "    -   _EverNoteLoader_, _NotionLoader_, etc., dependiendo de integraciones específicas.\n",
        "    -   Cargadores basados en servicios de bases de datos (por ejemplo, MySQL, PostgreSQL).\n",
        "\n",
        "Aquí puedes encontrar todos: https://python.langchain.com/docs/integrations/document_loaders/"
      ],
      "metadata": {
        "id": "jKwIMIHibQkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Flujo de trabajo básico con Document Loaders\n",
        "---\n",
        "La idea principal es elegir el loader adecuado para tu fuente de datos (archivos, web, API), configurarlo con las librerías necesarias y ejecutar la extracción de información. Tras obtener el contenido, se pueden aplicar pasos como chunking, limpieza de texto y enriquecimiento de metadatos, antes de utilizarlo en cadenas o modelos dentro de LangChain.\n",
        "\n",
        "Vamos a ver algunos ejemplos."
      ],
      "metadata": {
        "id": "wZmfpIemcZ8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextLoader"
      ],
      "metadata": {
        "id": "G83FEGqHcwTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Cargamos el contenido de un archivo de texto\n",
        "ruta_fichero = \"/content/mifichero.txt\"  # Ajusta la ruta según tu caso\n",
        "loader = TextLoader(ruta_fichero)\n",
        "documentos = loader.load()\n",
        "\n",
        "print(f\"Se han cargado {len(documentos)} documento(s).\")\n",
        "# if documentos:\n",
        "#     print(\"Primer documento - Metadatos:\", documentos[0].metadata)\n",
        "#     print(\"Primer documento - Contenido (primeros 200 caracteres):\")\n",
        "#     print(documentos[0].page_content[:200], \"...\")\n",
        "\n",
        "# Tomamos el texto cargado como 'contexto_texto'\n",
        "contexto_texto = documentos[0].page_content if documentos else \"Sin contenido\"\n",
        "\n",
        "# Definimos un prompt para resumir el contenido\n",
        "texto_prompt = \"\"\"\n",
        "El siguiente texto proviene de un archivo local:\n",
        "\n",
        "\"{texto}\"\n",
        "\n",
        "Por favor, elabora un resumen breve (no más de 3 líneas) que capture\n",
        "las ideas principales del contenido anterior.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"texto\"]\n",
        ")\n",
        "\n",
        "# Generamos el prompt usando el texto cargado\n",
        "prompt = prompt_template.format(texto=contexto_texto)\n",
        "# print(\"\\nPrompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo (asegúrate de configurar tu API key)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Hacemos la llamada al LLM para obtener el resumen\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "NuFm4E49e8Aw",
        "outputId": "db57ce73-ed70-4d2b-904d-9dfe3f099e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han cargado 1 documento(s).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Python es un lenguaje de programación interpretado, multiparadigma y de alto nivel, creado por Guido van Rossum en los años ochenta. Destaca por su legibilidad y facilidad de uso, y ha evolucionado a través de diversas versiones, siendo ampliamente utilizado en áreas como inteligencia artificial y machine learning. Desde 2020, solo se mantiene soporte para Python 3."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CsvLoader\n",
        "\n",
        "asa"
      ],
      "metadata": {
        "id": "V6ygHZ6Tk-yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "\n",
        "\n",
        "# Cargamos el contenido de un archivo CSV\n",
        "ruta_fichero = \"/datos.csv\"  # Ajusta la ruta según tu caso\n",
        "loader = CSVLoader(ruta_fichero)\n",
        "documentos = loader.load()\n",
        "\n",
        "print(f\"Se han cargado {len(documentos)} documento(s).\")\n",
        "\n",
        "# Tomamos el primer documento para ejemplificar un análisis\n",
        "contexto_texto = documentos[0].page_content if documentos else \"Sin contenido\"\n",
        "\n",
        "# Prompt para realizar un análisis específico de los datos\n",
        "texto_prompt = \"\"\"\n",
        "El texto a continuación representa los datos de un registro en un archivo CSV:\n",
        "\n",
        "\"{texto}\"\n",
        "\n",
        "Por favor, analiza este registro e indica:\n",
        "1. Un posible uso que se podría dar a esta información.\n",
        "2. Una hipótesis acerca del comportamiento o tendencia reflejada en estos datos.\n",
        "3. Una recomendación para mejorar o dar mayor contexto a este registro de datos.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"texto\"]\n",
        ")\n",
        "\n",
        "# Generamos el prompt usando el texto cargado\n",
        "prompt = prompt_template.format(texto=contexto_texto)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Hacemos la llamada al LLM para obtener el análisis\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "display(Markdown(respuesta.content))\n"
      ],
      "metadata": {
        "id": "BnQ5GA-ilCM5",
        "outputId": "00be45a0-8a5d-490d-e5c2-97eab2829ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han cargado 2729 documento(s).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-29e99b5d8a99>:39: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Claro, aquí tienes un análisis del registro proporcionado:\n\n### 1. Posible uso de la información:\nEsta información podría ser utilizada por entidades gubernamentales, organizaciones culturales o instituciones educativas para la gestión del patrimonio cultural. Por ejemplo, el registro detalla un inmueble afectado culturalmente, lo que podría servir para la planificación de intervenciones, conservación, o promoción de actividades culturales en el mismo. Además, podría ser útil para investigadores que estudian la disponibilidad de espacios culturales en la provincia de Valencia.\n\n### 2. Hipótesis sobre el comportamiento o tendencia reflejada en estos datos:\nUna posible hipótesis es que las propiedades catalogadas como \"Cultural\" y \"Suelo Urbano\" podrían estar aumentando en número en la provincia de Valencia debido a un creciente interés por la recuperación y conservación del patrimonio cultural. Esto podría reflejar una tendencia hacia la valorización de espacios que tienen un significado histórico o cultural, impulsada por políticas públicas que fomentan la educación y la preservación del patrimonio.\n\n### 3. Recomendación para mejorar o dar mayor contexto a este registro de datos:\nPara dar mayor contexto y utilidad a este registro, se podría incluir información adicional como:\n\n- **Coordenadas Geográficas**: Incluir latitud y longitud para facilitar la ubicación del inmueble en un mapa.\n- **Descripción Detallada**: Proporcionar una breve descripción del inmueble, su historia, importancia cultural y cualquier evento relevante que haya tenido lugar allí.\n- **Estado de Conservación**: Incluir detalles sobre el estado actual del inmueble y cualquier intervención de conservación realizada o planificada.\n- **Datos de Contacto**: Añadir información de contacto de la entidad responsable de la gestión del inmueble para facilitar la consulta de interesados o investigadores.\n\nEsto enriquecería el registro, haciéndolo más accesible y útil para diversos usuarios."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUaLO9EEZtpn"
      },
      "source": [
        "# Referencias\n",
        "---\n",
        "\n",
        "1. https://python.langchain.com/docs/integrations/document_loaders/\n",
        "\n",
        "2. https://blog.davideai.dev/the-ultimate-langchain-series-data-loaders\n",
        "\n",
        "3. https://www.youtube.com/watch?v=zKlZ8Atv6Bc\n",
        "\n",
        "4. https://www.youtube.com/watch?v=75uBcITe0gU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FpIRD0gwcym8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guión\n",
        "\n",
        "## 1\\. Introducción a los Document Loaders\n",
        "\n",
        "1.  **Definición de Document Loaders**\n",
        "    -   ¿Qué son los Document Loaders?\n",
        "    -   Relación con el proceso general de generación y/o procesamiento de lenguaje en LangChain.\n",
        "2.  **Por qué son importantes**\n",
        "    -   Papel fundamental en la preparación de datos: localización, extracción y preprocesamiento.\n",
        "    -   Garantizar que los datos se adapten al modelo y al flujo de trabajo (por ejemplo, chunking).\n",
        "    -   Reducción de costos y optimización de rendimiento cuando se limitan los datos innecesarios.\n",
        "\n",
        "* * *\n",
        "\n",
        "## 2\\. Tipos de Document Loaders en LangChain\n",
        "\n",
        "1.  **Cargadores para archivos locales**\n",
        "    -   _TextLoader_: para archivos .txt.\n",
        "    -   _CSVLoader_: para archivos .csv.\n",
        "    -   _PDFLoader_: para documentos en PDF (con dependencias como `PyPDF2` o `pdfminer`).\n",
        "    -   _UnstructuredFileLoader_: para distintos tipos de archivos basados en librerías “unstructured”.\n",
        "2.  **Cargadores para fuentes web**\n",
        "    -   _WebBaseLoader_: para extraer texto desde páginas web.\n",
        "    -   _URLsLoader_: para múltiples URLs de forma simultánea.\n",
        "    -   Consideraciones sobre rate-limiting, HTML parsing, etc.\n",
        "3.  **Cargadores para APIs o servicios en la nube**\n",
        "    -   _GoogleDriveLoader_: lectura de documentos en Google Drive.\n",
        "    -   _S3Loader_ (Amazon): acceso a buckets S3.\n",
        "    -   _OneDriveLoader_: conexión con Microsoft OneDrive.\n",
        "    -   Factores de autenticación y permisos.\n",
        "4.  **Cargadores especializados**\n",
        "    -   _YouTubeLoader_: extracción de transcripciones de videos de YouTube.\n",
        "    -   _EverNoteLoader_, _NotionLoader_, etc., dependiendo de integraciones específicas.\n",
        "    -   Cargadores basados en servicios de bases de datos (por ejemplo, MySQL, PostgreSQL).\n",
        "\n",
        "* * *\n",
        "\n",
        "## 3\\. Flujo de trabajo básico con Document Loaders\n",
        "\n",
        "1.  **Selección del Loader adecuado**\n",
        "    -   Identificar el tipo de fuente de datos (archivo, web, API, etc.).\n",
        "    -   Compatibilidad y requerimientos (dependencias, librerías adicionales).\n",
        "    -   Métodos de autenticación o acceso a la fuente de datos.\n",
        "2.  **Configuración e inicialización**\n",
        "    -   Instalación de librerías necesarias (por ejemplo, `requests`, `beautifulsoup4`, `PyPDF2`, `google-api-python-client`, etc.).\n",
        "    -   Parámetros de configuración (paths, URL, tokens de acceso).\n",
        "3.  **Extracción de documentos**\n",
        "    -   Ejecución del loader para obtener el contenido bruto.\n",
        "    -   Lectura incremental, por lotes o tiempo real (según el caso).\n",
        "    -   Manejo de excepciones y errores de conexión o acceso.\n",
        "4.  **Ejemplo práctico**\n",
        "    -   Demostración de un caso simple (por ejemplo, cargar un PDF).\n",
        "    -   Revisión del objeto resultante y metadatos (título, fuente, etc.).\n",
        "    -   Cómo se integran esos datos en una _chain_ posterior.\n",
        "\n",
        "* * *\n",
        "\n",
        "## 4\\. Preprocesamiento y Normalización\n",
        "\n",
        "1.  **Chunking (segmentación de texto)**\n",
        "    -   Por qué es necesario (limitación de tokens, segmentación semántica, etc.).\n",
        "    -   Estrategias de chunking: por número de tokens, por longitud de caracteres, por secciones lógicas.\n",
        "    -   Ejemplo de implementación de chunking en LangChain.\n",
        "2.  **Limpieza y formateo de texto**\n",
        "    -   Remoción de caracteres especiales, saltos de línea, metadatos irrelevantes.\n",
        "    -   Conversión a minúsculas o mantenimiento del caso original.\n",
        "    -   Manejo de signos de puntuación y otras normalizaciones.\n",
        "3.  **Enriquecimiento de metadatos**\n",
        "    -   Agregar fecha, autor, tipo de documento, fuente, URL, etc.\n",
        "    -   Efecto en la relevancia y filtrado posterior con los LLMs.\n",
        "    -   Uso de metadatos para afinar búsquedas en bases vectoriales.\n",
        "\n"
      ],
      "metadata": {
        "id": "2JvStRM-Zuvg"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}