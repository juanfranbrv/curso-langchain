{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhVFzX0WtsR9nvtsfVfD8/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/3.%20Output%20parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. OutputParsers en Langchain**\n",
        "---\n",
        "Imagina que le preguntas a un LLM \"¬øCu√°les son los tres planetas m√°s cercanos al sol?\" y te responde: \"Mercurio, Venus y la Tierra son los planetas m√°s cercanos al sol\". Si bien la respuesta es correcta para un humano, para que tu programa pueda usar esa informaci√≥n, lo ideal ser√≠a tenerla en un formato m√°s manejable, como una lista o un objeto JSON. Aqu√≠ es donde entran en juego los Output Parsers.  \n",
        "\n",
        "Los Output Parsers te permiten ‚Äúforzar‚Äù o ‚Äúguiar‚Äù al modelo para que devuelva la informaci√≥n seg√∫n un formato deseado (por ejemplo, un JSON con campos espec√≠ficos, una lista, etc).  \n",
        "\n",
        "Convertir el texto libre, en informaci√≥n organizada, en algo que puedes usar directamente en tu c√≥digo.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Sin output parser: texto plano\n",
        "\"Tom Hanks ha actuado en Forrest Gump y Saving Private Ryan\"\n",
        "\n",
        "# Con output parser: estructura definida\n",
        "{\n",
        "  \"actor\": \"Tom Hanks\",\n",
        "  \"peliculas\": [\"Forrest Gump\", \"Saving Private Ryan\"]\n",
        "}\n",
        "```\n",
        "\n",
        "La estructuras de datos de salida son entre otras:\n",
        "\n",
        "- Listas\n",
        "- Enumeraciones\n",
        "- Objetos JSON\n",
        "- Diccionarios\n",
        "- Modelos Pydantic\n",
        "\n",
        "\n",
        "LangChain ofrece una variedad de Output Parsers preconstruidos para diferentes necesidades. Algunos de los mas usados son estos:\n",
        "\n",
        "- **StrOutputParser:** Convierte la salida a string\n",
        "\n",
        "- **CommaSeparatedListOutputParser**: Convierte la salida en una lista separada por comas. √ötil para generar listas de elementos\n",
        "\n",
        "- **EnumOutputParser**: Restringe la salida a un conjunto predefinido de valores. Perfecto para categor√≠as o estados limitados. Idela cuedo se desea que el LLM elija de un conjunto de opciones.\n",
        "\n",
        "- **JsonOutputParser** : Transforma la salida directamente en formato JSON. Ideal para respuestas estructuradas simples. Dos variantes principales:\n",
        "\n",
        "    - SimpleJsonOutputParser\n",
        "    - JsonOutputParser\n",
        "\n",
        "- **DatetimeOutputParser**: Extrae y formatea fechas y horas. √ötil para parsear informaci√≥n temporal\n",
        "\n",
        "- **StructuredOutputParser**: Permite definir esquemas de salida m√°s complejos. Configurable con m√∫ltiples campos\n",
        "\n",
        "- **PydanticOutputParser**: Convierte la salida en objetos Pydantic. Permite definir estructuras de datos complejas. Gran flexibilidad para validaci√≥n\n",
        "\n",
        "- (**OutputFixingParser**: Intenta corregir salidas mal formateadas. √ötil cuando el modelo no genera la estructura perfecta.)\n",
        "\n",
        "\n",
        "Puedes ver todos los OutputParsers disponibles aqu√≠:\n",
        "https://python.langchain.com/docs/concepts/output_parsers/\n",
        "\n",
        "\n",
        "# `with_structured_output`\n",
        "\n",
        "Es un m√©todo nativo que aprovecha capacidades del modelo. Utiliza capacidades de llamada de funci√≥n (function calling) del modelo. Es m√°s eficiente y preciso. Esta soportado por modelos avanzados como OpenAI, Anthropic, Groq.\n",
        "\n",
        "Lo trataremos al final de este cuaderno.\n",
        "\n",
        "üí° **La mayor parte de modelos soportan esta funci√≥n y es el futuro de la extracci√≥n estructurada en LangChain. Prior√≠zalo cuando puedas.**\n",
        "\n",
        "Puede consultarse una lista de modelos y sus capacidades aqu√≠:\n",
        "https://python.langchain.com/docs/integrations/chat/\n",
        "\n",
        "\n",
        "Crear un ejemplo que dada una receta la presente estructurada en ingrdientes, pasos, etc\n",
        "Esta en este video https://www.youtube.com/watch?v=lbWxastyWPw\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry8P2gp7VSGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Preparando el entorno del cuaderno**\n",
        "----\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios.\n",
        "\n",
        "- Instalamos la librer√≠a LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases espec√≠ficas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dej√°ndolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n"
      ],
      "metadata": {
        "id": "KEWPJUQdCird"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librer√≠as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Instalamos Rich para mejorar la salida\n",
        "%pip install rich -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos las librerias para formatear mejor la salida\n",
        "from IPython.display import Markdown, display\n",
        "from rich import print as rprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. StrOutputParser**\n",
        "---\n",
        "\n",
        "`StrOutputParser` es el output parser m√°s simple de LangChain. Su funci√≥n principal es convertir la salida del modelo de lenguaje directamente en una cadena de texto sin realizar ninguna transformaci√≥n estructural.\n",
        "\n",
        "Caracter√≠sticas principales:\n",
        "- Convierte la salida del modelo a texto plano\n",
        "- No realiza ninguna validaci√≥n o estructuraci√≥n\n",
        "- √ötil cuando solo necesitas el texto sin procesar\n",
        "- Muy ligero y directo\n",
        "\n",
        "Casos de uso tipicos: Resumenes, traducciones simples, ..."
      ],
      "metadata": {
        "id": "Zer1dYqJ4EEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo: Generador de Res√∫menes Ejecutivos\n",
        "\n",
        "A partir de uos resultados de ventas (simulado) deseamos obtener un informe sobre el mismo.\n",
        "\n"
      ],
      "metadata": {
        "id": "J6qyHDnsQkWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Configuramos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# Creamos un prompt para generar un resumen ejecutivo\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente experto en crear res√∫menes ejecutivos concisos y claros.\"),\n",
        "    (\"human\", \"Genera un resumen ejecutivo sobre el siguiente informe de ventas: {informe}\")\n",
        "])\n",
        "\n",
        "# Configuramos el output parser de tipo String\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Ejemplo de uso\n",
        "informe_ventas = \"\"\"\n",
        "Ventas del Q1 2024:\n",
        "- Ingresos totales: $1.5M\n",
        "- Crecimiento interanual: 22%\n",
        "- Producto m√°s vendido: Software de gesti√≥n\n",
        "- Principales mercados: Tecnolog√≠a y Finanzas\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format_prompt(informe=informe_ventas)\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "rprint(f\"[bold]Respuesta del modelo:\\n {respuesta}\")\n",
        "rprint(\"\\n-----\\n\")\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "rprint(f\"[bold green4]Resumen generado:\\n {respuesta_formateada}\")\n"
      ],
      "metadata": {
        "id": "Viuc2cB14Jse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cadenas muy simples donde solo esperas texto plano y no necesitas un control estricto sobre el tipo de dato, la diferencia pr√°ctica entre usar StrOutputParser expl√≠citamente y no usar ning√∫n OutputParser puede ser m√≠nima. En muchos casos, obtendr√°s una salida de texto en ambos escenarios.\n",
        "\n",
        "Sin embargo, usar StrOutputParser expl√≠citamente es una buena pr√°ctica ya que mejora la claridad y legibilidad del c√≥digo.\n",
        "   \n",
        "Ahora que entendemos c√≥mo obtener texto plano, veamos c√≥mo podemos empezar a estructurar la salida.\n",
        "\n",
        "**Pero antes necesitamos conocer y manejar dos conceptos relacionados...**\n",
        "\n"
      ],
      "metadata": {
        "id": "PIveJbqKZCpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Partial variables`\n",
        "\n",
        "Las partial_variables son un mecanismo en LangChain para pre-rellenar variables en un prompt de manera parcial, antes de su uso final.  \n",
        "\n",
        "Pi√©nsalo de esta manera: un PromptTemplate es como una plantilla de texto con \"huecos\" que necesitas llenar para crear un prompt completo para el LLM. Hay dos formas principales de llenar estos huecos:\n",
        "\n",
        "-   **input\\_variables:** Estas son las variables que **cambian** cada vez que utilizas el prompt. Son los datos espec√≠ficos que quieres que el LLM procese en cada llamada.\n",
        "    \n",
        "-   **partial\\_variables:** Estas son las variables que tienen un valor **fijo** o **predefinido** para un uso particular del PromptTemplate. No cambian con cada llamada a la cadena o LLM que usa este prompt."
      ],
      "metadata": {
        "id": "sjDo8JmHxKb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo nuestro PromptTemplate tiene 4 \"huecos\" o 4 variables. Pero al crearlo hemos precargado 3 de ellas con valores via partial variables. De esta al invocar el prompt (es un runnable !!) basta que pasemos una (tema).   \n",
        "Sin imbargo podriamos pasar tambien las restanteas..."
      ],
      "metadata": {
        "id": "MsyxZE-iyC8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Prompt con partial_variables\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Eres un {role} especializado en {area}. {instrucciones}\",\n",
        "    input_variables=[\"tema\"],\n",
        "    partial_variables={\n",
        "        \"role\": \"analista\",\n",
        "        \"area\": \"tecnolog√≠a\",\n",
        "        \"instrucciones\": \"Proporciona un an√°lisis detallado y objetivo.\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Uso del prompt\n",
        "resultado = prompt.invoke({\"tema\": \"Inteligencia Artificial\"})\n",
        "print(f\"{resultado}\")\n",
        "\n",
        "resultado = prompt.invoke({\"tema\": \"Inteligencia Artificial\", \"instrucciones\": \"Contesta con un pareado que rime\"})\n",
        "rprint(f\"[bold green4]{resultado}\")\n"
      ],
      "metadata": {
        "id": "i0zpPWv1xu73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un uso tipico de las partial_variables es usarlas para introducir en el prompt las instrucciones de l formato que proporciona Langchain"
      ],
      "metadata": {
        "id": "3Tz1VTF1zL-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `.get_format_instructions()`\n",
        "\n",
        "**Obtener instrucciones de formato (opcional pero recomendado):** Muchos parsers tienen un m√©todo get\\_format\\_instructions() que devuelve texto que puedes incluir en tu prompt para guiar al LLM sobre el formato esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDsh5c2vaw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "id": "2ov8pnCtaeFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podriamos redactar nosotros mismos las instrucciones ? SI, sin duda. Esto es solo una funcion de utilidad que nos proporciona Langchain. Teoricamnte disponemos de esta forma de una redaccion tecnicamente correcta."
      ],
      "metadata": {
        "id": "N4xnI29VzrbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. CommaSeparatedListOutputParser**\n",
        "---\n",
        "\n",
        "El `CommaSeparatedListOutputParser` en Langchain es un OutputParser **simple pero muy √∫til** dise√±ado para **interpretar la salida de un modelo de lenguaje (LLM) como una lista de elementos separados por comas.** Su funci√≥n principal es tomar el texto generado por el LLM y **transformarlo en una lista de strings de Python**, donde cada string representa un elemento de la lista original que estaba separado por comas en el texto del LLM.\n",
        "\n",
        "  \n",
        "\n",
        "Imagina que le pides a un LLM que te d√© \"tres colores primarios separados por comas\". Podr√≠as esperar una respuesta como:\n",
        "\n",
        "\"rojo, azul, amarillo\"\n",
        "\n",
        "El CommaSeparatedListOutputParser toma esta cadena \"rojo, azul, amarillo\" y la procesa de la siguiente manera:\n",
        "\n",
        "- **Divide la cadena:** Utiliza la coma (,) como delimitador para dividir la cadena en partes m√°s peque√±as.\n",
        "    \n",
        "- **Elimina espacios en blanco (opcional):** Puede configurarse para eliminar espacios en blanco al principio y al final de cada parte extra√≠da. Por defecto, suele hacerlo para limpiar la lista resultante.\n",
        "    \n",
        "- **Crea la lista:** Cada parte resultante se convierte en un elemento de una lista de Python.\n",
        "    \n",
        "-   **Casos de uso:** Obtener listas de elementos, como nombres, ideas, pasos a seguir, o categor√≠as.\n",
        "    \n",
        "-   **Ventaja:** Simple y efectivo para extraer listas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "flcuOq9Nh6k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo: Lista de ingredientes\n",
        "Queremos obtener una lista de ingredientes para hacer una pizza casera y solo nos interesa la lista de ingredientes, pues la procesaremos posteriormente de alguan forma.\n",
        "\n",
        "üëÄObserva el uso de las partial_variables paara introducir en el prompt las instrucciones de formato\n",
        "\n",
        "üëÄObserva tambien el tipo de las dos respuestas. El primero es un string y no podriamos iterarlo. El segundo es una lista de python, si podemos iterarla."
      ],
      "metadata": {
        "id": "ILCufEkEiWLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Genera una lista de ingredientes para hacer {receta} casera. Solo lista los ingredientes, uno por l√≠nea. Sin opciones\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Generar la salida\n",
        "prompt = prompt_template.format(receta=\"pizza\")\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "# Mostrar los resultados\n",
        "rprint(f\"Respuesta del modelo SIN FORMATEAR:\\n [bold bright_cyan]{respuesta}\")\n",
        "rprint(type(respuesta))\n",
        "rprint(\"\\n\\n\")\n",
        "rprint(f\"Respuesta del modelo FORMATEADA:\\n [bold spring_green3]{respuesta_formateada}\")\n",
        "rprint(type(respuesta_formateada))\n"
      ],
      "metadata": {
        "id": "AySI2dvhi78N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo: Lista de etiquetas  \n",
        "Deseamos que el modelo genere una lista de hastags (o etiquetas) a partir del tema de un articulo"
      ],
      "metadata": {
        "id": "1xh-L0f-7gFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"\"\"Dame 5 etiquetas relevantes para un post de blog sobre: {tema}.\n",
        "                Las etiquetas deben estar separadas por comas.\"\"\",\n",
        "    input_variables=[\"tema\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "\n",
        "# Formatear el prompt con el tema del blog\n",
        "prompt = prompt_template.format(tema=\"Recetas de cocina vegana f√°ciles y r√°pidas para principiantes\")\n",
        "\n",
        "# Obtener la salida del LLM\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "# Mostrar los resultados\n",
        "rprint(f\"Respuesta del modelo SIN FORMATEAR:\\n [bold bright_cyan]{respuesta}\")\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(\"\\n\\n\")\n",
        "\n",
        "rprint(f\"Respuesta del modelo FORMATEADA:\\n [bold spring_green3]{respuesta_formateada}\")\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "rprint(\"\\n\\n\")\n",
        "\n",
        "# Imprimir las etiquetas generadas\n",
        "print(\"Etiquetas sugeridas:\")\n",
        "for etiqueta in respuesta_formateada:\n",
        "    rprint(f\"[bold spring_green3] - {etiqueta.strip()}\")\n"
      ],
      "metadata": {
        "id": "9p3CwHtv8Svc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. EnumOutputParser**\n",
        "---\n",
        "\n",
        "`EnumOutputParser` es un tipo de output parser en LangChain que se utiliza para restringir la salida de un modelo a un conjunto predefinido de valores. Esto es √∫til cuando se desea que la respuesta del modelo pertenezca a un conjunto espec√≠fico de opciones, como categor√≠as, estados o tipos.\n",
        "\n",
        "### Caracter√≠sticas Principales:\n",
        "\n",
        "-   **Restricci√≥n de Valores**: Permite definir un conjunto limitado de opciones que el modelo puede devolver.\n",
        "-   **Validaci√≥n Autom√°tica**: Si la salida del modelo no coincide con las opciones definidas, se puede manejar como un error.\n",
        "-   **Facilita la Consistencia**: Asegura que las respuestas sean coherentes y dentro de un rango esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "lj97SmlAofVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo de Caso de Uso: Clasificaci√≥n de Sentimientos\n",
        "\n",
        "Imaginemos que estamos construyendo un sistema que clasifica el sentimiento de comentarios de clientes sobre un producto. Queremos que el modelo devuelva solo tres categor√≠as: \"positivo\", \"negativo\" y \"neutral\".\n",
        "\n",
        "Este ejemplo que NO FUNCIONA muestra las limitaciones de los OutputParsers pero tambien unas de sus utilidades, poder hacer validaciones y atrapar el error."
      ],
      "metadata": {
        "id": "S94-9qKkFiJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definimos las opciones de sentimiento\n",
        "class Sentimientos(Enum):\n",
        "    POSITIVO = \"positivo\"\n",
        "    NEGATIVO = \"negativo\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=Sentimientos)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Clasifica el siguiente comentario: {comentario}\",\n",
        "    input_variables=[\"comentario\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "\n",
        "# Ejemplo de uso\n",
        "comentario_cliente = \"Me encanta este producto, es incre√≠ble y funciona muy bien.\"\n",
        "\n",
        "prompt = prompt_template.format(comentario=comentario_cliente)\n",
        "\n",
        "# Clasificamos el sentimiento\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "try:\n",
        "    respuesta_formateada = output_parser.parse(respuesta)\n",
        "except Exception as e:\n",
        "    respuesta_formateada = \"No se pudo clasificar el sentimiento\"\n",
        "    rprint(f\"[bold white on red]Error al parsear la salida: {e}\")\n",
        "\n",
        "\n",
        "rprint(\"Sentimiento Clasificado:\")\n",
        "rprint(respuesta_formateada)\n"
      ],
      "metadata": {
        "id": "R-xNyOZNFsHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que a pesar de que el modelo si evalua correctaement el sentimiento, NO DEVUELVE EXACTAMENTE lo que nececitamos. Y el OutputParser no consigue extraer de la resuesta el valor buscado.\n",
        "\n",
        "Lanchain plantea esta opciones (en el enlace):\n",
        "\n",
        "- Usar `with_structured_output`\n",
        "- Usar LangGraph\n",
        "- Mejorar el prompt\n",
        "- Cambiar de modelo\n",
        "- Usar reintentos (con algun parserfixing)\n",
        "\n",
        "En este momento, la forma disponible para nosotros y ademas la mas simple y directa es simplemente mejorar el prompt"
      ],
      "metadata": {
        "id": "GkG3JiQuTRHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definimos las opciones de sentimiento\n",
        "class Sentimientos(Enum):\n",
        "    POSITIVO = \"positivo\"\n",
        "    NEGATIVO = \"negativo\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=Sentimientos)\n",
        "\n",
        "# Crear el prompt MUCHO MAS PRECISO !!!\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Clasifica el siguiente comentario: {comentario} Contesta solo en minusculas con 'positivo', 'negativo' o 'neutral y nada m√°s\",\n",
        "    input_variables=[\"comentario\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "\n",
        "# Ejemplo de uso\n",
        "comentario_cliente = \"Me encanta este producto, es incre√≠ble y funciona muy bien.\"\n",
        "\n",
        "prompt = prompt_template.format(comentario=comentario_cliente)\n",
        "\n",
        "# Clasificamos el sentimiento\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "try:\n",
        "    respuesta_formateada = output_parser.parse(respuesta)\n",
        "except Exception as e:\n",
        "    respuesta_formateada = \"No se pudo clasificar el sentimiento\"\n",
        "    rprint(f\"[bold white on red]Error al parsear la salida: {e}\")\n",
        "\n",
        "\n",
        "rprint(\"Sentimiento Clasificado:\")\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n"
      ],
      "metadata": {
        "id": "ryp8xq5tUte4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬ø Que pinta aqui pues el OutputParser ? Esto lo podriamos simplemente con el prompt. SI.  \n",
        "\n",
        "El OutputParser nos proporciona validacion de datos, que es importante sobre todo en el momento del desarrolo y tipo de datos. El resultado NO es un string, sino algo del tipo Enum que hemos definido que por ejemplo podemos iterar.\n",
        "\n",
        "https://rico-schmidt.name/pymotw-3/enum/index.html"
      ],
      "metadata": {
        "id": "eycDlLi1Vpn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. SimpleJsonOutputParser**\n",
        "---\n",
        "\n",
        "El SimpleJsonOutputParser es una herramienta de LangChain que se utiliza para convertir el texto de salida de un modelo de lenguaje en un objeto JSON estructurado. Es particularmente √∫til cuando necesitas obtener datos estructurados de tus LLMs (Large Language Models).\n",
        "\n",
        "### Funcionalidad principal\n",
        "\n",
        "- Convierte respuestas de texto en formato JSON\n",
        "- Maneja errores de parseo\n",
        "- Es sencillo de implementar en tu flujo de trabajo con LLMs"
      ],
      "metadata": {
        "id": "dW90EPu2eqQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo : Queremos obtener cierta informacion de una ciudad para procesarla posteriormente"
      ],
      "metadata": {
        "id": "6wQYvxFRkVbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import SimpleJsonOutputParser\n",
        "\n",
        "\n",
        "# Crear el parser\n",
        "parser = SimpleJsonOutputParser()\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Hacer una consulta que espera una respuesta estructurada\n",
        "# Importante aqui las dobles llaves para que no se interprete como variables\n",
        "template = \"\"\"\n",
        "Proporciona informaci√≥n sobre {ciudad} con el siguiente formato:\n",
        "\n",
        "{{\n",
        "  \"ciudad\": \"nombre de la ciudad\",\n",
        "  \"pais\": \"pa√≠s donde se encuentra\",\n",
        "  \"poblacion\": \"n√∫mero aproximado de habitantes\",\n",
        "  \"atracciones\": [\"lista\", \"de\", \"atracciones\", \"principales\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"ciudad\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "prompt = prompt_template.format(ciudad=\"Barcelona\")\n",
        "\n",
        "# Obtener la respuesta del modelo\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la respuesta a JSON\n",
        "resultado = parser.parse(respuesta)\n",
        "\n",
        "# Ahora resultado es un diccionario Python\n",
        "rprint(resultado)\n",
        "rprint(type(resultado))  # <class 'dict'>\n",
        "rprint(f\"Primera atracci√≥n: {resultado['atracciones'][0]}\")\n"
      ],
      "metadata": {
        "id": "ApLQw73De5XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo: Analisis de sentimientos (un poco m√°s elaborado)\n",
        "\n",
        "üëÄ No vamos a usar PromptTempplate y directamente nos las arreglamos con f-strings\n",
        "\n",
        "üëÄ El modelo devuleve la chachara habitual junto con el formato JSON solicitado. El parser es capaz de extraer de todo ello la infomacion JSON qu no interesa en el formato adecuado"
      ],
      "metadata": {
        "id": "M1e14pt6kv8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import SimpleJsonOutputParser\n",
        "\n",
        "# Crear el parser\n",
        "parser = SimpleJsonOutputParser()\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "\n",
        "# Comentario de un cliente sobre un producto\n",
        "comentario_cliente = \"\"\"\n",
        "Compr√© este tel√©fono hace un mes y estoy muy contento con la calidad de la c√°mara y la bater√≠a dura todo el d√≠a.\n",
        "Sin embargo, el software tiene algunos fallos y a veces se congela cuando uso m√∫ltiples aplicaciones al mismo tiempo.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para analizar el sentimiento y extraer insights\n",
        "prompt = f\"\"\"\n",
        "Analiza el siguiente comentario de un cliente y devuelve la informaci√≥n en formato JSON:\n",
        "\n",
        "Comentario: {comentario_cliente}\n",
        "\n",
        "El JSON debe tener esta estructura:\n",
        "{{\n",
        "  \"sentimiento_general\": \"positivo/negativo/neutral\",\n",
        "  \"puntuacion\": \"valor num√©rico de 1 a 10\",\n",
        "  \"aspectos_positivos\": [\"lista\", \"de\", \"aspectos\", \"positivos\"],\n",
        "  \"aspectos_negativos\": [\"lista\", \"de\", \"aspectos\", \"negativos\"],\n",
        "  \"recomendaciones\": [\"lista\", \"de\", \"posibles\", \"mejoras\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Obtener respuesta y parsear\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "respuestaf = parser.parse(respuesta)\n",
        "\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "rprint(\"\\n\\n\")\n",
        "rprint(respuestaf)\n",
        "rprint(type(respuestaf))\n",
        "rprint(\"\\n\\n\")\n",
        "\n",
        "# Usar los datos estructurados\n",
        "rprint(f\"Sentimiento: {respuestaf['sentimiento_general']}\")\n",
        "rprint(f\"Puntuaci√≥n: {respuestaf['puntuacion']}/10\")\n",
        "rprint(\"Aspectos positivos:\", \", \".join(respuestaf[\"aspectos_positivos\"]))\n",
        "rprint(\"Aspectos negativos:\", \", \".join(respuestaf[\"aspectos_negativos\"]))"
      ],
      "metadata": {
        "id": "7dUWXHIHlUY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. JsonOutputParser**\n",
        "---\n",
        "\n",
        "JsonOutputParser es otro OutputParser dissponible en el framewwork LangChain.\n",
        "\n",
        "## Principales diferencias\n",
        "\n",
        "1. **Complejidad y flexibilidad**:\n",
        "    - **SimpleJsonOutputParser**: Como su nombre indica, es m√°s simple. Toma una cadena de texto que contiene JSON v√°lido y la convierte en un objeto Python.\n",
        "    - **JsonOutputParser**: Es m√°s complejo y flexible, permitiendo definir un esquema espec√≠fico para la estructura JSON esperada.\n",
        "2. **Esquemas y validaci√≥n**:\n",
        "    - **SimpleJsonOutputParser**: No requiere definir un esquema previo; simplemente intenta parsear cualquier JSON v√°lido.\n",
        "    - **JsonOutputParser**: Requiere definir la estructura esperada, lo que proporciona validaci√≥n y gu√≠a al LLM sobre c√≥mo estructurar su respuesta.\n",
        "3. **Instrucciones al modelo**:\n",
        "    - **SimpleJsonOutputParser**: No genera instrucciones espec√≠ficas para el modelo.\n",
        "    - **JsonOutputParser**: Proporciona instrucciones detalladas al modelo sobre el formato requerido con `get_format_instructions()`.\n",
        "4. **Manejo de errores**:\n",
        "    - **SimpleJsonOutputParser**: Manejo b√°sico de errores de parseo.\n",
        "    - **JsonOutputParser**: Manejo m√°s robusto de errores con validaci√≥n contra el esquema definido.\n",
        "\n",
        "## ¬øCu√°ndo usar cada uno?\n",
        "\n",
        "**Usa SimpleJsonOutputParser cuando:**\n",
        "\n",
        "- Necesitas una soluci√≥n r√°pida y sencilla\n",
        "- La estructura JSON puede variar o no es cr√≠tica\n",
        "- No requieres validaci√≥n estricta del esquema\n",
        "\n",
        "**Usa JsonOutputParser cuando:**\n",
        "\n",
        "- Necesitas validar contra un esquema espec√≠fico\n",
        "- Quieres proporcionar instrucciones detalladas al modelo\n",
        "- La estructura de datos es compleja o cr√≠tica para tu aplicaci√≥n\n",
        "- Trabajas con tipos de datos espec√≠ficos que requieren validaci√≥n\n",
        "\n",
        "El JsonOutputParser es especialmente √∫til en escenarios empresariales donde la consistencia y validaci√≥n de los datos son cruciales, como en an√°lisis de productos, extracci√≥n de informaci√≥n de documentos, o procesamiento de datos estructurados desde texto libre."
      ],
      "metadata": {
        "id": "626Vr1c5uzj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo: Deseamos obtener la bibliografia de un autor\n",
        "\n",
        "üëÄ Observa lo importante que es el prompt para conseguir que el modelo cree una respuesta que pueda ser parseada por el OutputParser.\n",
        "\n",
        "El OutputParser ademas de validacion de datos nos propociona los datos en el tipo deseado y no una simple repesentacion en string. **Observa los tipos de de respuesta y respuesta_formateada**"
      ],
      "metadata": {
        "id": "3PzKccq3tXXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Definir la estructura para un libro\n",
        "class Libro(BaseModel):\n",
        "    titulo: str = Field(description=\"T√≠tulo del libro\")\n",
        "    a√±o: int = Field(description=\"A√±o de publicaci√≥n\")\n",
        "\n",
        "# Definir la estructura para la bibliograf√≠a completa\n",
        "class Bibliografia(BaseModel):\n",
        "    bibliografia: List[Libro] = Field(description=\"Lista de libros publicados por el autor\")\n",
        "\n",
        "# Crear el JsonOutputParser\n",
        "output_parser = JsonOutputParser(pydantic_model=Bibliografia)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "Proporciona √öNICAMENTE la bibliograf√≠a del autor: {autor}.\n",
        "\n",
        "INSTRUCCIONES ESTRICTAS:\n",
        "1. Sigue EXACTAMENTE el formato JSON especificado a continuaci√≥n.\n",
        "2. NO incluyas campos adicionales como nacionalidad, nacimiento, fallecimiento, premios, etc.\n",
        "3. Solo crea una lista de sus libros con t√≠tulo y a√±o.\n",
        "4. La lista de libros debe usar la clave \"bibliografia\", no \"obras\" ni ninguna otra.\n",
        "5. Cada libro debe tener SOLO los campos \"titulo\" y \"a√±o\".\n",
        "\n",
        "Es CR√çTICO seguir este formato exacto:\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "IMPORTANTE: No agregues ning√∫n campo que no est√© especificado en el esquema. Tu respuesta debe ser √∫nicamente un JSON v√°lido con la estructura exacta solicitada.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"autor\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "autor = \"Gabriel Garc√≠a M√°rquez\"\n",
        "prompt = prompt_template.format(autor=autor)\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Analizamos la respuesta\n",
        "try:\n",
        "    respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "    # Imprimimos los resultados\n",
        "    print(f\"Autor: {autor}\")\n",
        "    print(\"Bibliograf√≠a:\")\n",
        "    for libro in respuesta_formateada[\"bibliografia\"]:\n",
        "        print(f\"A√±o: {libro['a√±o']}, T√≠tulo: {libro['titulo']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al parsear la respuesta: {e}\")\n",
        "    print(\"La respuesta del modelo no cumple con el esquema esperado.\")"
      ],
      "metadata": {
        "id": "wgjgzWrSvjSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que el tipo de la respuesta obtenida es un diccionario de Python"
      ],
      "metadata": {
        "id": "lqPpK1M-1U4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Analisis de producto detallado**\n",
        "\n",
        "Dada un descripcion textual de un articulo, deseamos obtener un analisis detallado del producto con cierta estructura de datos\n"
      ],
      "metadata": {
        "id": "_xc1O4oFd8SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "# Definir el esquema que esperamos recibir\n",
        "class ProductoAnalisis(BaseModel):\n",
        "    nombre: str = Field(description=\"Nombre del producto analizado\")\n",
        "    categoria: str = Field(description=\"Categor√≠a principal del producto\")\n",
        "    ventajas: List[str] = Field(description=\"Lista de puntos fuertes o ventajas del producto\")\n",
        "    desventajas: List[str] = Field(description=\"Lista de puntos d√©biles o desventajas del producto\")\n",
        "    puntuacion: int = Field(description=\"Puntuaci√≥n de 1 a 10\")\n",
        "    recomendado: bool = Field(description=\"Si el producto es recomendable\")\n",
        "    mejor_para: List[str] = Field(description=\"Tipos de usuarios para los que este producto es m√°s adecuado\")\n",
        "\n",
        "# Crear el parser con nuestro esquema\n",
        "parser = JsonOutputParser(pydantic_model=ProductoAnalisis)\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Crear el template con las instrucciones de formato\n",
        "template = \"\"\"\n",
        "Analiza el siguiente producto y proporciona un an√°lisis detallado:\n",
        "\n",
        "Producto: {producto}\n",
        "\n",
        "Es CR√çTICO seguir este formato exacto:\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Debes realizar un an√°lisis del producto indicado. Presenta los resultados estrictamente siguiendo este esquema:\n",
        "\n",
        "nombre: (Nombre exacto del producto)\n",
        "\n",
        "categoria: (Categor√≠a principal a la que pertenece el producto)\n",
        "\n",
        "ventajas:\n",
        "\n",
        "(Lista claramente identificada de puntos fuertes o ventajas)\n",
        "\n",
        "desventajas:\n",
        "\n",
        "(Lista claramente identificada de puntos d√©biles o desventajas)\n",
        "\n",
        "puntuacion: (Valoraci√≥n num√©rica del producto entre 1 y 10, donde 10 es excelente)\n",
        "\n",
        "recomendado: (Indica expl√≠citamente \"S√≠\" o \"No\" dependiendo de si recomiendas el producto)\n",
        "\n",
        "mejor_para:\n",
        "\n",
        "(Lista concreta de perfiles o tipos de usuarios para los que este producto ser√≠a m√°s adecuado)\n",
        "\n",
        "Aseg√∫rate de completar cada secci√≥n de manera detallada y precisa, sin omitir ning√∫n campo.\n",
        "\n",
        "IMPORTANTE: No agregues ning√∫n campo que no est√© especificado en el esquema. Tu respuesta debe ser √∫nicamente un JSON v√°lido con la estructura exacta solicitada.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"producto\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Producto a analizar\n",
        "descripcion_producto = \"\"\"\n",
        "Auriculares inal√°mbricos XSound Pro - Con cancelaci√≥n activa de ruido,\n",
        "30 horas de bater√≠a, conexi√≥n Bluetooth 5.2, resistencia al agua IPX4,\n",
        "y sistema de micr√≥fono dual para llamadas. Precio: 129,99‚Ç¨\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format(producto=descripcion_producto)\n",
        "\n",
        "\n",
        "# Obtener y parsear la respuesta\n",
        "respuesta = modelo.invoke(prompt)\n",
        "respuestaf= parser.parse(respuesta.content)\n",
        "\n",
        "\n",
        "# Impresi√≥n estructurada\n",
        "print(f\"Nombre: {respuestaf['nombre']}\")\n",
        "print(f\"Categor√≠a: {respuestaf['categoria']}\\n\")\n",
        "\n",
        "print(\"Ventajas:\")\n",
        "for ventaja in respuestaf['ventajas']:\n",
        "    print(f\"- {ventaja}\")\n",
        "\n",
        "print(\"\\nDesventajas:\")\n",
        "for desventaja in respuestaf['desventajas']:\n",
        "    print(f\"- {desventaja}\")\n",
        "\n",
        "print(f\"\\nPuntuaci√≥n: {respuestaf['puntuacion']}/10\")\n",
        "print(f\"Recomendado: {respuestaf['recomendado']}\")\n",
        "\n",
        "print(\"\\nMejor para:\")\n",
        "for usuario in respuestaf['mejor_para']:\n",
        "    print(f\"- {usuario}\")"
      ],
      "metadata": {
        "id": "bLkXlzR64SNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. StructuredOutputParser**\n",
        "---\n",
        "StructuredOutputParser es un parser de salida estructurado pero sencillo pero a veces no necesitamos mucho mas.\n",
        "\n",
        "- **Prop√≥sito**: Generar respuestas estructuradas en formatos simples, ideal para modelos peque√±os o con restricciones.\n",
        "    \n",
        "- **Caracter√≠sticas clave**:  \n",
        "    ‚Ä¢ **Siempre devuelve un diccionario con campos de tipo _string_.**  \n",
        "    ‚Ä¢ Permite definir una estructura de salida predeterminada.  \n",
        "    ‚Ä¢ Menos complejo que opciones como _PydanticOutputParser_ (no soporta datos complejos).\n",
        "    \n",
        "- **Limitaciones**:  \n",
        "    ‚Ä¢ **Solo admite campos de texto (_string_).**  \n",
        "    ‚Ä¢ Menos flexible para escenarios que requieren tipos de datos avanzados.\n",
        "    \n",
        "- **Casos de uso**:  \n",
        "    ‚Ä¢ Proyectos con requisitos de estructura b√°sicos.  \n",
        "    ‚Ä¢ Compatibilidad con modelos de lenguaje menos potentes o entornos limitados.\n",
        "\n",
        "** Realmente puede manejar listas sencillas, pero es mejor evitarlo. Si debe haber listas en la respuesta, usa algun JSON *texto en cursiva*"
      ],
      "metadata": {
        "id": "4Zlw_DQiZPI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Crear datos dummy de usuarios**\n",
        "Observa que por las limitaciones de este pareser, que solo genera strings la edad sera un string que habra que convertir."
      ],
      "metadata": {
        "id": "hl1_LP0hxtcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "esquema_respuesta = [\n",
        "    ResponseSchema(name=\"nombre\", description=\"El nombre del usuario\"),\n",
        "    ResponseSchema(name=\"edad\", description=\"La edad del usuario\"),\n",
        "    ResponseSchema(name=\"email\", description=\"El correo electr√≥nico del usuario\")\n",
        "]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(esquema_respuesta)\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Genera informaci√≥n de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format()\n",
        "\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "rprint(\"\\n\")\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "rprint(\"\\n\")\n",
        "\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Nombre: {respuesta_formateada['nombre']}\")\n",
        "rprint(f\"Edad: {int(respuesta_formateada['edad'])}\")\n",
        "rprint(f\"Email: {respuesta_formateada['email']}\")\n"
      ],
      "metadata": {
        "id": "F4pnLdNTZP9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1_T9nPdbCIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Creacion de un test**\n",
        "---\n",
        "\n",
        "Vamos a dise√±ar una consulta para un modelo de lenguaje (LLM) que sirva como base para generar un test de preguntas sobre un tema espec√≠fico. Proporcionaremos el tema y un nivel de dificultad (bajo, medio, alto), y el LLM deber√° generar el texto de la pregunta, tres opciones de respuesta y el √≠ndice de la respuesta correcta. La respuesta debe estar estructurada en un diccionario con el siguiente formato:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"pregunta\": \"Texto de la pregunta\",\n",
        "    \"opciones\": [\"Opci√≥n 1\", \"Opci√≥n 2\", \"Opci√≥n 3\"],\n",
        "    \"respuesta_correcta\": √≠ndice_de_la_opci√≥n_correcta\n",
        "}\n",
        "```\n",
        "\n",
        "Este formato permitir√° una f√°cil interpretaci√≥n y uso de la pregunta generada.\n",
        "\n",
        "Como no estamos seguros de la capcidad de este OutputParser de generar listas, las opciones seran tres campos string independientes.\n"
      ],
      "metadata": {
        "id": "4-C1hmhItipv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "esquema_respuesta = [\n",
        "            ResponseSchema(name=\"pregunta\", description=\"Texto de la pregunta generada.\"),\n",
        "            ResponseSchema(name=\"opcion1\", description=\"Primera opci√≥n de respuesta.\"),\n",
        "            ResponseSchema(name=\"opcion2\", description=\"Segunda opci√≥n de respuesta.\"),\n",
        "            ResponseSchema(name=\"opcion3\", description=\"Tercera opci√≥n de respuesta.\"),\n",
        "            ResponseSchema(name=\"respuesta_correcta\", description=\"√çndice de la opci√≥n correcta (1, 2 o 3).\")\n",
        "                    ]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(esquema_respuesta)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "plantilla = \"\"\"\n",
        "        Genera una pregunta de test sobre el tema: {tema}.\n",
        "        El nivel de dificultad debe ser: {nivel}.\n",
        "        Genera tambien tres posibles respuestas (opcion1, opcion2, opcion3)\n",
        "        Una de ellas debe ser la correcta\n",
        "        Indica el numero (1,2,3) de la respuesta correcta (respuesta_correcta)\n",
        "\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=plantilla,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Pregunta: {respuesta_formateada['pregunta']}\")\n",
        "rprint(f\"Opci√≥n 1: {respuesta_formateada['opcion1']}\")\n",
        "rprint(f\"Opci√≥n 2: {respuesta_formateada['opcion2']}\")\n",
        "rprint(f\"Opci√≥n 3: {respuesta_formateada['opcion3']}\")\n",
        "rprint(f\"Respuesta correcta: Opci√≥n {respuesta_formateada['respuesta_correcta']}\")"
      ],
      "metadata": {
        "id": "Mj2h1zwYhedp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Creacion de un test II**\n",
        "---\n",
        "Vamos a comprobar que el StructuredOutputParser puede gestionar listas sencillas con exito.  \n",
        "üëÄ **Observa como el diccionario devuelto por el parser, contiene una lista de strings con las preguntas**\n"
      ],
      "metadata": {
        "id": "5pPpLoGbKOlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "# Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "# Pero ahora usamos una lista, al menos asi lo indicamos en el texto de la descripcion\n",
        "response_schemas = [\n",
        "            ResponseSchema(name=\"pregunta\", description=\"Texto de la pregunta generada.\"),\n",
        "            ResponseSchema(name=\"opciones\", description=\"LISTA de tres opciones de respuesta.\"),\n",
        "            ResponseSchema(name=\"respuesta_correcta\", description=\"√çndice de la opci√≥n correcta (1, 2 o 3).\")\n",
        "                    ]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "        Genera una pregunta de test sobre el tema: {tema}.\n",
        "        El nivel de dificultad debe ser: {nivel}.\n",
        "        La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Pregunta: {respuesta_formateada['pregunta']}\")\n",
        "rprint(\"Opciones:\")\n",
        "for i, opcion in enumerate(respuesta_formateada['opciones'], start=1):\n",
        "    rprint(f\"{opcion}\")\n",
        "rprint(f\"Respuesta correcta: Opci√≥n {respuesta_formateada['respuesta_correcta']}\")"
      ],
      "metadata": {
        "id": "MNxAZq7RtiNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Extracci√≥n de datos en anuncios inmobiliarios**\n",
        "\n",
        "Disponemos del texto de un anuncio inmobiliario y desamos extraer la informacion de forma estructurada."
      ],
      "metadata": {
        "id": "mB7k_8tJk6rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "\n",
        "# Definir los esquemas para la extracci√≥n de datos inmobiliarios\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"tipo_propiedad\", description=\"Tipo de propiedad (apartamento, casa, chalet, etc.)\"),\n",
        "    ResponseSchema(name=\"precio\", description=\"Precio de la propiedad en formato num√©rico sin s√≠mbolos\"),\n",
        "    ResponseSchema(name=\"moneda\", description=\"Moneda del precio (EUR, USD, etc.)\"),\n",
        "    ResponseSchema(name=\"superficie\", description=\"Superficie en metros cuadrados, solo n√∫mero\"),\n",
        "    ResponseSchema(name=\"habitaciones\", description=\"N√∫mero de habitaciones, solo n√∫mero\"),\n",
        "    ResponseSchema(name=\"ba√±os\", description=\"N√∫mero de ba√±os, solo n√∫mero\"),\n",
        "    ResponseSchema(name=\"ubicacion\", description=\"Ubicaci√≥n de la propiedad (barrio, ciudad)\"),\n",
        "    ResponseSchema(name=\"caracteristicas\", description=\"Lista de caracter√≠sticas destacadas de la propiedad\"),\n",
        "    ResponseSchema(name=\"estado\", description=\"Estado de la propiedad (nuevo, reformado, a reformar, etc.)\"),\n",
        "]\n",
        "\n",
        "# Crear el parser\n",
        "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Obtener instrucciones de formato\n",
        "format_instructions = parser.get_format_instructions()\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Texto del anuncio inmobiliario\n",
        "descripcion_inmueble = \"\"\"\n",
        "Magn√≠fico piso de 95 m¬≤ en el coraz√≥n de Salamanca. Consta de 3 dormitorios, 2 ba√±os completos,\n",
        "cocina equipada y amplio sal√≥n con balc√≥n. La propiedad ha sido recientemente reformada con\n",
        "materiales de alta calidad. Dispone de calefacci√≥n central, aire acondicionado, suelos de parquet,\n",
        "armarios empotrados y plaza de garaje incluida. Edificio con ascensor y servicio de porter√≠a.\n",
        "Excelente ubicaci√≥n cerca de todos los servicios, comercios y transporte p√∫blico.\n",
        "Precio: 450.000‚Ç¨. Gastos de comunidad: 150‚Ç¨/mes.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = f\"\"\"\n",
        "Extrae la informaci√≥n clave del siguiente anuncio inmobiliario:\n",
        "\n",
        "{descripcion_inmueble}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "# Obtener la respuesta\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "print(respuesta)\n",
        "\n",
        "# Parsear la respuesta\n",
        "propiedades = parser.parse(respuesta)\n",
        "\n",
        "rprint(propiedades)\n",
        "rprint(type(propiedades))\n",
        "\n",
        "# Imprimir los resultados de forma estructurada\n",
        "rprint(f\"[bold spring_green3]Tipo: {propiedades['tipo_propiedad']}\")\n",
        "rprint(f\"[bold spring_green3]Precio: {propiedades['precio']} {propiedades['moneda']}\")\n",
        "rprint(f\"[bold spring_green3]Superficie: {propiedades['superficie']} m¬≤\")\n",
        "rprint(f\"[bold spring_green3]Habitaciones: {propiedades['habitaciones']}\")\n",
        "rprint(f\"[bold spring_green3]Ba√±os: {propiedades['ba√±os']}\")\n",
        "rprint(f\"[bold spring_green3]Ubicaci√≥n: {propiedades['ubicacion']}\")\n",
        "rprint(f\"[bold spring_green3]Estado: {propiedades['estado']}\")\n",
        "rprint(f\"[bold spring_green3]Caracteristicas: {propiedades['caracteristicas']}\")\n"
      ],
      "metadata": {
        "id": "I9bShLJahail"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. PydanticOutputParser**\n",
        "---\n",
        "Convierte la salida a un modelo de datos Pydantic.\n",
        "Pydantic es una poderosa biblioteca en Python dise√±ada para la validaci√≥n y gesti√≥n de datos. Es especialmente √∫til cuando trabajas con datos que necesitan ser validados y transformados, como datos de entrada de API o formularios. Esta incluida como una dependencia en Langchain.\n",
        "\n",
        "El `PydanticOutputParser` es una herramienta avanzada de LangChain que combina el poder de la validaci√≥n de Pydantic con la capacidad de estructurar las salidas de los modelos de lenguaje. A diferencia del `SimpleJsonOutputParser` o el `StructuredOutputParser`, este parser utiliza modelos Pydantic completos para definir esquemas de datos rigurosos.\n",
        "\n",
        "## Caracter√≠sticas principales\n",
        "\n",
        "- Utiliza modelos Pydantic para definir la estructura de datos esperada\n",
        "- Proporciona validaci√≥n de tipos robusta\n",
        "- Soporta esquemas de datos complejos y anidados\n",
        "- Genera instrucciones detalladas para guiar al modelo de lenguaje\n",
        "- Convierte autom√°ticamente la respuesta en una instancia del modelo Pydantic\n",
        "\n",
        "## Funcionamiento\n",
        "\n",
        "1. Se define un modelo Pydantic que represente la estructura de datos deseada\n",
        "2. Se crea un `PydanticOutputParser` basado en ese modelo\n",
        "3. Se generan instrucciones para el modelo de lenguaje\n",
        "4. Se parsea la respuesta para obtener una instancia del modelo Pydantic\n",
        "\n",
        "Si buscas simplicidad y rapidez, StructuredOutputParser es una buena opci√≥n pero si necesitas validaci√≥n de datos robusta y est√°s utilizando Pydantic en tu proyecto, PydanticOutputParser es la mejor alternativa.\n",
        "\n"
      ],
      "metadata": {
        "id": "4NbToCP9tt-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Creacion de un test III**\n",
        "\n",
        "Vamos a crear de nuevo el ejemplo que genera una pregunta de test, pero esta vez con el PydanticOutputParser"
      ],
      "metadata": {
        "id": "qmyX9q0lNZwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Definimos el modelo Pydantic para la respuesta\n",
        "class PreguntaTest(BaseModel):\n",
        "    pregunta: str = Field(description=\"Texto de la pregunta generada.\")\n",
        "    opciones: List[str] = Field(description=\"Lista de tres opciones de respuesta.\")\n",
        "    respuesta_correcta: int = Field(description=\"√çndice de la opci√≥n correcta (0, 1 o 2).\")\n",
        "\n",
        "# Crear el PydanticOutputParser\n",
        "output_parser = PydanticOutputParser(pydantic_object=PreguntaTest)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "plantilla = \"\"\"\n",
        "Genera una pregunta de test sobre el tema: {tema}.\n",
        "El nivel de dificultad debe ser: {nivel}.\n",
        "La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=plantilla,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parseamos la respuesta fon el parser de salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Pregunta: {respuesta_formateada.pregunta}\")\n",
        "rprint(\"Opciones:\")\n",
        "# La funci√≥n enumerate toma un iterable (como una lista) y devuelve un objeto que genera pares de valores\n",
        "# start=1 , para que el 0 se considere 1\n",
        "for i, opcion in enumerate(respuesta_formateada.opciones):\n",
        "    rprint(f\"{i+1}. {opcion}\")\n",
        "\n",
        "rprint(f\"Respuesta correcta: Opci√≥n {respuesta_formateada.respuesta_correcta + 1}\")\n"
      ],
      "metadata": {
        "id": "dEJ_Wd1Lt0Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Informe m√©dico.**\n",
        "Veamos otro ejemplo con Pydantic. Dado un informe medico de un paciente, deseamos extraer de forma estructurada la informacion que contiene\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BnYJ6p9zt4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# Modelo Pydantic para representar un informe m√©dico estructurado\n",
        "class MedicalReport(BaseModel):\n",
        "    id_paciente: str = Field(description=\"Identificador √∫nico del paciente\")\n",
        "    diagnosticos: List[str] = Field(description=\"Lista de diagn√≥sticos principales\")\n",
        "    signos_vitales: dict = Field(description=\"Signos vitales como temperatura, presi√≥n arterial, etc.\")\n",
        "    medicaciones: List[dict] = Field(description=\"Lista de medicamentos con nombre, dosis y frecuencia\")\n",
        "    recomendaciones: List[str] = Field(description=\"Recomendaciones m√©dicas para el paciente\")\n",
        "    fecha_seguimiento: Optional[str] = Field(description=\"Fecha recomendada para seguimiento (formato YYYY-MM-DD)\")\n",
        "\n",
        "# Crear el parser\n",
        "parser = PydanticOutputParser(pydantic_object=MedicalReport)\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0)\n",
        "\n",
        "# Texto del informe m√©dico no estructurado\n",
        "informe_medico = \"\"\"\n",
        "Paciente: P12345\n",
        "Fecha de consulta: 15/03/2025\n",
        "Motivo de consulta: Dolor abdominal y fiebre de 3 d√≠as de evoluci√≥n.\n",
        "Examen f√≠sico: Temperatura 38.2¬∞C, Presi√≥n arterial 130/85, Frecuencia card√≠aca 95 lpm, Saturaci√≥n O2 98%.\n",
        "Abdomen doloroso a la palpaci√≥n en cuadrante inferior derecho.\n",
        "Diagn√≥stico: Apendicitis aguda. Deshidrataci√≥n leve.\n",
        "Tratamiento: Ceftriaxona 1g IV cada 12 horas, Metronidazol 500mg IV cada 8 horas,\n",
        "Paracetamol 1g VO cada 8 horas si fiebre o dolor.\n",
        "Plan: Programar cirug√≠a. Hidrataci√≥n intravenosa. Control de signos vitales.\n",
        "Recomendaciones: Dieta l√≠quida, reposo absoluto, vigilar cambios en el dolor o aparici√≥n de fiebre.\n",
        "Pr√≥xima cita: 10 de abril de 2025.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = f\"\"\"\n",
        "Extrae y estructura la informaci√≥n del siguiente informe m√©dico:\n",
        "\n",
        "{informe_medico}\n",
        "\n",
        "{parser.get_format_instructions()}\n",
        "\"\"\"\n",
        "\n",
        "# Obtener la respuesta\n",
        "respuesta = modelo.invoke(prompt)\n",
        "# Parsear la respuesta (esto devuelve una instancia de MedicalReport)\n",
        "informe_estructurado = parser.parse(respuesta.content)\n",
        "\n",
        "rprint(respuesta.content)\n",
        "rprint(type(respuesta.content))\n",
        "\n",
        "rprint(informe_estructurado)\n",
        "rprint(type(informe_estructurado))\n",
        "\n",
        "\n",
        "\n",
        "# Usar la instancia\n",
        "print(f\"ID del paciente: {informe_estructurado.id_paciente}\")\n",
        "print(f\"Diagn√≥sticos: {', '.join(informe_estructurado.diagnosticos)}\")\n",
        "print(\"Signos vitales:\")\n",
        "for signo, valor in informe_estructurado.signos_vitales.items():\n",
        "    print(f\"  - {signo}: {valor}\")\n",
        "print(\"Medicaciones:\")\n",
        "for med in informe_estructurado.medicaciones:\n",
        "    # Access keys using the names provided by the LLM: 'name', 'dose', 'frequency'\n",
        "    print(f\"  - {med['nombre']} {med['dosis']} {med['frecuencia']}\")\n",
        "\n",
        "print(f\"Fecha de seguimiento: {informe_estructurado.fecha_seguimiento}\")"
      ],
      "metadata": {
        "id": "cM25ELTDO9tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que la clase del objeto devuelto NO es un diccionario, ni un JSON sino un objeto heredado de la clase que hemos definido con Pydantic !!\n"
      ],
      "metadata": {
        "id": "dnHhQCe_Yb1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. with_structured_output()**\n",
        "\n",
        "Vamos a realizar algunos de los ejemplos anteriores, pero esta vez usando las capacidades modernas de los modelos para producir de forma nativa una salida estructurada, con lo cual podemos dejar de usar OutputParsers"
      ],
      "metadata": {
        "id": "w_bCk2tw8NZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Ejemplo: Bibliografia de un autor\n",
        "\n",
        "Este caso los resolvimos anteriormente con JSONOuputParser y tambien con PydanticOutputParser.\n",
        "\n",
        "Necesitamos de Pydantic todavia para validaciones e instruir al modelo de una forma mucho mas precisa que si tuviaremos que hacer toda la descripcion de campos del diccionario esparado en un prompt"
      ],
      "metadata": {
        "id": "abxLCiidXGSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Definir la estructura para un libro\n",
        "class Libro(BaseModel):\n",
        "    titulo: str = Field(description=\"T√≠tulo del libro\")\n",
        "    a√±o: int = Field(description=\"A√±o de publicaci√≥n\")\n",
        "\n",
        "# Definir la estructura para la bibliograf√≠a completa\n",
        "class Bibliografia(BaseModel):\n",
        "    bibliografia: List[Libro] = Field(description=\"Lista de libros publicados por el autor\")\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0.7)\n",
        "\n",
        "# Configuramos el modelo para que devuelva directamente una instancia de Bibliografia\n",
        "modelo_struct = modelo.with_structured_output(Bibliografia)\n",
        "\n",
        "# Ejemplo de uso\n",
        "autor = \"Gabriel Garc√≠a M√°rquez\"\n",
        "prompt = f\"\"\"\n",
        "Proporciona √öNICAMENTE la bibliograf√≠a del autor: {autor}.\n",
        "INSTRUCCIONES ESTRICTAS:\n",
        "1. Solo crea una lista de sus libros con t√≠tulo y a√±o.\n",
        "2. La lista de libros debe usar la clave \"bibliografia\".\n",
        "3. Cada libro debe tener SOLO los campos \"titulo\" y \"a√±o\".\n",
        "\"\"\"\n",
        "\n",
        "# Hacemos la llamada directamente al modelo estructurado\n",
        "resultado = modelo_struct.invoke(prompt)\n",
        "\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Autor: {autor}\")\n",
        "print(\"Bibliograf√≠a:\")\n",
        "for libro in resultado.bibliografia:\n",
        "    print(f\"A√±o: {libro.a√±o}, T√≠tulo: {libro.titulo}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "52GRpA5uXEhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß™ Ejemplo: Creacion de un test**\n",
        "\n",
        "Este ejemplo lo hemos realizado con tres OutputParsers diferentes.\n",
        "Ahora con  with_structured_output(). Es realmente facil y fiable."
      ],
      "metadata": {
        "id": "mXb5idU4XNqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Definimos un modelo Pydantic en lugar de ResponseSchema\n",
        "class PreguntaTest(BaseModel):\n",
        "    pregunta: str = Field(description=\"Texto de la pregunta generada.\")\n",
        "    opciones: List[str] = Field(description=\"Lista de tres opciones de respuesta.\")\n",
        "    respuesta_correcta: int = Field(description=\"√çndice de la opci√≥n correcta (1, 2 o 3).\")\n",
        "\n",
        "# Instanciamos el modelo (comenta el que no desees usar)\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "modelo = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY,temperature=0.7)\n",
        "\n",
        "# Configuramos el modelo para que devuelva directamente una instancia de QuizQuestion\n",
        "modelo_struct = modelo.with_structured_output(PreguntaTest)\n",
        "\n",
        "# Ejemplo de uso\n",
        "tema = \"LangChain\"\n",
        "nivel = \"medio\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "    Genera una pregunta de test sobre el tema: {tema}.\n",
        "    El nivel de dificultad debe ser: {nivel}.\n",
        "    La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "    \"\"\"\n",
        "\n",
        "# Hacemos la llamada directamente al modelo estructurado\n",
        "resultado = modelo_struct.invoke(prompt)\n",
        "rprint(resultado)\n",
        "rprint(type(resultado))\n",
        "\n",
        "# Mostramos los resultados\n",
        "# Como la respuesta es un objeto Pydantic (no un dict) usamos la notacu√≥n .\n",
        "print(f\"Pregunta: {resultado.pregunta}\")\n",
        "print(\"Opciones:\")\n",
        "for i, opcion in enumerate(resultado.opciones, start=1):\n",
        "    print(f\"{i}. {opcion}\")\n",
        "print(f\"Respuesta correcta: Opci√≥n {resultado.respuesta_correcta}\")\n",
        "print(\"\\n\\n\")\n",
        "# Si necesitas acceder como diccionario, lo pasamos a dict con .dump\n",
        "resultado_dict = resultado.model_dump()\n",
        "rprint(type(resultado_dict))\n"
      ],
      "metadata": {
        "id": "1McIY_TpWj36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **13. Ejercicios**\n",
        "---"
      ],
      "metadata": {
        "id": "oCLrXzigJyHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üë®üèª‚Äçüè´ Ejercicio 3.1: Extractor de Informaci√≥n de Pel√≠culas**\n",
        "\n",
        "**Objetivo**: Crear un sistema que extraiga informaci√≥n estructurada sobre pel√≠culas a partir de una consulta simple.\n",
        "\n",
        "**Descripci√≥n**: Implementa un programa utilizando LangChain y `StructuredOutputParser` que tome como entrada el nombre de una pel√≠cula y genere una ficha t√©cnica con informaci√≥n relevante como t√≠tulo, director, g√©nero, a√±o, puntuaci√≥n y resumen.\n",
        "\n",
        "**Requisitos**:\n",
        "\n",
        "1. Utilizar `ResponseSchema` para definir la estructura de los campos a extraer\n",
        "2. Implementar un `StructuredOutputParser` para procesar la respuesta del modelo\n",
        "3. Crear un `PromptTemplate` que incluya las instrucciones de formato\n",
        "4. Integrar el sistema con un modelo de lenguaje (ChatOpenAI o ChatGroq)\n",
        "5. Procesar y mostrar la informaci√≥n de manera estructurada\n",
        "\n",
        "**Entregable**: Un script de Python que, al ejecutarse con el nombre de una pel√≠cula como entrada, genere una ficha t√©cnica estructurada con la informaci√≥n solicitada.\n",
        "\n",
        "**Nivel**: Medio\n",
        "\n",
        "**Aplicaci√≥n pr√°ctica**: Este sistema puede ser √∫til para crear bases de datos de pel√≠culas, generar fichas t√©cnicas autom√°ticas o construir sistemas de recomendaci√≥n basados en atributos espec√≠ficos de las pel√≠culas.\n",
        "\n",
        "### Resuelve el ejerciccio en este cuaderno. Una solucion la puedes encontrar en el repositorio de esta serie de cuadernos.\n",
        "https://github.com/juanfranbrv/curso-langchain"
      ],
      "metadata": {
        "id": "vKVE9E5QJEmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe tu c√≥digo aqu√≠\n"
      ],
      "metadata": {
        "id": "9uvJFFGTKQvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üë®üèª‚Äçüè´ **Ejercicio 3.2: Extractor de informaci√≥n personal de un texto**\n",
        "\n",
        "**Objetivo**:  \n",
        "Crear un programa que **extraiga informaci√≥n estructurada** de un texto utilizando `with_structured_output()`. La informaci√≥n a extraer incluye:\n",
        "\n",
        "- Nombre completo\n",
        "    \n",
        "- Edad (n√∫mero entero)\n",
        "    \n",
        "- Poblaci√≥n (ciudad o pa√≠s)\n",
        "    \n",
        "- Correo electr√≥nico\n",
        "    \n",
        "- Lista de informaci√≥n adicional\n",
        "\n",
        "\n",
        "**Requerimientos**:\n",
        "\n",
        "1. **Esquema Pydantic**:  \n",
        "    Define una clase `InformacionExtraida` usando `pydantic.BaseModel` con los campos solicitados y sus descripciones.\n",
        "    \n",
        "2. **Prompt Template**:  \n",
        "    Crea un prompt que indique al modelo de lenguaje qu√© informaci√≥n extraer y en qu√© formato.\n",
        "    \n",
        "3. **Modelo con salida estructurada**:  \n",
        "    Usa `with_structured_output()` para configurar el modelo y garantizar que la salida cumpla con el esquema definido.\n",
        "    \n",
        "4. **Pruebas**:  \n",
        "    Ejecuta el programa con el texto de ejemplo y muestra los resultados estructurados.\n",
        "\n",
        "### Resuelve el ejercicio en este cuaderno. Una solucion la puedes encontrar en el repositorio de esta serie de cuadernos.\n",
        "https://github.com/juanfranbrv/curso-langchain"
      ],
      "metadata": {
        "id": "vtA1eMcAOiPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe tu c√≥digo aqu√≠\n"
      ],
      "metadata": {
        "id": "uUVZekza1nRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üë®üèª‚Äçüè´  **Ejercicio 3.3: Extracci√≥n de Recetas Estructuradas**\n",
        "\n",
        "#### **Objetivo**\n",
        "\n",
        "Crear un script en Python que:\n",
        "\n",
        "1. Extraiga el texto de una p√°gina web de recetas.\n",
        "    \n",
        "2. Utilice un modelo de lenguaje (LLM) para estructurar la informaci√≥n en un formato espec√≠fico.\n",
        "    \n",
        "3. Muestre los resultados de forma organizada usando Pydantic.\n",
        "\n",
        "\n",
        "### **Requerimientos**\n",
        "\n",
        "1. **Extracci√≥n de texto web**:\n",
        "    \n",
        "    - Usar `requests` y `BeautifulSoup` para obtener el texto crudo de una URL de receta.\n",
        "        \n",
        "        \n",
        "2. **Modelado de datos con Pydantic**:\n",
        "    \n",
        "    - Definir 3 clases:\n",
        "        \n",
        "        - `Ingrediente` (nombre y cantidad).\n",
        "            \n",
        "        - `Paso` (n√∫mero y descripci√≥n).\n",
        "            \n",
        "        - `Receta` (t√≠tulo, lista de ingredientes, lista de pasos).\n",
        "            \n",
        "3. **Prompt Engineering**:\n",
        "    \n",
        "    - Crear un prompt que indique al LLM extraer: t√≠tulo, ingredientes y pasos de la receta.\n",
        "        \n",
        "4. **Configuraci√≥n del LLM**:\n",
        "    \n",
        "    - Usar `with_structured_output()` para garantizar que la salida del modelo coincida con el esquema `Receta`.\n",
        "        \n",
        "5. **Pruebas y visualizaci√≥n**:\n",
        "    \n",
        "    - Mostrar el resultado estructurado y acceder a sus campos (ej: `resultado.titulo`).\n",
        "\n",
        "  \n",
        "     \n",
        "     \n",
        "### Resuelve el ejercicio en este cuaderno. Una solucion la puedes encontrar en el repositorio de esta serie de cuadernos.\n",
        "https://github.com/juanfranbrv/curso-langchain"
      ],
      "metadata": {
        "id": "MV0EDGiPcjeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install requests beautifulsoup4 -qU\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.bonviveur.es/recetas/coles-de-bruselas-en-freidora-de-aire\"\n",
        "url = \"https://www.directoalpaladar.com/postres/tarta-mango-postre-perfecto-para-cualquier-ocasion\"\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "texto_raw= soup.get_text()\n",
        "\n",
        "# =====================\n",
        "\n",
        "# Escribe tu c√≥digo aqu√≠"
      ],
      "metadata": {
        "id": "AQif1bvacil_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **14. Referencias:**\n",
        "\n",
        "1. https://freedium.cfd/https://python.plainenglish.io/langchain-in-chains-7-output-parsers-e1a2cdd40cd3\n",
        "\n",
        "2. https://medium.com/@juanc.olamendy/parsing-llm-structured-outputs-in-langchain-a-comprehensive-guide-f05ffa88261f\n",
        "\n",
        "3. https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834\n",
        "\n",
        "4. https://cobusgreyling.medium.com/langchain-structured-output-parser-using-openai-c3fe6927beb7\n",
        "\n",
        "5. https://python.langchain.com/docs/how_to/output_parser_structured/\n",
        "\n",
        "6. https://www.comet.com/site/blog/mastering-output-parsing-in-langchain/\n",
        "\n",
        "7. https://www.gettingstarted.ai/how-to-langchain-output-parsers-convert-text-to-objects/\n",
        "\n",
        "8. https://www.gettingstarted.ai/how-to-extract-metadata-from-pdf-convert-to-json-langchain/  Este es un buen reto\n",
        "\n",
        "9. https://www.analyticsvidhya.com/blog/2024/11/output-parsers/\n"
      ],
      "metadata": {
        "id": "lo85c5QHb66h"
      }
    }
  ]
}