{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGCDhXXny0GoZzk3VjMC4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/3.%20Analizadores%20de%20salida%20(Output%20parsers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Output parsers en Langchain**\n",
        "---\n",
        "Imagina que le preguntas a un LLM \"¬øCu√°les son los tres planetas m√°s cercanos al sol?\" y te responde: \"Mercurio, Venus y la Tierra son los planetas m√°s cercanos al sol\". Si bien la respuesta es correcta para un humano, para que tu programa pueda usar esa informaci√≥n, lo ideal ser√≠a tenerla en un formato m√°s manejable, como una lista o un objeto JSON. Aqu√≠ es donde entran en juego los Output Parsers.  \n",
        "\n",
        "Los Output Parsers te permiten ‚Äúforzar‚Äù o ‚Äúguiar‚Äù al modelo para que devuelva la informaci√≥n seg√∫n un formato deseado (por ejemplo, un JSON con campos espec√≠ficos, una lista, etc).  \n",
        "\n",
        "Convertir el texto libre, en informaci√≥n organizada, en algo que puedes usar directamente en tu c√≥digo.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Sin output parser: texto plano\n",
        "\"Tom Hanks ha actuado en Forrest Gump y Saving Private Ryan\"\n",
        "\n",
        "# Con output parser: estructura definida\n",
        "{\n",
        "  \"actor\": \"Tom Hanks\",\n",
        "  \"peliculas\": [\"Forrest Gump\", \"Saving Private Ryan\"]\n",
        "}\n",
        "```\n",
        "\n",
        "La estructuras de datos de salida son entre otras:\n",
        "\n",
        "- Objetos JSON\n",
        "- Modelos Pydantic\n",
        "- Listas\n",
        "- Diccionarios\n",
        "- Enumeraciones\n",
        "\n",
        "\n",
        "LangChain ofrece una variedad de Output Parsers preconstruidos para diferentes necesidades. Algunos de los mas usados son estos:\n",
        "\n",
        "- **StrOutputParser:** Convierte la salida a string\n",
        "\n",
        "- **CommaSeparatedListOutputParser**: Convierte la salida en una lista separada por comas. √ötil para generar listas de elementos\n",
        "\n",
        "- **EnumOutputParser**: Restringe la salida a un conjunto predefinido de valores. Perfecto para categor√≠as o estados limitados. Idela cuedo se desea que el LLM elija de un conjunto de opciones.\n",
        "\n",
        "- **JsonOutputParser** : Transforma la salida directamente en formato JSON. Ideal para respuestas estructuradas simples. Dos variantes principales:\n",
        "\n",
        "    - SimpleJsonOutputParser\n",
        "    - JsonOutputParser\n",
        "\n",
        "- **DatetimeOutputParser**: Extrae y formatea fechas y horas. √ötil para parsear informaci√≥n temporal\n",
        "\n",
        "- **StructuredOutputParser**: Permite definir esquemas de salida m√°s complejos. Configurable con m√∫ltiples campos\n",
        "\n",
        "- **PydanticOutputParser**: Convierte la salida en objetos Pydantic. Permite definir estructuras de datos complejas. Gran flexibilidad para validaci√≥n\n",
        "\n",
        "- **OutputFixingParser**: Intenta corregir salidas mal formateadas. √ötil cuando el modelo no genera la estructura perfecta.\n",
        "\n",
        "\n",
        "Puedes ver todos los OutputParsers disponibles aqu√≠:\n",
        "https://python.langchain.com/docs/concepts/output_parsers/\n",
        "\n",
        "\n",
        "# `with_structured_output`\n",
        "\n",
        "Es un m√©todo nativo que aprovecha capacidades del modelo. Utiliza capacidades de llamada de funci√≥n (function calling) del modelo. Es m√°s eficiente y preciso. Esta soportado por modelos avanzados como OpenAI, Anthropic, Groq.\n",
        "\n",
        "Lo trataremos al final de este cuaderno.\n",
        "\n",
        "‚òùüèª La mayor parte de modelos soportan esta funci√≥n y es el futuro de la extracci√≥n estructurada en LangChain. Prior√≠zalo cuando puedas.\n",
        "\n",
        "Puede consultarse una lista de modelos y sus capacidades aqu√≠:\n",
        "https://python.langchain.com/docs/integrations/chat/\n",
        "\n",
        "\n",
        "\n",
        "Crear un ejemplo que dada una receta la presente estructurada en ingrdientes, pasos, etc\n",
        "Esta en este video https://www.youtube.com/watch?v=lbWxastyWPw\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry8P2gp7VSGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**0. Preparando el entorno del cuaderno**\n",
        "----\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "- Instalamos la librer√≠a LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases espec√≠ficas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dej√°ndolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n"
      ],
      "metadata": {
        "id": "KEWPJUQdCird"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librer√≠as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Instalamos Rich para mejorar la salida\n",
        "%pip install rich -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos las librerias para formatear mejor la salida\n",
        "from IPython.display import Markdown, display\n",
        "from rich import print as rprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **StrOutputParser**\n",
        "---\n",
        "\n",
        "`StrOutputParser` es el output parser m√°s simple de LangChain. Su funci√≥n principal es convertir la salida del modelo de lenguaje directamente en una cadena de texto sin realizar ninguna transformaci√≥n estructural.\n",
        "\n",
        "Caracter√≠sticas principales:\n",
        "- Convierte la salida del modelo a texto plano\n",
        "- No realiza ninguna validaci√≥n o estructuraci√≥n\n",
        "- √ötil cuando solo necesitas el texto sin procesar\n",
        "- Muy ligero y directo\n",
        "\n",
        "Casos de uso tipicos: Resumenes, traducciones simples, ..."
      ],
      "metadata": {
        "id": "Zer1dYqJ4EEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo: Generador de Res√∫menes Ejecutivos\n",
        "\n",
        "A partir de un ejemplo de ventas (simulado) deseamos obtener un informe sobre el mismo.\n",
        "\n"
      ],
      "metadata": {
        "id": "J6qyHDnsQkWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Configuramos el modelo\n",
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"qwen-qwq-32b\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# Creamos un prompt para generar un resumen ejecutivo\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente experto en crear res√∫menes ejecutivos concisos y claros.\"),\n",
        "    (\"human\", \"Genera un resumen ejecutivo sobre el siguiente informe de ventas: {informe}\")\n",
        "])\n",
        "\n",
        "# Configuramos el output parser de tipo String\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Ejemplo de uso\n",
        "informe_ventas = \"\"\"\n",
        "Ventas del Q1 2024:\n",
        "- Ingresos totales: $1.5M\n",
        "- Crecimiento interanual: 22%\n",
        "- Producto m√°s vendido: Software de gesti√≥n\n",
        "- Principales mercados: Tecnolog√≠a y Finanzas\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format_prompt(informe=informe_ventas)\n",
        "respuesta = modelo1.invoke(prompt).content\n",
        "rprint(f\"[bold bright_cyan]Respuesta del modelo:\\n {respuesta}\")\n",
        "rprint(\"-----\")\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "rprint(f\"[bold dark_sea_green4]Resumen generado:\\n {respuesta_formateada}\")\n"
      ],
      "metadata": {
        "id": "Viuc2cB14Jse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cadenas muy simples donde solo esperas texto plano y no necesitas un control estricto sobre el tipo de dato, la diferencia pr√°ctica entre usar StrOutputParser expl√≠citamente y no usar ning√∫n OutputParser puede ser m√≠nima. En muchos casos, obtendr√°s una salida de texto en ambos escenarios.\n",
        "\n",
        "Sin embargo, usar StrOutputParser expl√≠citamente es una buena pr√°ctica ya que mejora la claridad y legibilidad del c√≥digo.\n",
        "   \n",
        "Ahora que entendemos c√≥mo obtener texto plano, veamos c√≥mo podemos empezar a estructurar la salida.\n",
        "\n",
        "**Pero antes necesitamos conocer y manejar dos conceptos relacionados...**\n",
        "\n"
      ],
      "metadata": {
        "id": "PIveJbqKZCpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Partial variables`\n",
        "\n",
        "Las partial_variables son un mecanismo en LangChain para pre-rellenar variables en un prompt de manera parcial, antes de su uso final.  \n",
        "\n",
        "Pi√©nsalo de esta manera: un PromptTemplate es como una plantilla de texto con \"huecos\" que necesitas llenar para crear un prompt completo para el LLM. Hay dos formas principales de llenar estos huecos:\n",
        "\n",
        "-   **input\\_variables:** Estas son las variables que **cambian** cada vez que utilizas el prompt. Son los datos espec√≠ficos que quieres que el LLM procese en cada llamada.\n",
        "    \n",
        "-   **partial\\_variables:** Estas son las variables que tienen un valor **fijo** o **predefinido** para un uso particular del PromptTemplate. No cambian con cada llamada a la cadena o LLM que usa este prompt."
      ],
      "metadata": {
        "id": "sjDo8JmHxKb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo nuestro PromptTemplate tiene 4 \"huecos\" o 4 variables. Pero al crearlo hemos precargado 3 de ellas con valores via partial variables. De esta al invocar el prompt (es un runnable !!) basta que pasemos una (tema).   \n",
        "Sin imbargo podriamos pasar tambien las restanteas..."
      ],
      "metadata": {
        "id": "MsyxZE-iyC8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Prompt con partial_variables\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Eres un {role} especializado en {area}. {instrucciones}\",\n",
        "    input_variables=[\"tema\"],\n",
        "    partial_variables={\n",
        "        \"role\": \"analista\",\n",
        "        \"area\": \"tecnolog√≠a\",\n",
        "        \"instrucciones\": \"Proporciona un an√°lisis detallado y objetivo.\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Uso del prompt\n",
        "resultado = prompt.invoke({\"tema\": \"Inteligencia Artificial\"})\n",
        "print(resultado)\n",
        "\n",
        "resultado = prompt.invoke({\"tema\": \"Inteligencia Artificial\", \"instrucciones\": \"Contesta con un pareado que rime\"})\n",
        "print(resultado)\n"
      ],
      "metadata": {
        "id": "i0zpPWv1xu73",
        "outputId": "a989cc4a-ad3d-453a-af12-4622606aa3ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Eres un analista especializado en tecnolog√≠a. Proporciona un an√°lisis detallado y objetivo.'\n",
            "text='Eres un analista especializado en tecnolog√≠a. Contesta con un pareado que rime'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un uso tipico de las partial_variables es usarlas para introducir en el prompt las instrucciones de l formato que proporciona Langchain"
      ],
      "metadata": {
        "id": "3Tz1VTF1zL-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `.get_format_instructions()`\n",
        "\n",
        "**Obtener instrucciones de formato (opcional pero recomendado):** Muchos parsers tienen un m√©todo get\\_format\\_instructions() que devuelve texto que puedes incluir en tu prompt para guiar al LLM sobre el formato esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDsh5c2vaw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "id": "2ov8pnCtaeFT",
        "outputId": "78bc60e0-576e-4400-f5c7-9bd8890bd6de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podriamos redactar nosotros mismos las instrucciones ? SI, sin duda. Esto es solo una funcion de utilidad que nos proporciona Langchain. Teoricamnte disponemos de esta forma de una redaccion tecnicamente correcta."
      ],
      "metadata": {
        "id": "N4xnI29VzrbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CommaSeparatedListOutputParser:**\n",
        "---\n",
        "\n",
        "El `CommaSeparatedListOutputParser` en Langchain es un OutputParser **simple pero muy √∫til** dise√±ado para **interpretar la salida de un modelo de lenguaje (LLM) como una lista de elementos separados por comas.** Su funci√≥n principal es tomar el texto generado por el LLM y **transformarlo en una lista de strings de Python**, donde cada string representa un elemento de la lista original que estaba separado por comas en el texto del LLM.\n",
        "\n",
        "  \n",
        "\n",
        "Imagina que le pides a un LLM que te d√© \"tres colores primarios separados por comas\". Podr√≠as esperar una respuesta como:\n",
        "\n",
        "\"rojo, azul, amarillo\"\n",
        "\n",
        "El CommaSeparatedListOutputParser toma esta cadena \"rojo, azul, amarillo\" y la procesa de la siguiente manera:\n",
        "\n",
        "- **Divide la cadena:** Utiliza la coma (,) como delimitador para dividir la cadena en partes m√°s peque√±as.\n",
        "    \n",
        "- **Elimina espacios en blanco (opcional):** Puede configurarse para eliminar espacios en blanco al principio y al final de cada parte extra√≠da. Por defecto, suele hacerlo para limpiar la lista resultante.\n",
        "    \n",
        "- **Crea la lista:** Cada parte resultante se convierte en un elemento de una lista de Python.\n",
        "    \n",
        "-   **Casos de uso:** Obtener listas de elementos, como nombres, ideas, pasos a seguir, o categor√≠as.\n",
        "    \n",
        "-   **Ventaja:** Simple y efectivo para extraer listas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "flcuOq9Nh6k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo: Lista de ingredientes\n",
        "Queremos obtener una lista de ingredientes para hacer una pizza casera y solo nos interesa la lista de ingredientes, pues la procesaremos posteriormente de alguan forma.\n",
        "\n",
        "üëÄObserva el uso de las partial_variables paara introducir en el prompt las instrucciones de formato\n",
        "\n",
        "üëÄObserva tambien el tipo de las dos respuestas. El primero es un string y no podriamos iterarlo. El segundo es una lista de python, si podemos iterarla."
      ],
      "metadata": {
        "id": "ILCufEkEiWLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Genera una lista de ingredientes para hacer {receta} casera. Solo lista los ingredientes, uno por l√≠nea. Sin opciones\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Generar la salida\n",
        "prompt = prompt_template.format(receta=\"pizza\")\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "# Mostrar los resultados\n",
        "rprint(f\"Respuesta del modelo SIN FORMATEAR:\\n [bold bright_cyan]{respuesta}\")\n",
        "rprint(type(respuesta))\n",
        "rprint(\"\\n\\n\")\n",
        "rprint(f\"Respuesta del modelo FORMATEADA:\\n [bold spring_green3]{respuesta_formateada}\")\n",
        "rprint(type(respuesta_formateada))\n"
      ],
      "metadata": {
        "id": "AySI2dvhi78N",
        "outputId": "46e1520d-650e-47ad-f6ea-6e09b0027e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo SIN FORMATEAR:\n",
              " \u001b[1;96mharina, agua, levadura, sal, aceite de oliva, salsa de tomate, queso mozzarella, or√©gano, pepperoni, pimientos, \u001b[0m\n",
              "\u001b[1;96mchampi√±ones, cebolla\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo SIN FORMATEAR:\n",
              " <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">harina, agua, levadura, sal, aceite de oliva, salsa de tomate, queso mozzarella, or√©gano, pepperoni, pimientos, </span>\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">champi√±ones, cebolla</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo FORMATEADA:\n",
              " \u001b[1;38;5;41m[\u001b[0m\u001b[1;38;5;41m'harina'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'agua'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'levadura'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'sal'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'aceite de oliva'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'salsa de tomate'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'queso mozzarella'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'or√©gano'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\n",
              "\u001b[1;38;5;41m'pepperoni'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'pimientos'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'champi√±ones'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'cebolla'\u001b[0m\u001b[1;38;5;41m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo FORMATEADA:\n",
              " <span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">[</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'harina'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'agua'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'levadura'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'sal'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'aceite de oliva'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'salsa de tomate'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'queso mozzarella'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'or√©gano'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span>\n",
              "<span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'pepperoni'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'pimientos'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'champi√±ones'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'cebolla'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo: Lista de etiquetas  \n",
        "Deseamos que le modelo genere una lista de hastags (o etiquetas) a partir del tema de un articulo"
      ],
      "metadata": {
        "id": "1xh-L0f-7gFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"\"\"Dame 5 etiquetas relevantes para un post de blog sobre: {tema}.\n",
        "                Las etiquetas deben estar separadas por comas.\"\"\",\n",
        "    input_variables=[\"tema\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "\n",
        "# Formatear el prompt con el tema del blog\n",
        "prompt = prompt_template.format(tema=\"Recetas de cocina vegana f√°ciles y r√°pidas para principiantes\")\n",
        "\n",
        "# Obtener la salida del LLM\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "# Mostrar los resultados\n",
        "rprint(f\"Respuesta del modelo SIN FORMATEAR:\\n [bold bright_cyan]{respuesta}\")\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(\"\\n\\n\")\n",
        "\n",
        "rprint(f\"Respuesta del modelo FORMATEADA:\\n [bold spring_green3]{respuesta_formateada}\")\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "rprint(\"\\n\\n\")\n",
        "\n",
        "# Imprimir las etiquetas generadas\n",
        "print(\"Etiquetas sugeridas:\")\n",
        "for etiqueta in respuesta_formateada:\n",
        "    rprint(f\"[bold spring_green3] - {etiqueta.strip()}\")\n"
      ],
      "metadata": {
        "id": "9p3CwHtv8Svc",
        "outputId": "597c4605-392f-4535-9770-735bf6ab8053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo SIN FORMATEAR:\n",
              " \u001b[1;96mrecetas veganas, cocina f√°cil, principiantes, comida saludable, recetas r√°pidas\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo SIN FORMATEAR:\n",
              " <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">recetas veganas, cocina f√°cil, principiantes, comida saludable, recetas r√°pidas</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo FORMATEADA:\n",
              " \u001b[1;38;5;41m[\u001b[0m\u001b[1;38;5;41m'recetas veganas'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'cocina f√°cil'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'principiantes'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'comida saludable'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'recetas r√°pidas'\u001b[0m\u001b[1;38;5;41m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo FORMATEADA:\n",
              " <span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">[</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'recetas veganas'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'cocina f√°cil'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'principiantes'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'comida saludable'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'recetas r√°pidas'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Etiquetas sugeridas:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - recetas veganas\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - recetas veganas</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - cocina f√°cil\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - cocina f√°cil</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - principiantes\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - principiantes</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - comida saludable\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - comida saludable</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - recetas r√°pidas\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - recetas r√°pidas</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EnumOutputParser**\n",
        "\n",
        "`EnumOutputParser` es un tipo de output parser en LangChain que se utiliza para restringir la salida de un modelo a un conjunto predefinido de valores. Esto es √∫til cuando se desea que la respuesta del modelo pertenezca a un conjunto espec√≠fico de opciones, como categor√≠as, estados o tipos.\n",
        "\n",
        "### Caracter√≠sticas Principales:\n",
        "\n",
        "-   **Restricci√≥n de Valores**: Permite definir un conjunto limitado de opciones que el modelo puede devolver.\n",
        "-   **Validaci√≥n Autom√°tica**: Si la salida del modelo no coincide con las opciones definidas, se puede manejar como un error.\n",
        "-   **Facilita la Consistencia**: Asegura que las respuestas sean coherentes y dentro de un rango esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "lj97SmlAofVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de Caso de Uso: Clasificaci√≥n de Sentimientos\n",
        "\n",
        "Imaginemos que estamos construyendo un sistema que clasifica el sentimiento de comentarios de clientes sobre un producto. Queremos que el modelo devuelva solo tres categor√≠as: \"positivo\", \"negativo\" y \"neutral\"."
      ],
      "metadata": {
        "id": "S94-9qKkFiJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definimos las opciones de sentimiento\n",
        "class Sentimientos(Enum):\n",
        "    POSITIVO = \"positivo\"\n",
        "    NEGATIVO = \"negativo\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=Sentimientos)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Clasifica el siguiente comentario: {comentario}\",\n",
        "    input_variables=[\"comentario\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "\n",
        "# Ejemplo de uso\n",
        "comentario_cliente = \"Me encanta este producto, es incre√≠ble y funciona muy bien.\"\n",
        "\n",
        "prompt = prompt_template.format(comentario=comentario_cliente)\n",
        "\n",
        "# Clasificamos el sentimiento\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "\n",
        "rprint(\"Sentimiento Clasificado:\")\n",
        "rprint(respuesta_formateada)\n"
      ],
      "metadata": {
        "id": "R-xNyOZNFsHh",
        "outputId": "09c5531c-1800-4cca-e593-881d782cf127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Response 'El comentario se puede clasificar como **positivo**. Expresa satisfacci√≥n y aprecio por el producto, destacando su efectividad.' is not one of the expected values: ['positivo', 'negativo', 'neutral']\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/output_parsers/enum.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start, boundary)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simple value lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m   1136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mve_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'El comentario se puede clasificar como **positivo**. Expresa satisfacci√≥n y aprecio por el producto, destacando su efectividad.' is not a valid Sentimientos",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-dd04cd202cd2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Parsear la salida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mrespuesta_formateada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrespuesta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/output_parsers/enum.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             raise OutputParserException(\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0;34mf\"Response '{response}' is not one of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;34mf\"expected values: {self._valid_values}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Response 'El comentario se puede clasificar como **positivo**. Expresa satisfacci√≥n y aprecio por el producto, destacando su efectividad.' is not one of the expected values: ['positivo', 'negativo', 'neutral']\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **StructuredOutputParser**\n",
        "Un analizador de salida estructurado est√° dise√±ado para extraer y estructurar informaci√≥n de texto no estructurado o semiestructurado. Esto es adecuado para situaciones que requieren la extracci√≥n de m√∫ltiples campos.\n",
        "\n",
        "Permite analizar la salida del LLM en una estructura predefinida. Le proporcionas un esquema (generalmente una lista de ResponseSchema que definen los campos esperados) y el parser intenta extraer la informaci√≥n y mapearla a ese esquema.\n",
        "\n",
        "Aunque el analizador Pydantic (explicado a continuaci√≥n) ofrece capacidades m√°s s√≥lidas, StructuredOutputParser es ventajoso para su uso con modelos m√°s simples.\n",
        "\n",
        "**Casos de uso:** Extraer informaci√≥n espec√≠fica de un texto, como atributos de un producto, detalles de un evento, o datos de contacto. Es muy vers√°til para obtener datos estructurados.\n",
        "\n",
        "**Ventaja:** Flexible y permite definir la estructura esperada de la salida.\n",
        "\n",
        "Veamos un ejemplo en el que el resultado ser√° un diccionario estructurado con los campos nombre, edad y email, listo para ser utilizado en la aplicaci√≥n."
      ],
      "metadata": {
        "id": "4Zlw_DQiZPI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"nombre\", description=\"El nombre del usuario\"),\n",
        "    ResponseSchema(name=\"edad\", description=\"La edad del usuario\"),\n",
        "    ResponseSchema(name=\"email\", description=\"El correo electr√≥nico del usuario\")\n",
        "]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "F4pnLdNTZP9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera informaci√≥n de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "p1_T9nPdbCIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sobre las `partial variables`\n",
        "Las **partial\\_variables** dentro de un PromptTemplate son una forma de **pre-cargar o fijar ciertos valores dentro de la plantilla del prompt antes de que se proporcionen las variables de entrada principales, es decir cuando se crea el propmpt template y antes de que se formatee**\n",
        "\n",
        "Pi√©nsalo de esta manera: un PromptTemplate es como una plantilla de texto con \"huecos\" que necesitas llenar para crear un prompt completo para el LLM. Hay dos formas principales de llenar estos huecos:\n",
        "\n",
        "-   **input\\_variables:** Estas son las variables que **cambian** cada vez que utilizas el prompt. Son los datos espec√≠ficos que quieres que el LLM procese en cada llamada.\n",
        "    \n",
        "-   **partial\\_variables:** Estas son las variables que tienen un valor **fijo** o **predefinido** para un uso particular del PromptTemplate. No cambian con cada llamada a la cadena o LLM que usa este prompt.\n",
        "    \n",
        "\n",
        "**En el codigo anterior:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "prompt = PromptTemplate(\n",
        "            template=\"Genera informaci√≥n de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "            input_variables=[],\n",
        "            partial_variables={\"format_instructions\": format_instructions}\n",
        "```\n",
        "En otras palabras, son una forma de incluir en el prompt template una parte fija, pero mediante una variable, lo que proporciona mas flexibilidad programatica.\n",
        "En este caso metemos en el prompt, las instruciones que nos ha devuelto el parser."
      ],
      "metadata": {
        "id": "faC9fR3acUzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "respuesta = llm.invoke(prompt.format()) # la llamada no requiere ningun par√°metro\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "respuesta_analizada\n",
        "\n"
      ],
      "metadata": {
        "id": "WrZPCFGieWkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a dise√±ar una consulta para un modelo de lenguaje (LLM) que sirva como base para generar un test de preguntas sobre un tema espec√≠fico. Proporcionaremos el tema y un nivel de dificultad (bajo, medio, alto), y el LLM deber√° generar el texto de la pregunta, tres opciones de respuesta y el √≠ndice de la respuesta correcta. La respuesta debe estar estructurada en un diccionario con el siguiente formato:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"pregunta\": \"Texto de la pregunta\",\n",
        "    \"opciones\": [\"Opci√≥n 1\", \"Opci√≥n 2\", \"Opci√≥n 3\"],\n",
        "    \"respuesta_correcta\": √≠ndice_de_la_opci√≥n_correcta\n",
        "}\n",
        "```\n",
        "\n",
        "Este formato permitir√° una f√°cil interpretaci√≥n y uso de la pregunta generada.\n"
      ],
      "metadata": {
        "id": "4-C1hmhItipv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "response_schemas = [\n",
        "            ResponseSchema(name=\"pregunta\", description=\"Texto de la pregunta generada.\"),\n",
        "            ResponseSchema(name=\"opciones\", description=\"Lista de tres opciones de respuesta.\"),\n",
        "            ResponseSchema(name=\"respuesta_correcta\", description=\"√çndice de la opci√≥n correcta (0, 1 o 2).\")\n",
        "                    ]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "        Genera una pregunta de test sobre el tema: {tema}.\n",
        "        El nivel de dificultad debe ser: {nivel}.\n",
        "        La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "respuesta_analizada"
      ],
      "metadata": {
        "id": "MNxAZq7RtiNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PydanticOutputParser**\n",
        "\n",
        "Convierte la salida a un modelo de datos Pydantic.\n",
        "\n",
        "Si buscas simplicidad y rapidez, StructuredOutputParser es una buena opci√≥n pero si necesitas validaci√≥n de datos robusta y est√°s utilizando Pydantic en tu proyecto, PydanticOutputParser es la mejor alternativa.\n",
        "\n",
        "| Caracter√≠stica              | StructuredOutputParser               | PydanticOutputParser               |\n",
        "|-----------------------------|--------------------------------------|-------------------------------------|\n",
        "| **Definici√≥n de esquema**    | Lista de `ResponseSchema`            | Clase Pydantic (`BaseModel`)        |\n",
        "| **Validaci√≥n de tipos**      | No estricta                         | Estricta (usando Pydantic)          |\n",
        "| **Uso**                     | Sencillo y r√°pido                   | M√°s verboso pero organizado         |\n",
        "| **Integraci√≥n con Pydantic**| No                                  | S√≠                                  |\n",
        "| **Recomendado para**         | Prototipos r√°pidos                  | Aplicaciones robustas y complejas   |\n",
        "\n",
        "Veamos como realizar el ejemplo anterior con PydanticOutputParser"
      ],
      "metadata": {
        "id": "4NbToCP9tt-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Definimos el modelo Pydantic para la respuesta\n",
        "class PreguntaTest(BaseModel):\n",
        "    pregunta: str = Field(description=\"Texto de la pregunta generada.\")\n",
        "    opciones: List[str] = Field(description=\"Lista de tres opciones de respuesta.\")\n",
        "    respuesta_correcta: int = Field(description=\"√çndice de la opci√≥n correcta (0, 1 o 2).\")\n",
        "\n",
        "# Crear el PydanticOutputParser\n",
        "output_parser = PydanticOutputParser(pydantic_object=PreguntaTest)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "Genera una pregunta de test sobre el tema: {tema}.\n",
        "El nivel de dificultad debe ser: {nivel}.\n",
        "La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "print(\"Respuesta analizada:\\n\", respuesta_analizada)\n"
      ],
      "metadata": {
        "id": "dEJ_Wd1Lt0Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otro ejemplo con Pydantic. Deseamos obtener la bibliografia ordenada del autor indicado. deseamos una lista ordenada con el a√±o y el titulo de cada libro.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BnYJ6p9zt4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Definimos el modelo Pydantic para la bibliograf√≠a\n",
        "class Libro(BaseModel):\n",
        "    a√±o: int = Field(description=\"A√±o de publicaci√≥n del libro.\")\n",
        "    t√≠tulo: str = Field(description=\"T√≠tulo del libro.\")\n",
        "\n",
        "class BibliografiaAutor(BaseModel):\n",
        "    libros: List[Libro] = Field(description=\"Lista de libros publicados por el autor.\")\n",
        "\n",
        "# Crear el PydanticOutputParser\n",
        "output_parser = PydanticOutputParser(pydantic_object=BibliografiaAutor)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "Proporciona la bibliograf√≠a del autor: {autor}.\n",
        "La respuesta debe incluir solo el a√±o y el t√≠tulo de cada libro.\n",
        "No incluyas informaci√≥n adicional.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"autor\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "autor=\"George R. R. Martin\"\n",
        "prompt = prompt_template.format(autor=autor)\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "display(Markdown(f\"**Respuesta del LLM sin parsear:**\\n{respuesta.content}\"))\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Imprimir la bibliograf√≠a de forma legible\n",
        "# La funci√≥n enumerate toma un iterable (como una lista) y devuelve un objeto que genera pares de valores\n",
        "# start=1 , para que el 0 se considere 1\n",
        "print(f\"Bibliograf√≠a de {autor}:\\n\")\n",
        "for i, libro in enumerate(respuesta_analizada.libros, start=1):\n",
        "    print(f\"Libro {i}:\")\n",
        "    print(f\"  A√±o: {libro.a√±o}\")\n",
        "    print(f\"  T√≠tulo: {libro.t√≠tulo}\\n\")\n",
        "\n",
        "print(type(respuesta_analizada))"
      ],
      "metadata": {
        "id": "Ba2fHvFJt4VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que la clase del objeto devuelto NO es un diccionario, ni un JSON sino un objeto heredado de la clase que hemos definido con Pydantic !!\n"
      ],
      "metadata": {
        "id": "dnHhQCe_Yb1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **JsonOutputParser**\n",
        "\n",
        "JsonOutputParser de LangChain es una herramienta que te permite parsear la salida de un modelo de lenguaje (LLM) en formato JSON. Esto es especialmente √∫til cuando quieres que el LLM genere una respuesta estructurada en JSON y luego convertir esa respuesta en un objeto de Python (como un diccionario o una lista) para su manipulaci√≥n."
      ],
      "metadata": {
        "id": "626Vr1c5uzj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "\n",
        "# Definimos el esquema de salida esperado en JSON\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"libros\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"a√±o\": {\"type\": \"integer\", \"description\": \"A√±o de publicaci√≥n del libro.\"},\n",
        "                    \"t√≠tulo\": {\"type\": \"string\", \"description\": \"T√≠tulo del libro.\"}\n",
        "                },\n",
        "                \"required\": [\"a√±o\", \"t√≠tulo\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"libros\"]\n",
        "}\n",
        "\n",
        "# Crear el JsonOutputParser\n",
        "output_parser = JsonOutputParser(pydantic_schema=schema)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "Proporciona la bibliograf√≠a del autor: {autor}.\n",
        "La respuesta debe incluir solo el a√±o y el t√≠tulo de cada libro.\n",
        "No incluyas informaci√≥n adicional.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"autor\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "autor = \"Gabriel Garc√≠a M√°rquez\"\n",
        "prompt = prompt_template.format(autor=autor)\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt)\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_analizada = output_parser.parse(respuesta.content)\n",
        "print(\"Respuesta analizada:\\n\", respuesta_analizada)\n",
        "print(type(respuesta_analizada))"
      ],
      "metadata": {
        "id": "wgjgzWrSvjSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que el tipo de la respuesta obtenida es direcatmente un diccionario de Python"
      ],
      "metadata": {
        "id": "lqPpK1M-1U4N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bofNcvPzvhUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RegexOutputParser**"
      ],
      "metadata": {
        "id": "Ah3HuvErvNl3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NjKPw8TZvmdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSpQlpy-vmDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definir las opciones del enumerado\n",
        "class ColoresOjos(Enum):\n",
        "    MARR√ìN = \"marr√≥n\"\n",
        "    AVELANA = \"avellana\"\n",
        "    √ÅMBAR = \"√°mbar\"\n",
        "    VERDE = \"verde\"\n",
        "    AZUL = \"azul\"\n",
        "    GRIS = \"gris\"\n",
        "\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=ColoresOjos)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"De que color tenia o tiene los ojos esta persona: {persona}.\",\n",
        "    input_variables=[\"persona\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format(persona= \"Frank Sinatra\")\n",
        "output = llm.invoke(input_prompt).content.lower()\n",
        "\n",
        "parsed_output = output_parser.parse(output)\n",
        "\n",
        "# Parsear la salida\n",
        "print(f\"El g√©nero elegido es: {parsed_output}\")\n"
      ],
      "metadata": {
        "id": "tCY84L22o-Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo realizamos dos llamadas al modelo. En la primera le pedimos que escoja entre una opcion del Enum, y con ella realizamos la segunda invocaci√≥n."
      ],
      "metadata": {
        "id": "jCgtmxHREQtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "# Definir los g√©neros literarios como un Enum\n",
        "class GenerosLiterarios(Enum):\n",
        "    FANTAS√çA = \"fantas√≠a\"\n",
        "    CIENCIA_FICCI√ìN = \"ciencia ficci√≥n\"\n",
        "    MISTERIO = \"misterio\"\n",
        "    ROMANCE = \"romance\"\n",
        "    HIST√ìRICA = \"novela hist√≥rica\"\n",
        "    NO_FICCI√ìN = \"no ficci√≥n\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=GenerosLiterarios)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Elige un g√©nero literario de la siguiente lista: {generos_literarios}.\\nResponde solo con el genero escogido \\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\n",
        "        \"generos_literarios\": \", \".join([g.value for g in GenerosLiterarios]),\n",
        "        \"format_instructions\": output_parser.get_format_instructions()\n",
        "    }\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=2)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "print(input_prompt)\n",
        "respuesta = llm.invoke(input_prompt).content.lower()\n",
        "print (respuesta)\n",
        "\n"
      ],
      "metadata": {
        "id": "cooXmCR3zwNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsear la salida\n",
        "try:\n",
        "    genero_elegido = output_parser.parse(respuesta)\n",
        "    print(f\"El g√©nero elegido es: {genero_elegido.value}\")\n",
        "\n",
        "    # Ahora pedimos una recomendaci√≥n de libro para el g√©nero elegido\n",
        "    recomendacion_prompt = PromptTemplate(\n",
        "        template=\"Recomienda un libro de g√©nero {genero}.\",\n",
        "        input_variables=[\"genero\"]\n",
        "    )\n",
        "    recomendacion_input = recomendacion_prompt.format(genero=genero_elegido.value)\n",
        "    recomendacion = llm.invoke(recomendacion_input)\n",
        "\n",
        "    print(f\"Recomendaci√≥n: {recomendacion.content}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "e9gtulAtFaSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No esta demasiado clara la conveniencia de ests OutputParser. Porque no hacer simplemente una eleccion aleatoria  ¬ø?¬ø?¬ø?\n",
        "\n",
        "\n",
        "```\n",
        "genero_elegido=random.choice(list(GenerosLiterarios))\n",
        "```\n",
        "En lugar de invocar al modelo ¬ø?¬ø?¬ø?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LDBm8EPeHcNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BooleanOutputParser**\n",
        "\n"
      ],
      "metadata": {
        "id": "lM39D_VGIglJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import BooleanOutputParser\n",
        "\n",
        "\n",
        "# Crear el BooleanOutputParser\n",
        "output_parser = BooleanOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Responde con 'Yes' o 'No' a la siguiente afirmaci√≥n: 'El sol es una estrella.'\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\":\" \"}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "9vweQ7_PIkHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import RegexParser\n",
        "\n",
        "\n",
        "# Definir la expresi√≥n regular para extraer nombre y edad\n",
        "regex_pattern = r\"Nombre: (?P<nombre>.+)\\nEdad: (?P<edad>\\d+)\"\n",
        "\n",
        "# Crear el RegexParser\n",
        "output_parser = RegexParser(regex=regex_pattern, output_keys=[\"nombre\", \"edad\"])\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Genera informaci√≥n de una persona ficticia, incluyendo su nombre y edad.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": \"\"}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)\n",
        "\n",
        "# Generar la salida\n",
        "input_prompt = prompt.format()\n",
        "respuesta = llm.invoke(input_prompt)\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_parseada = output_parser.parse(respuesta.content)\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(respuesta_parseada)"
      ],
      "metadata": {
        "id": "u0zW0wz-v8yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OutputFixingParser**"
      ],
      "metadata": {
        "id": "BTfA9lJNv18c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YD59a2H4v6Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb3wSO9Dv6nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **YamlOutputParser**"
      ],
      "metadata": {
        "id": "mmy3d6aT5YN7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXTL_k347_U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XmlOutputParser**"
      ],
      "metadata": {
        "id": "zW60RFs972y0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMPmB_vD71l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DateTimeOutputParser**"
      ],
      "metadata": {
        "id": "E_NrlM6q8AB6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJseHcmY8HOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OutputFixingParser**"
      ],
      "metadata": {
        "id": "3P9Og_Wl8IxD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pYK_eYF8JYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RetrywithError**"
      ],
      "metadata": {
        "id": "1lenJVEU8J08"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETLntMYN8KfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **with_structured_output()**\n",
        "\n",
        "solo para llm que lo soportan (openai ...)"
      ],
      "metadata": {
        "id": "w_bCk2tw8NZV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-FIf22J8OWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    'In JSON format, give me a list of {topic} and their '\n",
        "    'corresponding names in French, Spanish and in a '\n",
        "    'Cat Language.'\n",
        ")\n",
        "\n",
        "model = ChatOpenAI()\n",
        "chain = prompt | model | SimpleJsonOutputParser()\n",
        "\n",
        "async for chunk in chain.astream({'topic': 'colors'}):\n",
        "    print('-')  # noqa: T201\n",
        "    print(chunk, sep='', flush=True)  # noqa: T201"
      ],
      "metadata": {
        "id": "DIFbJlhaRqhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://freedium.cfd/https://python.plainenglish.io/langchain-in-chains-7-output-parsers-e1a2cdd40cd3\n",
        "\n",
        "2. https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834\n",
        "\n",
        "3. https://cobusgreyling.medium.com/langchain-structured-output-parser-using-openai-c3fe6927beb7\n",
        "\n",
        "4. https://python.langchain.com/docs/how_to/output_parser_structured/\n",
        "\n",
        "5. https://www.comet.com/site/blog/mastering-output-parsing-in-langchain/\n",
        "\n",
        "6. https://www.gettingstarted.ai/how-to-langchain-output-parsers-convert-text-to-objects/\n",
        "\n",
        "7. https://www.gettingstarted.ai/how-to-extract-metadata-from-pdf-convert-to-json-langchain/  Este es un buen reto\n",
        "\n",
        "8. https://www.analyticsvidhya.com/blog/2024/11/output-parsers/\n"
      ],
      "metadata": {
        "id": "lo85c5QHb66h"
      }
    }
  ]
}