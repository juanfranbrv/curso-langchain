{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOg+BiytTUOQYJSs2+N+hdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/3.%20Analizadores%20de%20salida%20(Output%20parsers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. OutputParsers en Langchain**\n",
        "---\n",
        "Imagina que le preguntas a un LLM \"¬øCu√°les son los tres planetas m√°s cercanos al sol?\" y te responde: \"Mercurio, Venus y la Tierra son los planetas m√°s cercanos al sol\". Si bien la respuesta es correcta para un humano, para que tu programa pueda usar esa informaci√≥n, lo ideal ser√≠a tenerla en un formato m√°s manejable, como una lista o un objeto JSON. Aqu√≠ es donde entran en juego los Output Parsers.  \n",
        "\n",
        "Los Output Parsers te permiten ‚Äúforzar‚Äù o ‚Äúguiar‚Äù al modelo para que devuelva la informaci√≥n seg√∫n un formato deseado (por ejemplo, un JSON con campos espec√≠ficos, una lista, etc).  \n",
        "\n",
        "Convertir el texto libre, en informaci√≥n organizada, en algo que puedes usar directamente en tu c√≥digo.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Sin output parser: texto plano\n",
        "\"Tom Hanks ha actuado en Forrest Gump y Saving Private Ryan\"\n",
        "\n",
        "# Con output parser: estructura definida\n",
        "{\n",
        "  \"actor\": \"Tom Hanks\",\n",
        "  \"peliculas\": [\"Forrest Gump\", \"Saving Private Ryan\"]\n",
        "}\n",
        "```\n",
        "\n",
        "La estructuras de datos de salida son entre otras:\n",
        "\n",
        "- Listas\n",
        "- Enumeraciones\n",
        "- Objetos JSON\n",
        "- Diccionarios\n",
        "- Modelos Pydantic\n",
        "\n",
        "\n",
        "LangChain ofrece una variedad de Output Parsers preconstruidos para diferentes necesidades. Algunos de los mas usados son estos:\n",
        "\n",
        "- **StrOutputParser:** Convierte la salida a string\n",
        "\n",
        "- **CommaSeparatedListOutputParser**: Convierte la salida en una lista separada por comas. √ötil para generar listas de elementos\n",
        "\n",
        "- **EnumOutputParser**: Restringe la salida a un conjunto predefinido de valores. Perfecto para categor√≠as o estados limitados. Idela cuedo se desea que el LLM elija de un conjunto de opciones.\n",
        "\n",
        "- **JsonOutputParser** : Transforma la salida directamente en formato JSON. Ideal para respuestas estructuradas simples. Dos variantes principales:\n",
        "\n",
        "    - SimpleJsonOutputParser\n",
        "    - JsonOutputParser\n",
        "\n",
        "- **DatetimeOutputParser**: Extrae y formatea fechas y horas. √ötil para parsear informaci√≥n temporal\n",
        "\n",
        "- **StructuredOutputParser**: Permite definir esquemas de salida m√°s complejos. Configurable con m√∫ltiples campos\n",
        "\n",
        "- **PydanticOutputParser**: Convierte la salida en objetos Pydantic. Permite definir estructuras de datos complejas. Gran flexibilidad para validaci√≥n\n",
        "\n",
        "- **OutputFixingParser**: Intenta corregir salidas mal formateadas. √ötil cuando el modelo no genera la estructura perfecta.\n",
        "\n",
        "\n",
        "Puedes ver todos los OutputParsers disponibles aqu√≠:\n",
        "https://python.langchain.com/docs/concepts/output_parsers/\n",
        "\n",
        "\n",
        "# `with_structured_output`\n",
        "\n",
        "Es un m√©todo nativo que aprovecha capacidades del modelo. Utiliza capacidades de llamada de funci√≥n (function calling) del modelo. Es m√°s eficiente y preciso. Esta soportado por modelos avanzados como OpenAI, Anthropic, Groq.\n",
        "\n",
        "Lo trataremos al final de este cuaderno.\n",
        "\n",
        "‚òùüèª La mayor parte de modelos soportan esta funci√≥n y es el futuro de la extracci√≥n estructurada en LangChain. Prior√≠zalo cuando puedas.\n",
        "\n",
        "Puede consultarse una lista de modelos y sus capacidades aqu√≠:\n",
        "https://python.langchain.com/docs/integrations/chat/\n",
        "\n",
        "\n",
        "\n",
        "Crear un ejemplo que dada una receta la presente estructurada en ingrdientes, pasos, etc\n",
        "Esta en este video https://www.youtube.com/watch?v=lbWxastyWPw\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry8P2gp7VSGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Preparando el entorno del cuaderno**\n",
        "----\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).\n",
        "\n",
        "- Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "- Instalamos la librer√≠a LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "- Importamos las clases espec√≠ficas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dej√°ndolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar.\n"
      ],
      "metadata": {
        "id": "KEWPJUQdCird"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ihaYmy1rgga_"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librer√≠as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "\n",
        "# Instalamos Rich para mejorar la salida\n",
        "%pip install rich -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos las librerias para formatear mejor la salida\n",
        "from IPython.display import Markdown, display\n",
        "from rich import print as rprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. StrOutputParser**\n",
        "---\n",
        "\n",
        "`StrOutputParser` es el output parser m√°s simple de LangChain. Su funci√≥n principal es convertir la salida del modelo de lenguaje directamente en una cadena de texto sin realizar ninguna transformaci√≥n estructural.\n",
        "\n",
        "Caracter√≠sticas principales:\n",
        "- Convierte la salida del modelo a texto plano\n",
        "- No realiza ninguna validaci√≥n o estructuraci√≥n\n",
        "- √ötil cuando solo necesitas el texto sin procesar\n",
        "- Muy ligero y directo\n",
        "\n",
        "Casos de uso tipicos: Resumenes, traducciones simples, ..."
      ],
      "metadata": {
        "id": "Zer1dYqJ4EEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Ejemplo: Generador de Res√∫menes Ejecutivos\n",
        "\n",
        "A partir de uos resultados de ventas (simulado) deseamos obtener un informe sobre el mismo.\n",
        "\n"
      ],
      "metadata": {
        "id": "J6qyHDnsQkWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Configuramos el modelo\n",
        "modelo1 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "modelo2 = ChatGroq(model=\"qwen-qwq-32b\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# Creamos un prompt para generar un resumen ejecutivo\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente experto en crear res√∫menes ejecutivos concisos y claros.\"),\n",
        "    (\"human\", \"Genera un resumen ejecutivo sobre el siguiente informe de ventas: {informe}\")\n",
        "])\n",
        "\n",
        "# Configuramos el output parser de tipo String\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Ejemplo de uso\n",
        "informe_ventas = \"\"\"\n",
        "Ventas del Q1 2024:\n",
        "- Ingresos totales: $1.5M\n",
        "- Crecimiento interanual: 22%\n",
        "- Producto m√°s vendido: Software de gesti√≥n\n",
        "- Principales mercados: Tecnolog√≠a y Finanzas\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format_prompt(informe=informe_ventas)\n",
        "respuesta = modelo1.invoke(prompt).content\n",
        "rprint(f\"[bold bright_cyan]Respuesta del modelo:\\n {respuesta}\")\n",
        "rprint(\"-----\")\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "rprint(f\"[bold dark_sea_green4]Resumen generado:\\n {respuesta_formateada}\")\n"
      ],
      "metadata": {
        "id": "Viuc2cB14Jse",
        "outputId": "c83bb8f8-e786-4849-b244-817627539702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;96mRespuesta del modelo:\u001b[0m\n",
              "\u001b[1;96m **Resumen Ejecutivo: Informe de Ventas Q1 \u001b[0m\u001b[1;96m2024\u001b[0m\u001b[1;96m**\u001b[0m\n",
              "\n",
              "\u001b[1;96mDurante el primer trimestre de \u001b[0m\u001b[1;96m2024\u001b[0m\u001b[1;96m, la empresa ha reportado ingresos totales de $\u001b[0m\u001b[1;96m1.5\u001b[0m\u001b[1;96m millones, reflejando un \u001b[0m\n",
              "\u001b[1;96mcrecimiento interanual del \u001b[0m\u001b[1;96m22\u001b[0m\u001b[1;96m%. El producto m√°s vendido fue el software de gesti√≥n, destac√°ndose como la soluci√≥n \u001b[0m\n",
              "\u001b[1;96mpreferida en nuestros principales mercados, que son tecnolog√≠a y finanzas. Este desempe√±o positivo indica un buen \u001b[0m\n",
              "\u001b[1;96mposicionamiento en el mercado y una s√≥lida demanda de nuestras ofertas. Se recomienda continuar enfoc√°ndose en \u001b[0m\n",
              "\u001b[1;96mestas √°reas para maximizar el crecimiento futuro.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">Respuesta del modelo:</span>\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\"> **Resumen Ejecutivo: Informe de Ventas Q1 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">2024</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">**</span>\n",
              "\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">Durante el primer trimestre de </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">2024</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">, la empresa ha reportado ingresos totales de $</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">1.5</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\"> millones, reflejando un </span>\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">crecimiento interanual del </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">22</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">%. El producto m√°s vendido fue el software de gesti√≥n, destac√°ndose como la soluci√≥n </span>\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">preferida en nuestros principales mercados, que son tecnolog√≠a y finanzas. Este desempe√±o positivo indica un buen </span>\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">posicionamiento en el mercado y una s√≥lida demanda de nuestras ofertas. Se recomienda continuar enfoc√°ndose en </span>\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">estas √°reas para maximizar el crecimiento futuro.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-----\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-----\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;71mResumen generado:\u001b[0m\n",
              "\u001b[1;38;5;71m **Resumen Ejecutivo: Informe de Ventas Q1 \u001b[0m\u001b[1;38;5;71m2024\u001b[0m\u001b[1;38;5;71m**\u001b[0m\n",
              "\n",
              "\u001b[1;38;5;71mDurante el primer trimestre de \u001b[0m\u001b[1;38;5;71m2024\u001b[0m\u001b[1;38;5;71m, la empresa ha reportado ingresos totales de $\u001b[0m\u001b[1;38;5;71m1.5\u001b[0m\u001b[1;38;5;71m millones, reflejando un \u001b[0m\n",
              "\u001b[1;38;5;71mcrecimiento interanual del \u001b[0m\u001b[1;38;5;71m22\u001b[0m\u001b[1;38;5;71m%. El producto m√°s vendido fue el software de gesti√≥n, destac√°ndose como la soluci√≥n \u001b[0m\n",
              "\u001b[1;38;5;71mpreferida en nuestros principales mercados, que son tecnolog√≠a y finanzas. Este desempe√±o positivo indica un buen \u001b[0m\n",
              "\u001b[1;38;5;71mposicionamiento en el mercado y una s√≥lida demanda de nuestras ofertas. Se recomienda continuar enfoc√°ndose en \u001b[0m\n",
              "\u001b[1;38;5;71mestas √°reas para maximizar el crecimiento futuro.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">Resumen generado:</span>\n",
              "<span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\"> **Resumen Ejecutivo: Informe de Ventas Q1 </span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">2024</span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">**</span>\n",
              "\n",
              "<span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">Durante el primer trimestre de </span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">2024</span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">, la empresa ha reportado ingresos totales de $</span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">1.5</span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\"> millones, reflejando un </span>\n",
              "<span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">crecimiento interanual del </span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">22</span><span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">%. El producto m√°s vendido fue el software de gesti√≥n, destac√°ndose como la soluci√≥n </span>\n",
              "<span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">preferida en nuestros principales mercados, que son tecnolog√≠a y finanzas. Este desempe√±o positivo indica un buen </span>\n",
              "<span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">posicionamiento en el mercado y una s√≥lida demanda de nuestras ofertas. Se recomienda continuar enfoc√°ndose en </span>\n",
              "<span style=\"color: #5faf5f; text-decoration-color: #5faf5f; font-weight: bold\">estas √°reas para maximizar el crecimiento futuro.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cadenas muy simples donde solo esperas texto plano y no necesitas un control estricto sobre el tipo de dato, la diferencia pr√°ctica entre usar StrOutputParser expl√≠citamente y no usar ning√∫n OutputParser puede ser m√≠nima. En muchos casos, obtendr√°s una salida de texto en ambos escenarios.\n",
        "\n",
        "Sin embargo, usar StrOutputParser expl√≠citamente es una buena pr√°ctica ya que mejora la claridad y legibilidad del c√≥digo.\n",
        "   \n",
        "Ahora que entendemos c√≥mo obtener texto plano, veamos c√≥mo podemos empezar a estructurar la salida.\n",
        "\n",
        "**Pero antes necesitamos conocer y manejar dos conceptos relacionados...**\n",
        "\n"
      ],
      "metadata": {
        "id": "PIveJbqKZCpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Partial variables`\n",
        "\n",
        "Las partial_variables son un mecanismo en LangChain para pre-rellenar variables en un prompt de manera parcial, antes de su uso final.  \n",
        "\n",
        "Pi√©nsalo de esta manera: un PromptTemplate es como una plantilla de texto con \"huecos\" que necesitas llenar para crear un prompt completo para el LLM. Hay dos formas principales de llenar estos huecos:\n",
        "\n",
        "-   **input\\_variables:** Estas son las variables que **cambian** cada vez que utilizas el prompt. Son los datos espec√≠ficos que quieres que el LLM procese en cada llamada.\n",
        "    \n",
        "-   **partial\\_variables:** Estas son las variables que tienen un valor **fijo** o **predefinido** para un uso particular del PromptTemplate. No cambian con cada llamada a la cadena o LLM que usa este prompt."
      ],
      "metadata": {
        "id": "sjDo8JmHxKb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo nuestro PromptTemplate tiene 4 \"huecos\" o 4 variables. Pero al crearlo hemos precargado 3 de ellas con valores via partial variables. De esta al invocar el prompt (es un runnable !!) basta que pasemos una (tema).   \n",
        "Sin imbargo podriamos pasar tambien las restanteas..."
      ],
      "metadata": {
        "id": "MsyxZE-iyC8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Prompt con partial_variables\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Eres un {role} especializado en {area}. {instrucciones}\",\n",
        "    input_variables=[\"tema\"],\n",
        "    partial_variables={\n",
        "        \"role\": \"analista\",\n",
        "        \"area\": \"tecnolog√≠a\",\n",
        "        \"instrucciones\": \"Proporciona un an√°lisis detallado y objetivo.\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Uso del prompt\n",
        "resultado = prompt.invoke({\"tema\": \"Inteligencia Artificial\"})\n",
        "rprint(f\"[bold bright_cyan]{resultado}\")\n",
        "\n",
        "resultado = prompt.invoke({\"tema\": \"Inteligencia Artificial\", \"instrucciones\": \"Contesta con un pareado que rime\"})\n",
        "rprint(f\"[bold spring_green3]{resultado}\")\n"
      ],
      "metadata": {
        "id": "i0zpPWv1xu73",
        "outputId": "c2a97ee0-4bc0-44fb-f159-000703e1078d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;96mtext\u001b[0m\u001b[1;96m=\u001b[0m\u001b[1;96m'Eres un analista especializado en tecnolog√≠a. Proporciona un an√°lisis detallado y objetivo.'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">text</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">=</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">'Eres un analista especializado en tecnolog√≠a. Proporciona un an√°lisis detallado y objetivo.'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mtext\u001b[0m\u001b[1;38;5;41m=\u001b[0m\u001b[1;38;5;41m'Eres un analista especializado en tecnolog√≠a. Contesta con un pareado que rime'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">text</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">=</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'Eres un analista especializado en tecnolog√≠a. Contesta con un pareado que rime'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un uso tipico de las partial_variables es usarlas para introducir en el prompt las instrucciones de l formato que proporciona Langchain"
      ],
      "metadata": {
        "id": "3Tz1VTF1zL-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `.get_format_instructions()`\n",
        "\n",
        "**Obtener instrucciones de formato (opcional pero recomendado):** Muchos parsers tienen un m√©todo get\\_format\\_instructions() que devuelve texto que puedes incluir en tu prompt para guiar al LLM sobre el formato esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDsh5c2vaw2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "id": "2ov8pnCtaeFT",
        "outputId": "9d138c7b-61a9-4785-84f7-46efa13d278d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podriamos redactar nosotros mismos las instrucciones ? SI, sin duda. Esto es solo una funcion de utilidad que nos proporciona Langchain. Teoricamnte disponemos de esta forma de una redaccion tecnicamente correcta."
      ],
      "metadata": {
        "id": "N4xnI29VzrbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. CommaSeparatedListOutputParser**\n",
        "---\n",
        "\n",
        "El `CommaSeparatedListOutputParser` en Langchain es un OutputParser **simple pero muy √∫til** dise√±ado para **interpretar la salida de un modelo de lenguaje (LLM) como una lista de elementos separados por comas.** Su funci√≥n principal es tomar el texto generado por el LLM y **transformarlo en una lista de strings de Python**, donde cada string representa un elemento de la lista original que estaba separado por comas en el texto del LLM.\n",
        "\n",
        "  \n",
        "\n",
        "Imagina que le pides a un LLM que te d√© \"tres colores primarios separados por comas\". Podr√≠as esperar una respuesta como:\n",
        "\n",
        "\"rojo, azul, amarillo\"\n",
        "\n",
        "El CommaSeparatedListOutputParser toma esta cadena \"rojo, azul, amarillo\" y la procesa de la siguiente manera:\n",
        "\n",
        "- **Divide la cadena:** Utiliza la coma (,) como delimitador para dividir la cadena en partes m√°s peque√±as.\n",
        "    \n",
        "- **Elimina espacios en blanco (opcional):** Puede configurarse para eliminar espacios en blanco al principio y al final de cada parte extra√≠da. Por defecto, suele hacerlo para limpiar la lista resultante.\n",
        "    \n",
        "- **Crea la lista:** Cada parte resultante se convierte en un elemento de una lista de Python.\n",
        "    \n",
        "-   **Casos de uso:** Obtener listas de elementos, como nombres, ideas, pasos a seguir, o categor√≠as.\n",
        "    \n",
        "-   **Ventaja:** Simple y efectivo para extraer listas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "flcuOq9Nh6k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß©Ejemplo: Lista de ingredientes\n",
        "Queremos obtener una lista de ingredientes para hacer una pizza casera y solo nos interesa la lista de ingredientes, pues la procesaremos posteriormente de alguan forma.\n",
        "\n",
        "üëÄObserva el uso de las partial_variables paara introducir en el prompt las instrucciones de formato\n",
        "\n",
        "üëÄObserva tambien el tipo de las dos respuestas. El primero es un string y no podriamos iterarlo. El segundo es una lista de python, si podemos iterarla."
      ],
      "metadata": {
        "id": "ILCufEkEiWLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Genera una lista de ingredientes para hacer {receta} casera. Solo lista los ingredientes, uno por l√≠nea. Sin opciones\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Generar la salida\n",
        "prompt = prompt_template.format(receta=\"pizza\")\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "# Mostrar los resultados\n",
        "rprint(f\"Respuesta del modelo SIN FORMATEAR:\\n [bold bright_cyan]{respuesta}\")\n",
        "rprint(type(respuesta))\n",
        "rprint(\"\\n\\n\")\n",
        "rprint(f\"Respuesta del modelo FORMATEADA:\\n [bold spring_green3]{respuesta_formateada}\")\n",
        "rprint(type(respuesta_formateada))\n"
      ],
      "metadata": {
        "id": "AySI2dvhi78N",
        "outputId": "46e1520d-650e-47ad-f6ea-6e09b0027e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo SIN FORMATEAR:\n",
              " \u001b[1;96mharina, agua, levadura, sal, aceite de oliva, salsa de tomate, queso mozzarella, or√©gano, pepperoni, pimientos, \u001b[0m\n",
              "\u001b[1;96mchampi√±ones, cebolla\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo SIN FORMATEAR:\n",
              " <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">harina, agua, levadura, sal, aceite de oliva, salsa de tomate, queso mozzarella, or√©gano, pepperoni, pimientos, </span>\n",
              "<span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">champi√±ones, cebolla</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo FORMATEADA:\n",
              " \u001b[1;38;5;41m[\u001b[0m\u001b[1;38;5;41m'harina'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'agua'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'levadura'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'sal'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'aceite de oliva'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'salsa de tomate'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'queso mozzarella'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'or√©gano'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\n",
              "\u001b[1;38;5;41m'pepperoni'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'pimientos'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'champi√±ones'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'cebolla'\u001b[0m\u001b[1;38;5;41m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo FORMATEADA:\n",
              " <span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">[</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'harina'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'agua'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'levadura'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'sal'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'aceite de oliva'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'salsa de tomate'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'queso mozzarella'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'or√©gano'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span>\n",
              "<span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'pepperoni'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'pimientos'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'champi√±ones'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'cebolla'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Ejemplo: Lista de etiquetas  \n",
        "Deseamos que el modelo genere una lista de hastags (o etiquetas) a partir del tema de un articulo"
      ],
      "metadata": {
        "id": "1xh-L0f-7gFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "\n",
        "# Crear el ListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"\"\"Dame 5 etiquetas relevantes para un post de blog sobre: {tema}.\n",
        "                Las etiquetas deben estar separadas por comas.\"\"\",\n",
        "    input_variables=[\"tema\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "\n",
        "# Formatear el prompt con el tema del blog\n",
        "prompt = prompt_template.format(tema=\"Recetas de cocina vegana f√°ciles y r√°pidas para principiantes\")\n",
        "\n",
        "# Obtener la salida del LLM\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "# Mostrar los resultados\n",
        "rprint(f\"Respuesta del modelo SIN FORMATEAR:\\n [bold bright_cyan]{respuesta}\")\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(\"\\n\\n\")\n",
        "\n",
        "rprint(f\"Respuesta del modelo FORMATEADA:\\n [bold spring_green3]{respuesta_formateada}\")\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "rprint(\"\\n\\n\")\n",
        "\n",
        "# Imprimir las etiquetas generadas\n",
        "print(\"Etiquetas sugeridas:\")\n",
        "for etiqueta in respuesta_formateada:\n",
        "    rprint(f\"[bold spring_green3] - {etiqueta.strip()}\")\n"
      ],
      "metadata": {
        "id": "9p3CwHtv8Svc",
        "outputId": "a072390e-2c81-4d23-9def-11fbb52d8c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo SIN FORMATEAR:\n",
              " \u001b[1;96mrecetas veganas, cocina f√°cil, principiantes, comida saludable, recetas r√°pidas\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo SIN FORMATEAR:\n",
              " <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold\">recetas veganas, cocina f√°cil, principiantes, comida saludable, recetas r√°pidas</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta del modelo FORMATEADA:\n",
              " \u001b[1;38;5;41m[\u001b[0m\u001b[1;38;5;41m'recetas veganas'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'cocina f√°cil'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'principiantes'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'comida saludable'\u001b[0m\u001b[1;38;5;41m, \u001b[0m\u001b[1;38;5;41m'recetas r√°pidas'\u001b[0m\u001b[1;38;5;41m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta del modelo FORMATEADA:\n",
              " <span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">[</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'recetas veganas'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'cocina f√°cil'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'principiantes'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'comida saludable'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">, </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">'recetas r√°pidas'</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'list'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Etiquetas sugeridas:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - recetas veganas\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - recetas veganas</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - cocina f√°cil\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - cocina f√°cil</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - principiantes\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - principiantes</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - comida saludable\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - comida saludable</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41m - recetas r√°pidas\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> - recetas r√°pidas</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. EnumOutputParser**\n",
        "---\n",
        "\n",
        "`EnumOutputParser` es un tipo de output parser en LangChain que se utiliza para restringir la salida de un modelo a un conjunto predefinido de valores. Esto es √∫til cuando se desea que la respuesta del modelo pertenezca a un conjunto espec√≠fico de opciones, como categor√≠as, estados o tipos.\n",
        "\n",
        "### Caracter√≠sticas Principales:\n",
        "\n",
        "-   **Restricci√≥n de Valores**: Permite definir un conjunto limitado de opciones que el modelo puede devolver.\n",
        "-   **Validaci√≥n Autom√°tica**: Si la salida del modelo no coincide con las opciones definidas, se puede manejar como un error.\n",
        "-   **Facilita la Consistencia**: Asegura que las respuestas sean coherentes y dentro de un rango esperado.\n",
        "\n"
      ],
      "metadata": {
        "id": "lj97SmlAofVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Ejemplo de Caso de Uso: Clasificaci√≥n de Sentimientos\n",
        "\n",
        "Imaginemos que estamos construyendo un sistema que clasifica el sentimiento de comentarios de clientes sobre un producto. Queremos que el modelo devuelva solo tres categor√≠as: \"positivo\", \"negativo\" y \"neutral\".\n",
        "\n",
        "Este ejemplo que NO FUNCIONA muestra las limitaciones de los OutputParsers pero tambien unas de sus utilidades, poder hacer validaciones y atrapar el error."
      ],
      "metadata": {
        "id": "S94-9qKkFiJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definimos las opciones de sentimiento\n",
        "class Sentimientos(Enum):\n",
        "    POSITIVO = \"positivo\"\n",
        "    NEGATIVO = \"negativo\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=Sentimientos)\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Clasifica el siguiente comentario: {comentario}\",\n",
        "    input_variables=[\"comentario\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "\n",
        "# Ejemplo de uso\n",
        "comentario_cliente = \"Me encanta este producto, es incre√≠ble y funciona muy bien.\"\n",
        "\n",
        "prompt = prompt_template.format(comentario=comentario_cliente)\n",
        "\n",
        "# Clasificamos el sentimiento\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "try:\n",
        "    respuesta_formateada = output_parser.parse(respuesta)\n",
        "except Exception as e:\n",
        "    respuesta_formateada = \"No se pudo clasificar el sentimiento\"\n",
        "    rprint(f\"[bold white on red]Error al parsear la salida: {e}\")\n",
        "\n",
        "\n",
        "rprint(\"Sentimiento Clasificado:\")\n",
        "rprint(respuesta_formateada)\n"
      ],
      "metadata": {
        "id": "R-xNyOZNFsHh",
        "outputId": "05535065-b960-4cb3-d8b3-a14048fc2ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;37;41mError al parsear la salida: Response \u001b[0m\u001b[1;37;41m'El comentario se puede clasificar como **positivo**. Expresa satisfacci√≥n y \u001b[0m\n",
              "\u001b[1;37;41maprecio por el producto, destacando su efectividad.'\u001b[0m\u001b[1;37;41m is not one of the expected values: \u001b[0m\u001b[1;37;41m[\u001b[0m\u001b[1;37;41m'positivo'\u001b[0m\u001b[1;37;41m, \u001b[0m\u001b[1;37;41m'negativo'\u001b[0m\u001b[1;37;41m, \u001b[0m\n",
              "\u001b[1;37;41m'neutral'\u001b[0m\u001b[1;37;41m]\u001b[0m\n",
              "\u001b[1;37;41mFor troubleshooting, visit: \u001b[0m\u001b[1;4;37;41mhttps://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\u001b[0m\u001b[1;37;41m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">Error al parsear la salida: Response </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">'El comentario se puede clasificar como **positivo**. Expresa satisfacci√≥n y </span>\n",
              "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">aprecio por el producto, destacando su efectividad.'</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\"> is not one of the expected values: [</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">'positivo'</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">, </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">'negativo'</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">, </span>\n",
              "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">'neutral'</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">]</span>\n",
              "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\">For troubleshooting, visit: </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold; text-decoration: underline\">https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #800000; font-weight: bold\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sentimiento Clasificado:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sentimiento Clasificado:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "No se pudo clasificar el sentimiento\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No se pudo clasificar el sentimiento\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que a pesar de que el modelo si evalua correctaement el sentimiento, NO DEVUELVE EXACTAMENTE lo que nececitamos. Y el OutputParser no consigue extraer de la resuesta el valor buscado.\n",
        "\n",
        "Lanchain lanteas esta opciones (en el enlace):\n",
        "\n",
        "- Usar `with_structured_output`\n",
        "- Usar LangGraph\n",
        "- Mejorar el prompt\n",
        "- Cambiar de modelo\n",
        "- Usar reintentos (con algun parserfixing)\n",
        "\n",
        "En este momento, la forma disponible para nosotros y ademas la mas simple y directa es simplemente mejorar el prompt"
      ],
      "metadata": {
        "id": "GkG3JiQuTRHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from langchain.output_parsers import EnumOutputParser\n",
        "\n",
        "# Definimos las opciones de sentimiento\n",
        "class Sentimientos(Enum):\n",
        "    POSITIVO = \"positivo\"\n",
        "    NEGATIVO = \"negativo\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "# Crear el EnumOutputParser\n",
        "output_parser = EnumOutputParser(enum=Sentimientos)\n",
        "\n",
        "# Crear el prompt MUCHO MAS PRECISO !!!\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Clasifica el siguiente comentario: {comentario} Contesta solo con 'positivo', 'negativo' o 'neutral y nada m√°s\",\n",
        "    input_variables=[\"comentario\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "\n",
        "# Ejemplo de uso\n",
        "comentario_cliente = \"Me encanta este producto, es incre√≠ble y funciona muy bien.\"\n",
        "\n",
        "prompt = prompt_template.format(comentario=comentario_cliente)\n",
        "\n",
        "# Clasificamos el sentimiento\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la salida\n",
        "try:\n",
        "    respuesta_formateada = output_parser.parse(respuesta)\n",
        "except Exception as e:\n",
        "    respuesta_formateada = \"No se pudo clasificar el sentimiento\"\n",
        "    rprint(f\"[bold white on red]Error al parsear la salida: {e}\")\n",
        "\n",
        "\n",
        "rprint(\"Sentimiento Clasificado:\")\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n"
      ],
      "metadata": {
        "outputId": "4ddd8057-e2b9-4a90-8a01-00fecc494e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "ryp8xq5tUte4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sentimiento Clasificado:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sentimiento Clasificado:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sentimientos.POSITIVO\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sentimientos.POSITIVO\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95menum\u001b[0m\u001b[39m \u001b[0m\u001b[32m'Sentimientos'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">enum</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'Sentimientos'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¬ø Que pinta aqui pues el OutputParser ? Esto lo podriamos simplemente con el prompt. SI.  \n",
        "\n",
        "El OutputParser nos proporciona validacion de datos, que es importante sobre todo en el momento del desarrolo y tipo de datos. El resultado NO es un string, sino algo del tipo Enum que hemos definido que por ejemplo podemos iterar.\n",
        "\n",
        "https://rico-schmidt.name/pymotw-3/enum/index.html"
      ],
      "metadata": {
        "id": "eycDlLi1Vpn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. SimpleJsonOutputParser**\n",
        "---\n",
        "\n",
        "El SimpleJsonOutputParser es una herramienta de LangChain que se utiliza para convertir el texto de salida de un modelo de lenguaje en un objeto JSON estructurado. Es particularmente √∫til cuando necesitas obtener datos estructurados de tus LLMs (Large Language Models).\n",
        "\n",
        "### Funcionalidad principal\n",
        "\n",
        "- Convierte respuestas de texto en formato JSON\n",
        "- Maneja errores de parseo\n",
        "- Es sencillo de implementar en tu flujo de trabajo con LLMs"
      ],
      "metadata": {
        "id": "dW90EPu2eqQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Ejemplo : Queremos obtener cierta informacion de una ciudad para procesarla posteriormente"
      ],
      "metadata": {
        "id": "6wQYvxFRkVbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import SimpleJsonOutputParser\n",
        "\n",
        "\n",
        "# Crear el parser\n",
        "parser = SimpleJsonOutputParser()\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Hacer una consulta que espera una respuesta estructurada\n",
        "# Importante aqui las dobles llaves para que no si interpretne como variables\n",
        "template = \"\"\"\n",
        "Proporciona informaci√≥n sobre {ciudad} con el siguiente formato:\n",
        "\n",
        "{{\n",
        "  \"ciudad\": \"nombre de la ciudad\",\n",
        "  \"pais\": \"pa√≠s donde se encuentra\",\n",
        "  \"poblacion\": \"n√∫mero aproximado de habitantes\",\n",
        "  \"atracciones\": [\"lista\", \"de\", \"atracciones\", \"principales\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"ciudad\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "prompt = prompt_template.format(ciudad=\"Barcelona\")\n",
        "\n",
        "# Obtener la respuesta del modelo\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parsear la respuesta a JSON\n",
        "resultado = parser.parse(respuesta)\n",
        "\n",
        "# Ahora resultado es un diccionario Python\n",
        "rprint(resultado)\n",
        "rprint(type(resultado))  # <class 'dict'>\n",
        "rprint(f\"Primera atracci√≥n: {resultado['atracciones'][0]}\")\n"
      ],
      "metadata": {
        "id": "ApLQw73De5XJ",
        "outputId": "96ac95a4-6d90-471f-df92-ab4d330f1315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'ciudad'\u001b[0m: \u001b[32m'Barcelona'\u001b[0m,\n",
              "    \u001b[32m'pais'\u001b[0m: \u001b[32m'Espa√±a'\u001b[0m,\n",
              "    \u001b[32m'poblacion'\u001b[0m: \u001b[32m'aproximadamente 1.6 millones'\u001b[0m,\n",
              "    \u001b[32m'atracciones'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[32m'Sagrada Familia'\u001b[0m,\n",
              "        \u001b[32m'Parque G√ºell'\u001b[0m,\n",
              "        \u001b[32m'La Rambla'\u001b[0m,\n",
              "        \u001b[32m'Casa Batll√≥'\u001b[0m,\n",
              "        \u001b[32m'Barrio G√≥tico'\u001b[0m,\n",
              "        \u001b[32m'Camp Nou'\u001b[0m,\n",
              "        \u001b[32m'Museo Picasso'\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'ciudad'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Barcelona'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'pais'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Espa√±a'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'poblacion'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'aproximadamente 1.6 millones'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'atracciones'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'Sagrada Familia'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'Parque G√ºell'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'La Rambla'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'Casa Batll√≥'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'Barrio G√≥tico'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'Camp Nou'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'Museo Picasso'</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'dict'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Primera atracci√≥n: Sagrada Familia\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Primera atracci√≥n: Sagrada Familia\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Ejemplo: Analisis de sentimientos (un poco m√°s elaborado)\n",
        "\n",
        "üëÄ Observa que aqui no vamos a usar PromptTempplate y directamnete nos las arreglamos con f-strings"
      ],
      "metadata": {
        "id": "M1e14pt6kv8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import SimpleJsonOutputParser\n",
        "\n",
        "# Crear el parser\n",
        "parser = SimpleJsonOutputParser()\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "\n",
        "# Comentario de un cliente sobre un producto\n",
        "comentario_cliente = \"\"\"\n",
        "Compr√© este tel√©fono hace un mes y estoy muy contento con la calidad de la c√°mara y la bater√≠a dura todo el d√≠a.\n",
        "Sin embargo, el software tiene algunos fallos y a veces se congela cuando uso m√∫ltiples aplicaciones al mismo tiempo.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para analizar el sentimiento y extraer insights\n",
        "prompt = f\"\"\"\n",
        "Analiza el siguiente comentario de un cliente y devuelve la informaci√≥n en formato JSON:\n",
        "\n",
        "Comentario: {comentario_cliente}\n",
        "\n",
        "El JSON debe tener esta estructura:\n",
        "{{\n",
        "  \"sentimiento_general\": \"positivo/negativo/neutral\",\n",
        "  \"puntuacion\": \"valor num√©rico de 1 a 10\",\n",
        "  \"aspectos_positivos\": [\"lista\", \"de\", \"aspectos\", \"positivos\"],\n",
        "  \"aspectos_negativos\": [\"lista\", \"de\", \"aspectos\", \"negativos\"],\n",
        "  \"recomendaciones\": [\"lista\", \"de\", \"posibles\", \"mejoras\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Obtener respuesta y parsear\n",
        "respuesta = modelo.invoke(prompt)\n",
        "respuestaf = parser.parse(response.content)\n",
        "\n",
        "# Usar los datos estructurados\n",
        "rprint(f\"Sentimiento: {respuestaf['sentimiento_general']}\")\n",
        "rprint(f\"Puntuaci√≥n: {respuestaf['puntuacion']}/10\")\n",
        "rprint(\"Aspectos positivos:\", \", \".join(respuestaf[\"aspectos_positivos\"]))\n",
        "rprint(\"Aspectos negativos:\", \", \".join(respuestaf[\"aspectos_negativos\"]))"
      ],
      "metadata": {
        "id": "7dUWXHIHlUY2",
        "outputId": "c190a2eb-e51d-408d-e4ab-aec047a60e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sentimiento: positivo\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sentimiento: positivo\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Puntuaci√≥n: \u001b[1;36m7\u001b[0m/\u001b[1;36m10\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Puntuaci√≥n: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Aspectos positivos: calidad de la c√°mara, bater√≠a dura todo el d√≠a\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Aspectos positivos: calidad de la c√°mara, bater√≠a dura todo el d√≠a\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Aspectos negativos: fallos en el software, se congela al usar m√∫ltiples aplicaciones\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Aspectos negativos: fallos en el software, se congela al usar m√∫ltiples aplicaciones\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. JsonOutputParser**\n",
        "---\n",
        "\n",
        "JsonOutputParser es otro OutputParser dissponible en el framewwork LangChain.\n",
        "\n",
        "## Principales diferencias\n",
        "\n",
        "1. **Complejidad y flexibilidad**:\n",
        "    - **SimpleJsonOutputParser**: Como su nombre indica, es m√°s simple. Toma una cadena de texto que contiene JSON v√°lido y la convierte en un objeto Python.\n",
        "    - **JsonOutputParser**: Es m√°s complejo y flexible, permitiendo definir un esquema espec√≠fico para la estructura JSON esperada.\n",
        "2. **Esquemas y validaci√≥n**:\n",
        "    - **SimpleJsonOutputParser**: No requiere definir un esquema previo; simplemente intenta parsear cualquier JSON v√°lido.\n",
        "    - **JsonOutputParser**: Requiere definir la estructura esperada, lo que proporciona validaci√≥n y gu√≠a al LLM sobre c√≥mo estructurar su respuesta.\n",
        "3. **Instrucciones al modelo**:\n",
        "    - **SimpleJsonOutputParser**: No genera instrucciones espec√≠ficas para el modelo.\n",
        "    - **JsonOutputParser**: Proporciona instrucciones detalladas al modelo sobre el formato requerido con `get_format_instructions()`.\n",
        "4. **Manejo de errores**:\n",
        "    - **SimpleJsonOutputParser**: Manejo b√°sico de errores de parseo.\n",
        "    - **JsonOutputParser**: Manejo m√°s robusto de errores con validaci√≥n contra el esquema definido.\n",
        "\n",
        "## ¬øCu√°ndo usar cada uno?\n",
        "\n",
        "**Usa SimpleJsonOutputParser cuando:**\n",
        "\n",
        "- Necesitas una soluci√≥n r√°pida y sencilla\n",
        "- La estructura JSON puede variar o no es cr√≠tica\n",
        "- No requieres validaci√≥n estricta del esquema\n",
        "\n",
        "**Usa JsonOutputParser cuando:**\n",
        "\n",
        "- Necesitas validar contra un esquema espec√≠fico\n",
        "- Quieres proporcionar instrucciones detalladas al modelo\n",
        "- La estructura de datos es compleja o cr√≠tica para tu aplicaci√≥n\n",
        "- Trabajas con tipos de datos espec√≠ficos que requieren validaci√≥n\n",
        "\n",
        "El JsonOutputParser es especialmente √∫til en escenarios empresariales donde la consistencia y validaci√≥n de los datos son cruciales, como en an√°lisis de productos, extracci√≥n de informaci√≥n de documentos, o procesamiento de datos estructurados desde texto libre."
      ],
      "metadata": {
        "id": "626Vr1c5uzj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Ejemplo: Deseamos obtener la bibliografia de un autor\n",
        "\n",
        "üëÄ Observa lo importante que es el prompt para conseguir que el modelo cree una respuesta que pueda ser parseada por el OutputParser.\n",
        "\n",
        "El OutputParser ademas de validacion de datos nos propociona los datos en el tipo deseado y no una simple repesentacion en string. **Observa los tipos de de respuesta y respuesta_formateada**"
      ],
      "metadata": {
        "id": "3PzKccq3tXXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Definir la estructura para un libro\n",
        "class Libro(BaseModel):\n",
        "    titulo: str = Field(description=\"T√≠tulo del libro\")\n",
        "    a√±o: int = Field(description=\"A√±o de publicaci√≥n\")\n",
        "\n",
        "# Definir la estructura para la bibliograf√≠a completa\n",
        "class Bibliografia(BaseModel):\n",
        "    bibliografia: List[Libro] = Field(description=\"Lista de libros publicados por el autor\")\n",
        "\n",
        "# Crear el JsonOutputParser\n",
        "output_parser = JsonOutputParser(pydantic_model=Bibliografia)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "Proporciona √öNICAMENTE la bibliograf√≠a del autor: {autor}.\n",
        "\n",
        "INSTRUCCIONES ESTRICTAS:\n",
        "1. Sigue EXACTAMENTE el formato JSON especificado a continuaci√≥n.\n",
        "2. NO incluyas campos adicionales como nacionalidad, nacimiento, fallecimiento, premios, etc.\n",
        "3. Solo crea una lista de sus libros con t√≠tulo y a√±o.\n",
        "4. La lista de libros debe usar la clave \"bibliografia\", no \"obras\" ni ninguna otra.\n",
        "5. Cada libro debe tener SOLO los campos \"titulo\" y \"a√±o\".\n",
        "\n",
        "Es CR√çTICO seguir este formato exacto:\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "IMPORTANTE: No agregues ning√∫n campo que no est√© especificado en el esquema. Tu respuesta debe ser √∫nicamente un JSON v√°lido con la estructura exacta solicitada.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"autor\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "autor = \"Gabriel Garc√≠a M√°rquez\"\n",
        "prompt = prompt_template.format(autor=autor)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Analizamos la respuesta\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Analizamos la respuesta\n",
        "try:\n",
        "    respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "    # Imprimimos los resultados\n",
        "    print(f\"Autor: {autor}\")\n",
        "    print(\"Bibliograf√≠a:\")\n",
        "    for libro in respuesta_formateada[\"bibliografia\"]:\n",
        "        print(f\"A√±o: {libro['a√±o']}, T√≠tulo: {libro['titulo']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al parsear la respuesta: {e}\")\n",
        "    print(\"La respuesta del modelo no cumple con el esquema esperado.\")"
      ],
      "metadata": {
        "id": "wgjgzWrSvjSo",
        "outputId": "d02f481d-32bb-4c5a-c1df-3ea4032af340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "```json\n",
              "\u001b[1m{\u001b[0m\n",
              "  \u001b[32m\"bibliografia\"\u001b[0m: \u001b[1m[\u001b[0m\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"Cien a√±os de soledad\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m1967\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"El oto√±o del patriarca\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m1975\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"Cr√≥nica de una muerte anunciada\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m1981\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"El amor en los tiempos del c√≥lera\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m1985\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"El general en su laberinto\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m1989\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"Del amor y otros demonios\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m1994\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"El ruido y la furia\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m1996\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "      \u001b[32m\"titulo\"\u001b[0m: \u001b[32m\"Memoria de mis putas tristes\"\u001b[0m,\n",
              "      \u001b[32m\"a√±o\"\u001b[0m: \u001b[1;36m2004\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "  \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n",
              "```\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```json\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"bibliografia\"</span>: <span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Cien a√±os de soledad\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1967</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"El oto√±o del patriarca\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1975</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Cr√≥nica de una muerte anunciada\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1981</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"El amor en los tiempos del c√≥lera\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1985</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"El general en su laberinto\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1989</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Del amor y otros demonios\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1994</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"El ruido y la furia\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1996</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"titulo\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Memoria de mis putas tristes\"</span>,\n",
              "      <span style=\"color: #008000; text-decoration-color: #008000\">\"a√±o\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2004</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "  <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "```\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'bibliografia'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'Cien a√±os de soledad'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m1967\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'El oto√±o del patriarca'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m1975\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'Cr√≥nica de una muerte anunciada'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m1981\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'El amor en los tiempos del c√≥lera'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m1985\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'El general en su laberinto'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m1989\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'Del amor y otros demonios'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m1994\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'El ruido y la furia'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m1996\u001b[0m\u001b[1m}\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'titulo'\u001b[0m: \u001b[32m'Memoria de mis putas tristes'\u001b[0m, \u001b[32m'a√±o'\u001b[0m: \u001b[1;36m2004\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'bibliografia'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Cien a√±os de soledad'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1967</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'El oto√±o del patriarca'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1975</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Cr√≥nica de una muerte anunciada'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1981</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'El amor en los tiempos del c√≥lera'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1985</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'El general en su laberinto'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1989</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Del amor y otros demonios'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1994</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'El ruido y la furia'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1996</span><span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'titulo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Memoria de mis putas tristes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'a√±o'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2004</span><span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'dict'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autor: Gabriel Garc√≠a M√°rquez\n",
            "Bibliograf√≠a:\n",
            "A√±o: 1967, T√≠tulo: Cien a√±os de soledad\n",
            "A√±o: 1975, T√≠tulo: El oto√±o del patriarca\n",
            "A√±o: 1981, T√≠tulo: Cr√≥nica de una muerte anunciada\n",
            "A√±o: 1985, T√≠tulo: El amor en los tiempos del c√≥lera\n",
            "A√±o: 1989, T√≠tulo: El general en su laberinto\n",
            "A√±o: 1994, T√≠tulo: Del amor y otros demonios\n",
            "A√±o: 1996, T√≠tulo: El ruido y la furia\n",
            "A√±o: 2004, T√≠tulo: Memoria de mis putas tristes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que el tipo de la respuesta obtenida es direcatmente un diccionario de Python"
      ],
      "metadata": {
        "id": "lqPpK1M-1U4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß© Ejemplo: Analisis de producto detallado**\n",
        "---\n",
        "Dada un descripcion textual de un articulo, deseamos obtener un analisis detallado del producto con cierta estructura de datos\n"
      ],
      "metadata": {
        "id": "_xc1O4oFd8SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "# Definir el esquema que esperamos recibir\n",
        "class ProductoAnalisis(BaseModel):\n",
        "    nombre: str = Field(description=\"Nombre del producto analizado\")\n",
        "    categoria: str = Field(description=\"Categor√≠a principal del producto\")\n",
        "    ventajas: List[str] = Field(description=\"Lista de puntos fuertes o ventajas del producto\")\n",
        "    desventajas: List[str] = Field(description=\"Lista de puntos d√©biles o desventajas del producto\")\n",
        "    puntuacion: int = Field(description=\"Puntuaci√≥n de 1 a 10\")\n",
        "    recomendado: bool = Field(description=\"Si el producto es recomendable\")\n",
        "    mejor_para: List[str] = Field(description=\"Tipos de usuarios para los que este producto es m√°s adecuado\")\n",
        "\n",
        "# Crear el parser con nuestro esquema\n",
        "parser = JsonOutputParser(pydantic_model=ProductoAnalisis)\n",
        "\n",
        "# Inicializar el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.0)\n",
        "\n",
        "# Crear el template con las instrucciones de formato\n",
        "template = \"\"\"\n",
        "Analiza el siguiente producto y proporciona un an√°lisis detallado:\n",
        "\n",
        "Producto: {producto}\n",
        "\n",
        "Es CR√çTICO seguir este formato exacto:\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Debes realizar un an√°lisis del producto indicado. Presenta los resultados estrictamente siguiendo este esquema:\n",
        "\n",
        "nombre: (Nombre exacto del producto)\n",
        "\n",
        "categoria: (Categor√≠a principal a la que pertenece el producto)\n",
        "\n",
        "ventajas:\n",
        "\n",
        "(Lista claramente identificada de puntos fuertes o ventajas)\n",
        "\n",
        "desventajas:\n",
        "\n",
        "(Lista claramente identificada de puntos d√©biles o desventajas)\n",
        "\n",
        "puntuacion: (Valoraci√≥n num√©rica del producto entre 1 y 10, donde 10 es excelente)\n",
        "\n",
        "recomendado: (Indica expl√≠citamente \"S√≠\" o \"No\" dependiendo de si recomiendas el producto)\n",
        "\n",
        "mejor_para:\n",
        "\n",
        "(Lista concreta de perfiles o tipos de usuarios para los que este producto ser√≠a m√°s adecuado)\n",
        "\n",
        "Aseg√∫rate de completar cada secci√≥n de manera detallada y precisa, sin omitir ning√∫n campo.\n",
        "\n",
        "IMPORTANTE: No agregues ning√∫n campo que no est√© especificado en el esquema. Tu respuesta debe ser √∫nicamente un JSON v√°lido con la estructura exacta solicitada.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"producto\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Producto a analizar\n",
        "descripcion_producto = \"\"\"\n",
        "Auriculares inal√°mbricos XSound Pro - Con cancelaci√≥n activa de ruido,\n",
        "30 horas de bater√≠a, conexi√≥n Bluetooth 5.2, resistencia al agua IPX4,\n",
        "y sistema de micr√≥fono dual para llamadas. Precio: 129,99‚Ç¨\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format(producto=descripcion_producto)\n",
        "\n",
        "\n",
        "# Obtener y parsear la respuesta\n",
        "respuesta = modelo.invoke(prompt)\n",
        "respuestaf= parser.parse(respuesta.content)\n",
        "print(resultado)\n",
        "\n",
        "# Impresi√≥n estructurada\n",
        "print(f\"Nombre: {respuestaf['nombre']}\")\n",
        "print(f\"Categor√≠a: {respuestaf['categoria']}\\n\")\n",
        "\n",
        "print(\"Ventajas:\")\n",
        "for ventaja in respuestaf['ventajas']:\n",
        "    print(f\"- {ventaja}\")\n",
        "\n",
        "print(\"\\nDesventajas:\")\n",
        "for desventaja in respuestaf['desventajas']:\n",
        "    print(f\"- {desventaja}\")\n",
        "\n",
        "print(f\"\\nPuntuaci√≥n: {respuestaf['puntuacion']}/10\")\n",
        "print(f\"Recomendado: {respuestaf['recomendado']}\")\n",
        "\n",
        "print(\"\\nMejor para:\")\n",
        "for usuario in respuestaf['mejor_para']:\n",
        "    print(f\"- {usuario}\")"
      ],
      "metadata": {
        "id": "bLkXlzR64SNF",
        "outputId": "894b6b01-24fa-430e-81da-bc2e8e57ffad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nombre': 'Auriculares inal√°mbricos XSound Pro', 'categoria': 'Auriculares inal√°mbricos', 'ventajas': ['Cancelaci√≥n activa de ruido que mejora la experiencia auditiva en entornos ruidosos.', 'Hasta 30 horas de duraci√≥n de bater√≠a, lo que permite un uso prolongado sin necesidad de recarga frecuente.', 'Conexi√≥n Bluetooth 5.2 que ofrece una conexi√≥n m√°s estable y r√°pida.', 'Resistencia al agua IPX4, lo que los hace adecuados para actividades deportivas y uso en exteriores.', 'Sistema de micr√≥fono dual que mejora la calidad de las llamadas y reduce el ruido de fondo.'], 'desventajas': ['El precio de 129,99‚Ç¨ puede ser considerado elevado en comparaci√≥n con otros modelos en el mercado.', 'La resistencia al agua IPX4 es limitada y no es adecuada para inmersi√≥n total o condiciones de lluvia intensa.', 'La cancelaci√≥n activa de ruido puede no ser tan efectiva en frecuencias muy bajas.'], 'puntuacion': 8, 'recomendado': 'S√≠', 'mejor_para': ['Usuarios que buscan auriculares para uso diario en entornos ruidosos.', 'Deportistas que necesitan auriculares resistentes al agua para entrenamientos.', 'Profesionales que realizan muchas llamadas y requieren un micr√≥fono de buena calidad.', 'Personas que valoran una larga duraci√≥n de bater√≠a en sus dispositivos de audio.']}\n",
            "Nombre: Auriculares inal√°mbricos XSound Pro\n",
            "Categor√≠a: Auriculares inal√°mbricos\n",
            "\n",
            "Ventajas:\n",
            "- Cancelaci√≥n activa de ruido que mejora la experiencia auditiva en entornos ruidosos.\n",
            "- Autonom√≠a de 30 horas, ideal para uso prolongado sin necesidad de recarga frecuente.\n",
            "- Conexi√≥n Bluetooth 5.2 que ofrece una conexi√≥n m√°s estable y r√°pida.\n",
            "- Resistencia al agua IPX4, lo que los hace adecuados para actividades deportivas y uso en exteriores.\n",
            "- Sistema de micr√≥fono dual que mejora la calidad de las llamadas y reduce el ruido de fondo.\n",
            "\n",
            "Desventajas:\n",
            "- Precio relativamente alto en comparaci√≥n con otros modelos en el mercado.\n",
            "- La resistencia al agua IPX4 es limitada, no son completamente sumergibles.\n",
            "- La cancelaci√≥n activa de ruido puede no ser tan efectiva en todos los entornos.\n",
            "\n",
            "Puntuaci√≥n: 8/10\n",
            "Recomendado: S√≠\n",
            "\n",
            "Mejor para:\n",
            "- Usuarios que buscan auriculares para uso diario y actividades al aire libre.\n",
            "- Personas que trabajan en entornos ruidosos y necesitan concentraci√≥n.\n",
            "- Deportistas que requieren auriculares resistentes al agua.\n",
            "- Usuarios que realizan muchas llamadas y valoran la calidad de audio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. StructuredOutputParser**\n",
        "---\n",
        "StructuredOutputParser es un parser de salida estructurado pero sencillo pero a veces no necesitamos mucho mas.\n",
        "\n",
        "- **Prop√≥sito**: Generar respuestas estructuradas en formatos simples, ideal para modelos peque√±os o con restricciones.\n",
        "    \n",
        "- **Caracter√≠sticas clave**:  \n",
        "    ‚Ä¢ **Siempre devuelve un diccionario con campos de tipo _string_.**  \n",
        "    ‚Ä¢ Permite definir una estructura de salida predeterminada.  \n",
        "    ‚Ä¢ Menos complejo que opciones como _PydanticOutputParser_ (no soporta datos complejos).\n",
        "    \n",
        "- **Limitaciones**:  \n",
        "    ‚Ä¢ **Solo admite campos de texto (_string_).**  \n",
        "    ‚Ä¢ Menos flexible para escenarios que requieren tipos de datos avanzados.\n",
        "    \n",
        "- **Casos de uso**:  \n",
        "    ‚Ä¢ Proyectos con requisitos de estructura b√°sicos.  \n",
        "    ‚Ä¢ Compatibilidad con modelos de lenguaje menos potentes o entornos limitados.\n",
        "\n",
        "** Realmente puede manejar listas sencillas, pero es mejor evitarlo. Si debe haber listas en la respuesta, usa algun JSON *texto en cursiva*"
      ],
      "metadata": {
        "id": "4Zlw_DQiZPI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß© Ejemplo: Crear datos dummy de usuarios**\n",
        "Observa que por las limitaciones de este pareser, que solo genera strings la edad sera un string que habra que convertir."
      ],
      "metadata": {
        "id": "hl1_LP0hxtcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "esquema_respuesta = [\n",
        "    ResponseSchema(name=\"nombre\", description=\"El nombre del usuario\"),\n",
        "    ResponseSchema(name=\"edad\", description=\"La edad del usuario\"),\n",
        "    ResponseSchema(name=\"email\", description=\"El correo electr√≥nico del usuario\")\n",
        "]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(esquema_respuesta)\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Crear el prompt\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Genera informaci√≥n de un usuario ficticio.\\n{format_instructions}\\n\",\n",
        "    input_variables=[],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format()\n",
        "\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "rprint(\"\\n\")\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "rprint(\"\\n\")\n",
        "\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Nombre: {respuesta_formateada['nombre']}\")\n",
        "rprint(f\"Edad: {int(respuesta_formateada['edad'])}\")\n",
        "rprint(f\"Email: {respuesta_formateada['email']}\")\n"
      ],
      "metadata": {
        "id": "F4pnLdNTZP9r",
        "outputId": "d4836c9c-12c0-4a1d-9eba-d01d9383fca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "```json\n",
              "\u001b[1m{\u001b[0m\n",
              "        \u001b[32m\"nombre\"\u001b[0m: \u001b[32m\"Laura G√≥mez\"\u001b[0m,\n",
              "        \u001b[32m\"edad\"\u001b[0m: \u001b[32m\"28\"\u001b[0m,\n",
              "        \u001b[32m\"email\"\u001b[0m: \u001b[32m\"laura.gomez@example.com\"\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n",
              "```\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```json\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"nombre\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Laura G√≥mez\"</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"edad\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"28\"</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"laura.gomez@example.com\"</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "```\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'nombre'\u001b[0m: \u001b[32m'Laura G√≥mez'\u001b[0m, \u001b[32m'edad'\u001b[0m: \u001b[32m'28'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'laura.gomez@example.com'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'nombre'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Laura G√≥mez'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'edad'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'28'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'laura.gomez@example.com'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'dict'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Nombre: Laura G√≥mez\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Nombre: Laura G√≥mez\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Edad: \u001b[1;36m28\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Edad: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Email: laura.gomez@example.com\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Email: laura.gomez@example.com\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1_T9nPdbCIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß©Ejemplo: Creacion de un test**\n",
        "---\n",
        "\n",
        "Vamos a dise√±ar una consulta para un modelo de lenguaje (LLM) que sirva como base para generar un test de preguntas sobre un tema espec√≠fico. Proporcionaremos el tema y un nivel de dificultad (bajo, medio, alto), y el LLM deber√° generar el texto de la pregunta, tres opciones de respuesta y el √≠ndice de la respuesta correcta. La respuesta debe estar estructurada en un diccionario con el siguiente formato:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"pregunta\": \"Texto de la pregunta\",\n",
        "    \"opciones\": [\"Opci√≥n 1\", \"Opci√≥n 2\", \"Opci√≥n 3\"],\n",
        "    \"respuesta_correcta\": √≠ndice_de_la_opci√≥n_correcta\n",
        "}\n",
        "```\n",
        "\n",
        "Este formato permitir√° una f√°cil interpretaci√≥n y uso de la pregunta generada.\n"
      ],
      "metadata": {
        "id": "4-C1hmhItipv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "#Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "esquema_respuesta = [\n",
        "            ResponseSchema(name=\"pregunta\", description=\"Texto de la pregunta generada.\"),\n",
        "            ResponseSchema(name=\"opcion1\", description=\"Primera opci√≥n de respuesta.\"),\n",
        "            ResponseSchema(name=\"opcion2\", description=\"Segunda opci√≥n de respuesta.\"),\n",
        "            ResponseSchema(name=\"opcion3\", description=\"Tercera opci√≥n de respuesta.\"),\n",
        "            ResponseSchema(name=\"respuesta_correcta\", description=\"√çndice de la opci√≥n correcta (1, 2 o 3).\")\n",
        "                    ]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(esquema_respuesta)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "plantilla = \"\"\"\n",
        "        Genera una pregunta de test sobre el tema: {tema}.\n",
        "        El nivel de dificultad debe ser: {nivel}.\n",
        "        Genera tambien tres posibles respuestas (opcion1, opcion2, opcion3)\n",
        "        Una de ellas debe ser la correcta\n",
        "        Indica el numero (1,2,3) de la respuesta correcta (respuesta_correcta)\n",
        "\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=plantilla,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt).content\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Pregunta: {respuesta_formateada['pregunta']}\")\n",
        "rprint(f\"Opci√≥n 1: {respuesta_formateada['opcion1']}\")\n",
        "rprint(f\"Opci√≥n 2: {respuesta_formateada['opcion2']}\")\n",
        "rprint(f\"Opci√≥n 3: {respuesta_formateada['opcion3']}\")\n",
        "rprint(f\"Respuesta correcta: Opci√≥n {respuesta_formateada['respuesta_correcta']}\")"
      ],
      "metadata": {
        "id": "Mj2h1zwYhedp",
        "outputId": "d9f72723-dafa-44e0-d170-d033f025606b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt generado:\n",
            " \n",
            "        Genera una pregunta de test sobre el tema: LangChain.\n",
            "        El nivel de dificultad debe ser: medio.\n",
            "        Genera tambien tres posibles respuestas (opcion1, opcion2, opcion3)\n",
            "        Una de ellas debe ser la correcta\n",
            "        Indica el numero (1,2,3) de la respuesta correcta (respuesta_correcta)\n",
            "        \n",
            "\n",
            "        The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"pregunta\": string  // Texto de la pregunta generada.\n",
            "\t\"opcion1\": string  // Primera opci√≥n de respuesta.\n",
            "\t\"opcion2\": string  // Segunda opci√≥n de respuesta.\n",
            "\t\"opcion3\": string  // Tercera opci√≥n de respuesta.\n",
            "\t\"respuesta_correcta\": string  // √çndice de la opci√≥n correcta (1, 2 o 3).\n",
            "}\n",
            "```\n",
            "        \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "```json\n",
              "\u001b[1m{\u001b[0m\n",
              "        \u001b[32m\"pregunta\"\u001b[0m: \u001b[32m\"¬øCu√°l de las siguientes funcionalidades de LangChain permite integrar m√∫ltiples fuentes de \u001b[0m\n",
              "\u001b[32mdatos para generar respuestas m√°s complejas?\"\u001b[0m,\n",
              "        \u001b[32m\"opcion1\"\u001b[0m: \u001b[32m\"Cadenas de texto simples\"\u001b[0m,\n",
              "        \u001b[32m\"opcion2\"\u001b[0m: \u001b[32m\"Cadenas de documentos\"\u001b[0m,\n",
              "        \u001b[32m\"opcion3\"\u001b[0m: \u001b[32m\"Integraci√≥n de m√∫ltiples m√≥dulos\"\u001b[0m,\n",
              "        \u001b[32m\"respuesta_correcta\"\u001b[0m: \u001b[32m\"3\"\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n",
              "```\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```json\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"pregunta\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"¬øCu√°l de las siguientes funcionalidades de LangChain permite integrar m√∫ltiples fuentes de </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">datos para generar respuestas m√°s complejas?\"</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"opcion1\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Cadenas de texto simples\"</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"opcion2\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Cadenas de documentos\"</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"opcion3\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Integraci√≥n de m√∫ltiples m√≥dulos\"</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"respuesta_correcta\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"3\"</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "```\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'pregunta'\u001b[0m: \u001b[32m'¬øCu√°l de las siguientes funcionalidades de LangChain permite integrar m√∫ltiples fuentes de datos \u001b[0m\n",
              "\u001b[32mpara generar respuestas m√°s complejas?'\u001b[0m,\n",
              "    \u001b[32m'opcion1'\u001b[0m: \u001b[32m'Cadenas de texto simples'\u001b[0m,\n",
              "    \u001b[32m'opcion2'\u001b[0m: \u001b[32m'Cadenas de documentos'\u001b[0m,\n",
              "    \u001b[32m'opcion3'\u001b[0m: \u001b[32m'Integraci√≥n de m√∫ltiples m√≥dulos'\u001b[0m,\n",
              "    \u001b[32m'respuesta_correcta'\u001b[0m: \u001b[32m'3'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'pregunta'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'¬øCu√°l de las siguientes funcionalidades de LangChain permite integrar m√∫ltiples fuentes de datos </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">para generar respuestas m√°s complejas?'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'opcion1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Cadenas de texto simples'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'opcion2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Cadenas de documentos'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'opcion3'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Integraci√≥n de m√∫ltiples m√≥dulos'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'respuesta_correcta'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'dict'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Pregunta: ¬øCu√°l de las siguientes funcionalidades de LangChain permite integrar m√∫ltiples fuentes de datos para \n",
              "generar respuestas m√°s complejas?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pregunta: ¬øCu√°l de las siguientes funcionalidades de LangChain permite integrar m√∫ltiples fuentes de datos para \n",
              "generar respuestas m√°s complejas?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Opci√≥n \u001b[1;36m1\u001b[0m: Cadenas de texto simples\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Opci√≥n <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Cadenas de texto simples\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Opci√≥n \u001b[1;36m2\u001b[0m: Cadenas de documentos\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Opci√≥n <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Cadenas de documentos\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Opci√≥n \u001b[1;36m3\u001b[0m: Integraci√≥n de m√∫ltiples m√≥dulos\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Opci√≥n <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: Integraci√≥n de m√∫ltiples m√≥dulos\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta correcta: Opci√≥n \u001b[1;36m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta correcta: Opci√≥n <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß© Ejemplo: Creacion de un test II**\n",
        "---\n",
        "Vamos a comprobar que el StructuredOutputParser puede gestionar listas sencillas con exito.\n",
        "üëÄ Observa como el diccionario devuelto por el parser, contiene una lista de strings con las preguntas\n"
      ],
      "metadata": {
        "id": "5pPpLoGbKOlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "# Creamos un esquema de respuesta (ResponseSchema) para cada campo que queremos extraer:\n",
        "# Pero ahora usamos una lista, al menos asi lo indicamos en el texto de la descripcion\n",
        "response_schemas = [\n",
        "            ResponseSchema(name=\"pregunta\", description=\"Texto de la pregunta generada.\"),\n",
        "            ResponseSchema(name=\"opciones\", description=\"LISTA de tres opciones de respuesta.\"),\n",
        "            ResponseSchema(name=\"respuesta_correcta\", description=\"√çndice de la opci√≥n correcta (1, 2 o 3).\")\n",
        "                    ]\n",
        "\n",
        "# Crear el StructuredOutputParser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "texto_prompt = \"\"\"\n",
        "        Genera una pregunta de test sobre el tema: {tema}.\n",
        "        El nivel de dificultad debe ser: {nivel}.\n",
        "        La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "        {format_instructions}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=texto_prompt,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = llm.invoke(prompt).content\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Pregunta: {respuesta_formateada['pregunta']}\")\n",
        "rprint(\"Opciones:\")\n",
        "for i, opcion in enumerate(respuesta_formateada['opciones'], start=1):\n",
        "    rprint(f\"{opcion}\")\n",
        "rprint(f\"Respuesta correcta: Opci√≥n {respuesta_formateada['respuesta_correcta']}\")"
      ],
      "metadata": {
        "id": "MNxAZq7RtiNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "outputId": "e9130d64-bea8-4be3-e274-623c75dcf540"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt generado:\n",
            " \n",
            "        Genera una pregunta de test sobre el tema: LangChain.\n",
            "        El nivel de dificultad debe ser: medio.\n",
            "        La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
            "\n",
            "        The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"pregunta\": string  // Texto de la pregunta generada.\n",
            "\t\"opciones\": string  // LISTA de tres opciones de respuesta.\n",
            "\t\"respuesta_correcta\": string  // √çndice de la opci√≥n correcta (1, 2 o 3).\n",
            "}\n",
            "```\n",
            "        \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "```json\n",
              "\u001b[1m{\u001b[0m\n",
              "        \u001b[32m\"pregunta\"\u001b[0m: \u001b[32m\"¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\"\u001b[0m,\n",
              "        \u001b[32m\"opciones\"\u001b[0m: \u001b[1m[\u001b[0m\n",
              "                \u001b[32m\"1. LangChain es exclusivamente una biblioteca para la gesti√≥n de datos en JavaScript.\"\u001b[0m,\n",
              "                \u001b[32m\"2. LangChain permite la creaci√≥n de aplicaciones que integran modelos de lenguaje con m√∫ltiples \u001b[0m\n",
              "\u001b[32mfuentes de datos y herramientas.\"\u001b[0m,\n",
              "                \u001b[32m\"3. LangChain no es capaz de interactuar con bases de datos externas.\"\u001b[0m\n",
              "        \u001b[1m]\u001b[0m,\n",
              "        \u001b[32m\"respuesta_correcta\"\u001b[0m: \u001b[32m\"2\"\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n",
              "```\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```json\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"pregunta\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\"</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"opciones\"</span>: <span style=\"font-weight: bold\">[</span>\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"1. LangChain es exclusivamente una biblioteca para la gesti√≥n de datos en JavaScript.\"</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"2. LangChain permite la creaci√≥n de aplicaciones que integran modelos de lenguaje con m√∫ltiples </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">fuentes de datos y herramientas.\"</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">\"3. LangChain no es capaz de interactuar con bases de datos externas.\"</span>\n",
              "        <span style=\"font-weight: bold\">]</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">\"respuesta_correcta\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"2\"</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "```\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'pregunta'\u001b[0m: \u001b[32m'¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?'\u001b[0m,\n",
              "    \u001b[32m'opciones'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[32m'1. LangChain es exclusivamente una biblioteca para la gesti√≥n de datos en JavaScript.'\u001b[0m,\n",
              "        \u001b[32m'2. LangChain permite la creaci√≥n de aplicaciones que integran modelos de lenguaje con m√∫ltiples fuentes de\u001b[0m\n",
              "\u001b[32mdatos y herramientas.'\u001b[0m,\n",
              "        \u001b[32m'3. LangChain no es capaz de interactuar con bases de datos externas.'\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'respuesta_correcta'\u001b[0m: \u001b[32m'2'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'pregunta'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'opciones'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'1. LangChain es exclusivamente una biblioteca para la gesti√≥n de datos en JavaScript.'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'2. LangChain permite la creaci√≥n de aplicaciones que integran modelos de lenguaje con m√∫ltiples fuentes de</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">datos y herramientas.'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'3. LangChain no es capaz de interactuar con bases de datos externas.'</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'respuesta_correcta'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'dict'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Pregunta: ¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pregunta: ¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Opciones:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Opciones:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m1\u001b[0m. LangChain es exclusivamente una biblioteca para la gesti√≥n de datos en JavaScript.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. LangChain es exclusivamente una biblioteca para la gesti√≥n de datos en JavaScript.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2\u001b[0m. LangChain permite la creaci√≥n de aplicaciones que integran modelos de lenguaje con m√∫ltiples fuentes de datos y \n",
              "herramientas.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. LangChain permite la creaci√≥n de aplicaciones que integran modelos de lenguaje con m√∫ltiples fuentes de datos y \n",
              "herramientas.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m3\u001b[0m. LangChain no es capaz de interactuar con bases de datos externas.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. LangChain no es capaz de interactuar con bases de datos externas.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta correcta: Opci√≥n \u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta correcta: Opci√≥n <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß© Ejemplo: Extracci√≥n de datos en anuncios inmobiliarios**\n",
        "\n",
        "Disponemos del texto de un anuncio inmobiliario y desamos extraer la informacion de forma estructurada."
      ],
      "metadata": {
        "id": "mB7k_8tJk6rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "\n",
        "# Definir los esquemas para la extracci√≥n de datos inmobiliarios\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"tipo_propiedad\", description=\"Tipo de propiedad (apartamento, casa, chalet, etc.)\"),\n",
        "    ResponseSchema(name=\"precio\", description=\"Precio de la propiedad en formato num√©rico sin s√≠mbolos\"),\n",
        "    ResponseSchema(name=\"moneda\", description=\"Moneda del precio (EUR, USD, etc.)\"),\n",
        "    ResponseSchema(name=\"superficie\", description=\"Superficie en metros cuadrados, solo n√∫mero\"),\n",
        "    ResponseSchema(name=\"habitaciones\", description=\"N√∫mero de habitaciones, solo n√∫mero\"),\n",
        "    ResponseSchema(name=\"ba√±os\", description=\"N√∫mero de ba√±os, solo n√∫mero\"),\n",
        "    ResponseSchema(name=\"ubicacion\", description=\"Ubicaci√≥n de la propiedad (barrio, ciudad)\"),\n",
        "    ResponseSchema(name=\"caracteristicas\", description=\"Lista de caracter√≠sticas destacadas de la propiedad\"),\n",
        "    ResponseSchema(name=\"estado\", description=\"Estado de la propiedad (nuevo, reformado, a reformar, etc.)\"),\n",
        "]\n",
        "\n",
        "# Crear el parser\n",
        "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Obtener instrucciones de formato\n",
        "format_instructions = parser.get_format_instructions()\n",
        "\n",
        "# Inicializar el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.0)\n",
        "\n",
        "# Texto del anuncio inmobiliario\n",
        "descripcion_inmueble = \"\"\"\n",
        "Magn√≠fico piso de 95 m¬≤ en el coraz√≥n de Salamanca. Consta de 3 dormitorios, 2 ba√±os completos,\n",
        "cocina equipada y amplio sal√≥n con balc√≥n. La propiedad ha sido recientemente reformada con\n",
        "materiales de alta calidad. Dispone de calefacci√≥n central, aire acondicionado, suelos de parquet,\n",
        "armarios empotrados y plaza de garaje incluida. Edificio con ascensor y servicio de porter√≠a.\n",
        "Excelente ubicaci√≥n cerca de todos los servicios, comercios y transporte p√∫blico.\n",
        "Precio: 450.000‚Ç¨. Gastos de comunidad: 150‚Ç¨/mes.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = f\"\"\"\n",
        "Extrae la informaci√≥n clave del siguiente anuncio inmobiliario:\n",
        "\n",
        "{descripcion_inmueble}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "# Obtener la respuesta\n",
        "respuesta = modelo.invoke(prompt)\n",
        "\n",
        "# Parsear la respuesta\n",
        "propiedades = parser.parse(respuesta.content)\n",
        "\n",
        "rprint(propiedades)\n",
        "rprint(type(propiedades))\n",
        "\n",
        "# Imprimir los resultados de forma estructurada\n",
        "rprint(f\"[bold spring_green3]Tipo: {propiedades['tipo_propiedad']}\")\n",
        "rprint(f\"[bold spring_green3]Precio: {propiedades['precio']} {propiedades['moneda']}\")\n",
        "rprint(f\"[bold spring_green3]Superficie: {propiedades['superficie']} m¬≤\")\n",
        "rprint(f\"[bold spring_green3]Habitaciones: {propiedades['habitaciones']}\")\n",
        "rprint(f\"[bold spring_green3]Ba√±os: {propiedades['ba√±os']}\")\n",
        "rprint(f\"[bold spring_green3]Ubicaci√≥n: {propiedades['ubicacion']}\")\n",
        "rprint(f\"[bold spring_green3]Estado: {propiedades['estado']}\")\n",
        "rprint(f\"[bold spring_green3]Caracteristicas: {propiedades['caracteristicas']}\")\n"
      ],
      "metadata": {
        "id": "I9bShLJahail",
        "outputId": "849c4e4d-ed63-43a4-ecc4-1ce868185b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'tipo_propiedad'\u001b[0m: \u001b[32m'piso'\u001b[0m,\n",
              "    \u001b[32m'precio'\u001b[0m: \u001b[32m'450000'\u001b[0m,\n",
              "    \u001b[32m'moneda'\u001b[0m: \u001b[32m'EUR'\u001b[0m,\n",
              "    \u001b[32m'superficie'\u001b[0m: \u001b[32m'95'\u001b[0m,\n",
              "    \u001b[32m'habitaciones'\u001b[0m: \u001b[32m'3'\u001b[0m,\n",
              "    \u001b[32m'ba√±os'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
              "    \u001b[32m'ubicacion'\u001b[0m: \u001b[32m'Salamanca'\u001b[0m,\n",
              "    \u001b[32m'caracteristicas'\u001b[0m: \u001b[32m'cocina equipada, amplio sal√≥n con balc√≥n, calefacci√≥n central, aire acondicionado, suelos \u001b[0m\n",
              "\u001b[32mde parquet, armarios empotrados, plaza de garaje, edificio con ascensor, servicio de porter√≠a'\u001b[0m,\n",
              "    \u001b[32m'estado'\u001b[0m: \u001b[32m'reformado'\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'tipo_propiedad'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'piso'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'precio'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'450000'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'moneda'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'EUR'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'superficie'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'95'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'habitaciones'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'ba√±os'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'ubicacion'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Salamanca'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'caracteristicas'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cocina equipada, amplio sal√≥n con balc√≥n, calefacci√≥n central, aire acondicionado, suelos </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">de parquet, armarios empotrados, plaza de garaje, edificio con ascensor, servicio de porter√≠a'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'estado'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'reformado'</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'dict'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'dict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mTipo: piso\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Tipo: piso</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mPrecio: \u001b[0m\u001b[1;38;5;41m450000\u001b[0m\u001b[1;38;5;41m EUR\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Precio: </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">450000</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> EUR</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mSuperficie: \u001b[0m\u001b[1;38;5;41m95\u001b[0m\u001b[1;38;5;41m m¬≤\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Superficie: </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">95</span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\"> m¬≤</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mHabitaciones: \u001b[0m\u001b[1;38;5;41m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Habitaciones: </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mBa√±os: \u001b[0m\u001b[1;38;5;41m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Ba√±os: </span><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mUbicaci√≥n: Salamanca\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Ubicaci√≥n: Salamanca</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mEstado: reformado\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Estado: reformado</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;5;41mCaracteristicas: cocina equipada, amplio sal√≥n con balc√≥n, calefacci√≥n central, aire acondicionado, suelos de \u001b[0m\n",
              "\u001b[1;38;5;41mparquet, armarios empotrados, plaza de garaje, edificio con ascensor, servicio de porter√≠a\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">Caracteristicas: cocina equipada, amplio sal√≥n con balc√≥n, calefacci√≥n central, aire acondicionado, suelos de </span>\n",
              "<span style=\"color: #00d75f; text-decoration-color: #00d75f; font-weight: bold\">parquet, armarios empotrados, plaza de garaje, edificio con ascensor, servicio de porter√≠a</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. PydanticOutputParser**\n",
        "---\n",
        "Convierte la salida a un modelo de datos Pydantic.\n",
        "\n",
        "# PydanticOutputParser en LangChain\n",
        "\n",
        "El `PydanticOutputParser` es una herramienta avanzada de LangChain que combina el poder de la validaci√≥n de Pydantic con la capacidad de estructurar las salidas de los modelos de lenguaje. A diferencia del `SimpleJsonOutputParser` o el `StructuredOutputParser`, este parser utiliza modelos Pydantic completos para definir esquemas de datos rigurosos.\n",
        "\n",
        "## Caracter√≠sticas principales\n",
        "\n",
        "- Utiliza modelos Pydantic para definir la estructura de datos esperada\n",
        "- Proporciona validaci√≥n de tipos robusta\n",
        "- Soporta esquemas de datos complejos y anidados\n",
        "- Genera instrucciones detalladas para guiar al modelo de lenguaje\n",
        "- Convierte autom√°ticamente la respuesta en una instancia del modelo Pydantic\n",
        "\n",
        "## Funcionamiento\n",
        "\n",
        "1. Se define un modelo Pydantic que represente la estructura de datos deseada\n",
        "2. Se crea un `PydanticOutputParser` basado en ese modelo\n",
        "3. Se generan instrucciones para el modelo de lenguaje\n",
        "4. Se parsea la respuesta para obtener una instancia del modelo Pydantic\n",
        "\n",
        "Si buscas simplicidad y rapidez, StructuredOutputParser es una buena opci√≥n pero si necesitas validaci√≥n de datos robusta y est√°s utilizando Pydantic en tu proyecto, PydanticOutputParser es la mejor alternativa.\n",
        "\n"
      ],
      "metadata": {
        "id": "4NbToCP9tt-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üß© Ejemplo: Creacion de un test III**\n",
        "---\n",
        "Vamos a crear de nuevo el ejemplo que genera una pregunta de test, pero esta vez con el PydanticOutputParser"
      ],
      "metadata": {
        "id": "qmyX9q0lNZwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Definimos el modelo Pydantic para la respuesta\n",
        "class PreguntaTest(BaseModel):\n",
        "    pregunta: str = Field(description=\"Texto de la pregunta generada.\")\n",
        "    opciones: List[str] = Field(description=\"Lista de tres opciones de respuesta.\")\n",
        "    respuesta_correcta: int = Field(description=\"√çndice de la opci√≥n correcta (0, 1 o 2).\")\n",
        "\n",
        "# Crear el PydanticOutputParser\n",
        "output_parser = PydanticOutputParser(pydantic_object=PreguntaTest)\n",
        "\n",
        "# Obtener el formato de instrucciones del parser\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions\n",
        "\n",
        "# Crear el prompt\n",
        "plantilla = \"\"\"\n",
        "Genera una pregunta de test sobre el tema: {tema}.\n",
        "El nivel de dificultad debe ser: {nivel}.\n",
        "La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=plantilla,\n",
        "    input_variables=[\"tema\", \"nivel\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Generar el prompt para el LLM\n",
        "prompt = prompt_template.format(tema=\"LangChain\", nivel=\"medio\")\n",
        "print(\"Prompt generado:\\n\", prompt)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Hacemos la llamada\n",
        "respuesta = modelo.invoke(prompt).content\n",
        "\n",
        "# Parseamos la respuesta fon el parser de salida\n",
        "respuesta_formateada = output_parser.parse(respuesta)\n",
        "\n",
        "rprint(respuesta)\n",
        "rprint(type(respuesta))\n",
        "\n",
        "rprint(respuesta_formateada)\n",
        "rprint(type(respuesta_formateada))\n",
        "\n",
        "# Usamos los datos estructurados\n",
        "rprint(f\"Pregunta: {respuesta_formateada.pregunta}\")\n",
        "rprint(\"Opciones:\")\n",
        "# La funci√≥n enumerate toma un iterable (como una lista) y devuelve un objeto que genera pares de valores\n",
        "# start=1 , para que el 0 se considere 1\n",
        "for i, opcion in enumerate(respuesta_formateada.opciones):\n",
        "    rprint(f\"{i+1}. {opcion}\")\n",
        "\n",
        "rprint(f\"Respuesta correcta: Opci√≥n {respuesta_formateada.respuesta_correcta + 1}\")\n"
      ],
      "metadata": {
        "id": "dEJ_Wd1Lt0Ee",
        "outputId": "a392b385-7f0b-47a2-f6c1-887bd97605a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt generado:\n",
            " \n",
            "Genera una pregunta de test sobre el tema: LangChain.\n",
            "El nivel de dificultad debe ser: medio.\n",
            "La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"pregunta\": {\"description\": \"Texto de la pregunta generada.\", \"title\": \"Pregunta\", \"type\": \"string\"}, \"opciones\": {\"description\": \"Lista de tres opciones de respuesta.\", \"items\": {\"type\": \"string\"}, \"title\": \"Opciones\", \"type\": \"array\"}, \"respuesta_correcta\": {\"description\": \"√çndice de la opci√≥n correcta (0, 1 o 2).\", \"title\": \"Respuesta Correcta\", \"type\": \"integer\"}}, \"required\": [\"pregunta\", \"opciones\", \"respuesta_correcta\"]}\n",
            "```\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "```json\n",
              "\u001b[1m{\u001b[0m\n",
              "  \u001b[32m\"pregunta\"\u001b[0m: \u001b[32m\"¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\"\u001b[0m,\n",
              "  \u001b[32m\"opciones\"\u001b[0m: \u001b[1m[\u001b[0m\n",
              "    \u001b[32m\"LangChain es una plataforma exclusiva para modelos de lenguaje tipo GPT.\"\u001b[0m,\n",
              "    \u001b[32m\"LangChain permite conectar modelos de lenguaje con fuentes externas de datos y APIs.\"\u001b[0m,\n",
              "    \u001b[32m\"LangChain no admite el uso de memoria durante las interacciones.\"\u001b[0m\n",
              "  \u001b[1m]\u001b[0m,\n",
              "  \u001b[32m\"respuesta_correcta\"\u001b[0m: \u001b[1;36m1\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n",
              "```\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```json\n",
              "<span style=\"font-weight: bold\">{</span>\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"pregunta\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\"</span>,\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"opciones\"</span>: <span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"LangChain es una plataforma exclusiva para modelos de lenguaje tipo GPT.\"</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"LangChain permite conectar modelos de lenguaje con fuentes externas de datos y APIs.\"</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">\"LangChain no admite el uso de memoria durante las interacciones.\"</span>\n",
              "  <span style=\"font-weight: bold\">]</span>,\n",
              "  <span style=\"color: #008000; text-decoration-color: #008000\">\"respuesta_correcta\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "```\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mPreguntaTest\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mpregunta\u001b[0m=\u001b[32m'¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?'\u001b[0m,\n",
              "    \u001b[33mopciones\u001b[0m=\u001b[1m[\u001b[0m\n",
              "        \u001b[32m'LangChain es una plataforma exclusiva para modelos de lenguaje tipo GPT.'\u001b[0m,\n",
              "        \u001b[32m'LangChain permite conectar modelos de lenguaje con fuentes externas de datos y APIs.'\u001b[0m,\n",
              "        \u001b[32m'LangChain no admite el uso de memoria durante las interacciones.'\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[33mrespuesta_correcta\u001b[0m=\u001b[1;36m1\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PreguntaTest</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">pregunta</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">opciones</span>=<span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'LangChain es una plataforma exclusiva para modelos de lenguaje tipo GPT.'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'LangChain permite conectar modelos de lenguaje con fuentes externas de datos y APIs.'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'LangChain no admite el uso de memoria durante las interacciones.'</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">respuesta_correcta</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'__main__.PreguntaTest'\u001b[0m\u001b[1m>\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.PreguntaTest'</span><span style=\"font-weight: bold\">&gt;</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Pregunta: ¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pregunta: ¬øCu√°l de las siguientes afirmaciones sobre LangChain es correcta?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Opciones:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Opciones:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m1\u001b[0m. LangChain es una plataforma exclusiva para modelos de lenguaje tipo GPT.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. LangChain es una plataforma exclusiva para modelos de lenguaje tipo GPT.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m2\u001b[0m. LangChain permite conectar modelos de lenguaje con fuentes externas de datos y APIs.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. LangChain permite conectar modelos de lenguaje con fuentes externas de datos y APIs.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m3\u001b[0m. LangChain no admite el uso de memoria durante las interacciones.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. LangChain no admite el uso de memoria durante las interacciones.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Respuesta correcta: Opci√≥n \u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Respuesta correcta: Opci√≥n <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otro ejemplo con Pydantic. Deseamos obtener la bibliografia ordenada del autor indicado. deseamos una lista ordenada con el a√±o y el titulo de cada libro.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BnYJ6p9zt4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# Modelo Pydantic para representar un informe m√©dico estructurado\n",
        "class MedicalReport(BaseModel):\n",
        "    patient_id: str = Field(description=\"Identificador √∫nico del paciente\")\n",
        "    diagnosis: List[str] = Field(description=\"Lista de diagn√≥sticos principales\")\n",
        "    vital_signs: dict = Field(description=\"Signos vitales como temperatura, presi√≥n arterial, etc.\")\n",
        "    medications: List[dict] = Field(description=\"Lista de medicamentos con nombre, dosis y frecuencia\")\n",
        "    recommendations: List[str] = Field(description=\"Recomendaciones m√©dicas para el paciente\")\n",
        "    follow_up_date: Optional[str] = Field(description=\"Fecha recomendada para seguimiento (formato YYYY-MM-DD)\")\n",
        "\n",
        "    # Validador para asegurar que el ID del paciente siga un formato espec√≠fico\n",
        "    @validator(\"patient_id\")\n",
        "    def validate_patient_id(cls, v):\n",
        "        if not v.startswith(\"P\") or not v[1:].isdigit():\n",
        "            raise ValueError(\"El ID del paciente debe comenzar con 'P' seguido de n√∫meros\")\n",
        "        return v\n",
        "\n",
        "    # Validador para la fecha de seguimiento\n",
        "    @validator(\"follow_up_date\", pre=True)\n",
        "    def validate_date(cls, v):\n",
        "        if v:\n",
        "            try:\n",
        "                datetime.strptime(v, \"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "                raise ValueError(\"La fecha debe estar en formato YYYY-MM-DD\")\n",
        "        return v\n",
        "\n",
        "# Crear el parser\n",
        "parser = PydanticOutputParser(pydantic_object=MedicalReport)\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Texto del informe m√©dico no estructurado\n",
        "informe_medico = \"\"\"\n",
        "Paciente: P12345\n",
        "Fecha de consulta: 15/03/2025\n",
        "Motivo de consulta: Dolor abdominal y fiebre de 3 d√≠as de evoluci√≥n.\n",
        "Examen f√≠sico: Temperatura 38.2¬∞C, Presi√≥n arterial 130/85, Frecuencia card√≠aca 95 lpm, Saturaci√≥n O2 98%.\n",
        "Abdomen doloroso a la palpaci√≥n en cuadrante inferior derecho.\n",
        "Diagn√≥stico: Apendicitis aguda. Deshidrataci√≥n leve.\n",
        "Tratamiento: Ceftriaxona 1g IV cada 12 horas, Metronidazol 500mg IV cada 8 horas,\n",
        "Paracetamol 1g VO cada 8 horas si fiebre o dolor.\n",
        "Plan: Programar cirug√≠a. Hidrataci√≥n intravenosa. Control de signos vitales.\n",
        "Recomendaciones: Dieta l√≠quida, reposo absoluto, vigilar cambios en el dolor o aparici√≥n de fiebre.\n",
        "Pr√≥xima cita: 10 de abril de 2025.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el prompt\n",
        "prompt = f\"\"\"\n",
        "Extrae y estructura la informaci√≥n del siguiente informe m√©dico:\n",
        "\n",
        "{informe_medico}\n",
        "\n",
        "{parser.get_format_instructions()}\n",
        "\"\"\"\n",
        "\n",
        "# Obtener la respuesta\n",
        "respuesta = modelo.invoke(prompt)\n",
        "\n",
        "# Parsear la respuesta (esto devuelve una instancia de MedicalReport)\n",
        "informe_estructurado = parser.parse(respuesta.content)\n",
        "\n",
        "# Usar la instancia\n",
        "print(f\"ID del paciente: {informe_estructurado.patient_id}\")\n",
        "print(f\"Diagn√≥sticos: {', '.join(informe_estructurado.diagnosis)}\")\n",
        "print(\"Signos vitales:\")\n",
        "for signo, valor in informe_estructurado.vital_signs.items():\n",
        "    print(f\"  - {signo}: {valor}\")\n",
        "print(\"Medicaciones:\")\n",
        "for med in informe_estructurado.medications:\n",
        "    # Access keys using the names provided by the LLM: 'name', 'dose', 'frequency'\n",
        "    print(f\"  - {med['name']} {med['dose']} {med['frequency']}\")\n",
        "\n",
        "print(f\"Fecha de seguimiento: {informe_estructurado.follow_up_date}\")"
      ],
      "metadata": {
        "id": "cM25ELTDO9tn",
        "outputId": "2f4eef65-575b-4790-e0bf-d1d9f4dc530c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-9adbd96202e4>:17: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"patient_id\")\n",
            "<ipython-input-41-9adbd96202e4>:24: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"follow_up_date\", pre=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID del paciente: P12345\n",
            "Diagn√≥sticos: Apendicitis aguda, Deshidrataci√≥n leve\n",
            "Signos vitales:\n",
            "  - temperature: 38.2¬∞C\n",
            "  - blood_pressure: 130/85\n",
            "  - heart_rate: 95 lpm\n",
            "  - oxygen_saturation: 98%\n",
            "Medicaciones:\n",
            "  - Ceftriaxona 1g IV cada 12 horas\n",
            "  - Metronidazol 500mg IV cada 8 horas\n",
            "  - Paracetamol 1g VO cada 8 horas si fiebre o dolor\n",
            "Fecha de seguimiento: 2025-04-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que la clase del objeto devuelto NO es un diccionario, ni un JSON sino un objeto heredado de la clase que hemos definido con Pydantic !!\n"
      ],
      "metadata": {
        "id": "dnHhQCe_Yb1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. DateTimeOutputParser**\n",
        "---\n",
        "# DateTimeOutputParser en LangChain\n",
        "\n",
        "El `DateTimeOutputParser` en LangChain es una herramienta para transformar texto en formato de fecha y hora en objetos estructurados de Python. Este parser es √∫til cuando necesitas extraer informaci√≥n temporal de textos generados por modelos de lenguaje y convertirlos en formatos estandarizados que puedan ser utilizados en aplicaciones o sistemas.\n",
        "\n",
        "## Caracter√≠sticas principales\n",
        "\n",
        "- Convierte texto que describe fechas y horas en objetos `datetime` de Python\n",
        "- Permite manejar diferentes formatos de entrada y estandarizarlos\n",
        "- Es √∫til para aplicaciones que requieren procesamiento de informaci√≥n temporal\n",
        "\n",
        "## C√≥mo funciona\n",
        "\n",
        "El parser toma el texto generado por un LLM que contiene referencias a fechas/horas y lo convierte en un formato estructurado seg√∫n las especificaciones proporcionadas. Esto facilita la integraci√≥n con bases de datos, planificadores, y otros sistemas que necesitan datos temporales bien formateados."
      ],
      "metadata": {
        "id": "E_NrlM6q8AB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo: Citas m√©dicas.\n",
        "En este ejemplo, crearemos un sistema para gestionar citas m√©dicas que:\n",
        "\n",
        "1. Recibe solicitudes en lenguaje natural del paciente\n",
        "2. Utiliza un LLM para extraer la informaci√≥n temporal relevante\n",
        "3. Emplea el `DateTimeOutputParser` para convertir el texto generado en un objeto `datetime` de Python\n",
        "4. Procesa esta informaci√≥n para proporcionar detalles adicionales √∫tiles:\n",
        "    - Formato de fecha legible para humanos\n",
        "    - Verificaci√≥n si la cita cae en fin de semana\n",
        "    - C√°lculo de d√≠as restantes hasta la cita"
      ],
      "metadata": {
        "id": "WUASQRwlSb21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "\n",
        "# Crear el parser de fecha/hora\n",
        "datetime_parser = DatetimeOutputParser(format=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "\n",
        "# Definir el prompt\n",
        "prompt_template = \"\"\"\n",
        "Act√∫as como un asistente de programaci√≥n de citas m√©dicas.\n",
        "El paciente ha solicitado una cita con la siguiente informaci√≥n:\n",
        "\n",
        "\"{user_input}\"\n",
        "\n",
        "Basado en esta solicitud, identifica la fecha y hora exacta para la cita.\n",
        "Responde SOLO con la fecha y hora en formato ISO (YYYY-MM-DD HH:MM:SS).\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"user_input\"]\n",
        ")\n",
        "\n",
        "# Crear la secuencia de procesamiento (sin usar chains)\n",
        "def process_appointment_request(user_input):\n",
        "    formatted_prompt = prompt.format(user_input=user_input)\n",
        "    llm_response = modelo.invoke(formatted_prompt)\n",
        "\n",
        "    # Parsear la respuesta para obtener un objeto datetime\n",
        "    datetime_obj = datetime_parser.parse(llm_response.content)\n",
        "\n",
        "    return {\n",
        "        \"appointment_datetime\": datetime_obj,\n",
        "        \"original_request\": user_input,\n",
        "        \"formatted_date\": datetime_obj.strftime(\"%d de %B de %Y\"),\n",
        "        \"formatted_time\": datetime_obj.strftime(\"%H:%M\"),\n",
        "        \"is_weekend\": datetime_obj.weekday() >= 5,\n",
        "        \"days_from_now\": (datetime_obj.date() - datetime.now().date()).days\n",
        "    }\n",
        "\n",
        "# Ejemplo de uso\n",
        "user_request = \"Necesito una cita con el Dr. Garc√≠a para el pr√≥ximo martes a las 3 de la tarde\"\n",
        "appointment_details = process_appointment_request(user_request)\n",
        "\n",
        "print(f\"Cita programada para: {appointment_details['formatted_date']} a las {appointment_details['formatted_time']}\")\n",
        "print(f\"D√≠as restantes: {appointment_details['days_from_now']}\")\n",
        "if appointment_details['is_weekend']:\n",
        "    print(\"Advertencia: La cita est√° programada para un fin de semana\")"
      ],
      "metadata": {
        "id": "FJseHcmY8HOM",
        "outputId": "8f65a49a-72b3-4140-f0cf-ffba3d714dd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cita programada para: 31 de October de 2023 a las 15:00\n",
            "D√≠as restantes: -503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. OutputFixingParser**\n",
        "---"
      ],
      "metadata": {
        "id": "3P9Og_Wl8IxD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pYK_eYF8JYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. with_structured_output()**\n",
        "\n",
        "solo para llm que lo soportan (openai ...)"
      ],
      "metadata": {
        "id": "w_bCk2tw8NZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß©ejemplo"
      ],
      "metadata": {
        "id": "abxLCiidXGSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Definir la estructura para un libro\n",
        "class Libro(BaseModel):\n",
        "    titulo: str = Field(description=\"T√≠tulo del libro\")\n",
        "    a√±o: int = Field(description=\"A√±o de publicaci√≥n\")\n",
        "\n",
        "# Definir la estructura para la bibliograf√≠a completa\n",
        "class Bibliografia(BaseModel):\n",
        "    bibliografia: List[Libro] = Field(description=\"Lista de libros publicados por el autor\")\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0)\n",
        "\n",
        "# Configuramos el modelo para que devuelva directamente una instancia de Bibliografia\n",
        "structured_llm = modelo.with_structured_output(Bibliografia)\n",
        "\n",
        "# Funci√≥n para obtener la bibliograf√≠a de un autor\n",
        "def obtener_bibliografia(autor):\n",
        "    # Creamos el prompt directamente como una cadena de texto\n",
        "    prompt = f\"\"\"\n",
        "    Proporciona √öNICAMENTE la bibliograf√≠a del autor: {autor}.\n",
        "    INSTRUCCIONES ESTRICTAS:\n",
        "    1. Solo crea una lista de sus libros con t√≠tulo y a√±o.\n",
        "    2. La lista de libros debe usar la clave \"bibliografia\".\n",
        "    3. Cada libro debe tener SOLO los campos \"titulo\" y \"a√±o\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Hacemos la llamada directamente al modelo estructurado\n",
        "    resultado = structured_llm.invoke(prompt)\n",
        "\n",
        "    return resultado\n",
        "\n",
        "# Ejemplo de uso\n",
        "autor = \"Gabriel Garc√≠a M√°rquez\"\n",
        "bibliografia = obtener_bibliografia(autor)\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(f\"Autor: {autor}\")\n",
        "print(\"Bibliograf√≠a:\")\n",
        "for libro in bibliografia.bibliografia:\n",
        "    print(f\"A√±o: {libro.a√±o}, T√≠tulo: {libro.titulo}\")\n",
        "\n",
        "# Si necesitas acceder como diccionario (similar al ejemplo original)\n",
        "bibliografia_dict = bibliografia.model_dump()\n",
        "print(\"\\nAcceso como diccionario:\")\n",
        "print(f\"Autor: {autor}\")\n",
        "print(\"Bibliograf√≠a:\")\n",
        "for libro in bibliografia_dict[\"bibliografia\"]:\n",
        "    print(f\"A√±o: {libro['a√±o']}, T√≠tulo: {libro['titulo']}\")"
      ],
      "metadata": {
        "id": "52GRpA5uXEhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß©ejemplo"
      ],
      "metadata": {
        "id": "mXb5idU4XNqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Definimos un modelo Pydantic en lugar de ResponseSchema\n",
        "class QuizQuestion(BaseModel):\n",
        "    pregunta: str = Field(description=\"Texto de la pregunta generada.\")\n",
        "    opciones: List[str] = Field(description=\"Lista de tres opciones de respuesta.\")\n",
        "    respuesta_correcta: int = Field(description=\"√çndice de la opci√≥n correcta (1, 2 o 3).\")\n",
        "\n",
        "# Instanciamos el modelo\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Configuramos el modelo para que devuelva directamente una instancia de QuizQuestion\n",
        "structured_llm = llm.with_structured_output(QuizQuestion)\n",
        "\n",
        "# Creamos el prompt directamente como una cadena de texto\n",
        "def generar_pregunta(tema, nivel):\n",
        "    prompt = f\"\"\"\n",
        "    Genera una pregunta de test sobre el tema: {tema}.\n",
        "    El nivel de dificultad debe ser: {nivel}.\n",
        "    La pregunta debe tener tres opciones de respuesta y una respuesta correcta.\n",
        "    \"\"\"\n",
        "\n",
        "    # Hacemos la llamada directamente al modelo estructurado\n",
        "    pregunta_estructurada = structured_llm.invoke(prompt)\n",
        "\n",
        "    return pregunta_estructurada\n",
        "\n",
        "# Ejemplo de uso\n",
        "tema = \"LangChain\"\n",
        "nivel = \"medio\"\n",
        "resultado = generar_pregunta(tema, nivel)\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(f\"Pregunta: {resultado.pregunta}\")\n",
        "print(\"Opciones:\")\n",
        "for i, opcion in enumerate(resultado.opciones, start=1):\n",
        "    print(f\"{i}. {opcion}\")\n",
        "print(f\"Respuesta correcta: Opci√≥n {resultado.respuesta_correcta}\")\n",
        "\n",
        "# Si necesitas acceder como diccionario (similar al ejemplo original)\n",
        "resultado_dict = resultado.model_dump()\n",
        "print(\"\\nAcceso como diccionario:\")\n",
        "print(f\"Pregunta: {resultado_dict['pregunta']}\")\n",
        "print(\"Opciones:\")\n",
        "for opcion in resultado_dict['opciones']:\n",
        "    print(f\"- {opcion}\")\n",
        "print(f\"Respuesta correcta: Opci√≥n {resultado_dict['respuesta_correcta']}\")"
      ],
      "metadata": {
        "id": "1McIY_TpWj36",
        "outputId": "cf9fb124-3f47-46c0-f039-87ad1182c938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta: ¬øCu√°l es una de las principales caracter√≠sticas de LangChain que lo diferencia de otras bibliotecas de procesamiento de lenguaje natural?\n",
            "Opciones:\n",
            "1. Integraci√≥n nativa con modelos de lenguaje de m√∫ltiples proveedores\n",
            "2. Foco exclusivo en traducci√≥n de idiomas\n",
            "3. Entrenamiento de modelos personalizados desde cero\n",
            "Respuesta correcta: Opci√≥n 1\n",
            "\n",
            "Acceso como diccionario:\n",
            "Pregunta: ¬øCu√°l es una de las principales caracter√≠sticas de LangChain que lo diferencia de otras bibliotecas de procesamiento de lenguaje natural?\n",
            "Opciones:\n",
            "- Integraci√≥n nativa con modelos de lenguaje de m√∫ltiples proveedores\n",
            "- Foco exclusivo en traducci√≥n de idiomas\n",
            "- Entrenamiento de modelos personalizados desde cero\n",
            "Respuesta correcta: Opci√≥n 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß©ejemplo\n"
      ],
      "metadata": {
        "id": "UVNDcEZ1XPGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Modelo Pydantic para representar un informe m√©dico estructurado\n",
        "class MedicalReport(BaseModel):\n",
        "    patient_id: str = Field(description=\"Identificador √∫nico del paciente\")\n",
        "    diagnosis: List[str] = Field(description=\"Lista de diagn√≥sticos principales\")\n",
        "    vital_signs: dict = Field(description=\"Signos vitales como temperatura, presi√≥n arterial, etc.\")\n",
        "    medications: List[dict] = Field(description=\"Lista de medicamentos con nombre, dosis y frecuencia\")\n",
        "    recommendations: List[str] = Field(description=\"Recomendaciones m√©dicas para el paciente\")\n",
        "    follow_up_date: Optional[str] = Field(description=\"Fecha recomendada para seguimiento (formato YYYY-MM-DD)\")\n",
        "\n",
        "    # Validador para asegurar que el ID del paciente siga un formato espec√≠fico\n",
        "    @validator(\"patient_id\")\n",
        "    def validate_patient_id(cls, v):\n",
        "        if not v.startswith(\"P\") or not v[1:].isdigit():\n",
        "            raise ValueError(\"El ID del paciente debe comenzar con 'P' seguido de n√∫meros\")\n",
        "        return v\n",
        "\n",
        "    # Validador para la fecha de seguimiento\n",
        "    @validator(\"follow_up_date\", pre=True)\n",
        "    def validate_date(cls, v):\n",
        "        if v:\n",
        "            try:\n",
        "                datetime.strptime(v, \"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "                raise ValueError(\"La fecha debe estar en formato YYYY-MM-DD\")\n",
        "        return v\n",
        "\n",
        "# Instanciamos el modelo\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=1)\n",
        "\n",
        "# Configurar el modelo para que devuelva directamente una instancia de MedicalReport\n",
        "structured_llm = modelo.with_structured_output(MedicalReport)\n",
        "\n",
        "# Texto del informe m√©dico no estructurado\n",
        "informe_medico = \"\"\"\n",
        "Paciente: P12345\n",
        "Fecha de consulta: 15/03/2025\n",
        "Motivo de consulta: Dolor abdominal y fiebre de 3 d√≠as de evoluci√≥n.\n",
        "Examen f√≠sico: Temperatura 38.2¬∞C, Presi√≥n arterial 130/85, Frecuencia card√≠aca 95 lpm, Saturaci√≥n O2 98%.\n",
        "Abdomen doloroso a la palpaci√≥n en cuadrante inferior derecho.\n",
        "Diagn√≥stico: Apendicitis aguda. Deshidrataci√≥n leve.\n",
        "Tratamiento: Ceftriaxona 1g IV cada 12 horas, Metronidazol 500mg IV cada 8 horas,\n",
        "Paracetamol 1g VO cada 8 horas si fiebre o dolor.\n",
        "Plan: Programar cirug√≠a. Hidrataci√≥n intravenosa. Control de signos vitales.\n",
        "Recomendaciones: Dieta l√≠quida, reposo absoluto, vigilar cambios en el dolor o aparici√≥n de fiebre.\n",
        "Pr√≥xima cita: 10 de abril de 2025.\n",
        "\"\"\"\n",
        "\n",
        "# Crear el prompt directamente (sin usar PromptTemplate)\n",
        "prompt = f\"\"\"\n",
        "Extrae y estructura la informaci√≥n del siguiente informe m√©dico:\n",
        "{informe_medico}\n",
        "\"\"\"\n",
        "\n",
        "# Invocar el modelo directamente con el prompt\n",
        "informe_estructurado = structured_llm.invoke(prompt)\n",
        "\n",
        "# Usar la instancia\n",
        "print(f\"ID del paciente: {informe_estructurado.patient_id}\")\n",
        "print(f\"Diagn√≥sticos: {', '.join(informe_estructurado.diagnosis)}\")\n",
        "print(\"Signos vitales:\")\n",
        "for signo, valor in informe_estructurado.vital_signs.items():\n",
        "    print(f\"  - {signo}: {valor}\")\n",
        "print(\"Medicaciones:\")\n",
        "for med in informe_estructurado.medications:\n",
        "    print(f\"  - {med['name']} {med['dose']} {med['frequency']}\")\n",
        "print(f\"Fecha de seguimiento: {informe_estructurado.follow_up_date}\")"
      ],
      "metadata": {
        "id": "v3CnYaYZVZcd",
        "outputId": "ee57cd2d-66c4-4bb1-c4ac-c76a60292b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-36734976bd17>:16: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"patient_id\")\n",
            "<ipython-input-50-36734976bd17>:23: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"follow_up_date\", pre=True)\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:410: UserWarning: Invalid schema for OpenAI's structured output feature, which is the default method for `with_structured_output` as of langchain-openai==0.3. Specify `method=\"function_calling\"` instead or update your schema. See supported schemas: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'MedicalReport': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Extra required key 'vital_signs' supplied.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-36734976bd17>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Invocar el modelo directamente con el prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0minforme_estructurado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructured_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Usar la instancia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3025\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3027\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3028\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5363\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5364\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5365\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5366\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         return cast(\n\u001b[1;32m    306\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    842\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 results.append(\n\u001b[0;32m--> 683\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    684\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    909\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0m_handle_openai_bad_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_responses_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0moriginal_schema_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_handle_openai_bad_request\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    409\u001b[0m         )\n\u001b[1;32m    410\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0m_handle_openai_bad_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/beta/chat/completions.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    156\u001b[0m             )\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'MedicalReport': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Extra required key 'vital_signs' supplied.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retos\n",
        "\n",
        "Reto 1\n",
        "StructuredOutputParser (Nivel Medio)\n",
        "T√≠tulo: An√°lisis de rese√±as de pel√≠culas\n",
        "Descripci√≥n:\n",
        "Usa StructuredOutputParser para extraer datos clave de una cr√≠tica de cine generada por un modelo de lenguaje.\n",
        "\n",
        "Requisitos:\n",
        "\n",
        "Define un esquema con estos campos:\n",
        "\n",
        "titulo_pelicula (string).\n",
        "\n",
        "director (string).\n",
        "\n",
        "genero (string, ej: \"ciencia ficci√≥n\").\n",
        "\n",
        "puntaje (string, del 1 al 10).\n",
        "\n",
        "Genera una cr√≠tica ficticia de una pel√≠cula usando un prompt que incluya las instrucciones de formato.\n",
        "\n",
        "Parsea la respuesta y muestra el resultado como diccionario.\n",
        "\n",
        "Ejemplo de salida:\n",
        "\n",
        "python\n",
        "Copy\n",
        "{'titulo_pelicula': 'El √öltimo Eclipse', 'director': 'Carlos M√©ndez', 'genero': 'drama', 'puntaje': '8'}  \n",
        "\n",
        "\n",
        "## Reto 2: Generador de planes de viaje con with\\_structured\\_output()\n",
        "\n",
        "**Nivel: Medio**\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "Crear un generador de planes de viaje que proporcione itinerarios estructurados utilizando el m√©todo `with_structured_output()`.\n",
        "\n",
        "### Descripci√≥n\n",
        "\n",
        "Desarrolla una aplicaci√≥n que permita al usuario especificar un destino, duraci√≥n del viaje y preferencias, y genere un itinerario detallado y estructurado. El sistema debe proporcionar:\n",
        "\n",
        "- Resumen del viaje (p√°rrafo)\n",
        "- Itinerario diario (lista de d√≠as)\n",
        "- Recomendaciones de alojamiento (lista de opciones)\n",
        "- Presupuesto estimado (rangos por categor√≠a)\n",
        "- Consejos pr√°cticos (lista de recomendaciones)\n",
        "\n",
        "### Requisitos\n",
        "\n",
        "1. Define un modelo Pydantic para la estructura del plan de viaje\n",
        "2. Utiliza `with_structured_output()` para configurar el LLM\n",
        "3. Crea un prompt claro y conciso (sin necesidad de instrucciones de formato)\n",
        "4. Implementa una funci√≥n que genere planes para diferentes destinos\n",
        "5. Presenta el itinerario de manera visualmente atractiva\n",
        "\n",
        "### Ejemplo de entrada\n",
        "\n",
        "Copiar\n",
        "\n",
        "`Destino: Barcelona Duraci√≥n: 3 d√≠as Preferencias: Arte, gastronom√≠a, presupuesto medio`\n",
        "\n",
        "### Sugerencia de implementaci√≥n\n",
        "\n",
        "Comienza definiendo las clases Pydantic para actividades diarias y el plan completo. Luego configura el modelo con `with_structured_output()` y crea una funci√≥n principal que procese la solicitud del usuario.\n",
        "\n",
        "### Extensi√≥n opcional\n",
        "\n",
        "A√±ade una funci√≥n para exportar el itinerario a formato Markdown o HTML para compartirlo f√°cilmente.\n",
        "\n"
      ],
      "metadata": {
        "id": "xX-352vYX84K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referencias:\n",
        "\n",
        "1. https://freedium.cfd/https://python.plainenglish.io/langchain-in-chains-7-output-parsers-e1a2cdd40cd3\n",
        "\n",
        "2. https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834\n",
        "\n",
        "3. https://cobusgreyling.medium.com/langchain-structured-output-parser-using-openai-c3fe6927beb7\n",
        "\n",
        "4. https://python.langchain.com/docs/how_to/output_parser_structured/\n",
        "\n",
        "5. https://www.comet.com/site/blog/mastering-output-parsing-in-langchain/\n",
        "\n",
        "6. https://www.gettingstarted.ai/how-to-langchain-output-parsers-convert-text-to-objects/\n",
        "\n",
        "7. https://www.gettingstarted.ai/how-to-extract-metadata-from-pdf-convert-to-json-langchain/  Este es un buen reto\n",
        "\n",
        "8. https://www.analyticsvidhya.com/blog/2024/11/output-parsers/\n"
      ],
      "metadata": {
        "id": "lo85c5QHb66h"
      }
    }
  ]
}