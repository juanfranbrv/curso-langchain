{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbG6vpYyckteCyHObxPlxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadenas%20b%C3%A1sicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Cadenas en Langchain**\n",
        "\n",
        "Las cadenas en LangChain representan una secuencia de operaciones que procesan datos de entrada a través de uno o más modelos de lenguaje. Cada operación de una cadena puede considerarse como un paso de un flujo de trabajo, donde el resultado de un paso sirve como entrada para el siguiente. Estas cadenas pueden ser simples o complejas, según los requisitos del proyecto."
      ],
      "metadata": {
        "id": "0WdsL3-kpSC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nuevas cadenas con LangChain Expression Language (LCEL)**\n",
        "---\n",
        "\n",
        "En octubre de 2023, LangChain introdujo una nueva forma de trabajar con cadenas, más eficiente y simple, utilizando el operador | (pipe).  \n",
        "\n",
        "Esto permite encadenar elementos de manera intuitiva, con soporte para stream, batch y operaciones asíncronas.  \n",
        "\n",
        "Además, con LCEL (LangChain Expression Language), las cadenas se pueden definir de forma declarativa y funcional, eliminando la necesidad de clases complejas.   \n",
        "\n",
        "Aunque las cadenas \"antiguas\" siguen funcionando, es recomendable adoptar esta nueva sintaxis.\n",
        "\n",
        "### Ventajas del nuevo sistema:\n",
        "\n",
        "- **Alineación con el framework:** La nueva sintaxis es el estándar recomendado para futuros proyectos.\n",
        "- **Interface unificada:** Simplifica la integración de múltiples métodos.\n",
        "- **Facilidad de composición:** Permite estructurar cadenas de forma secuencial, paralela o con fallbacks.\n",
        "- **Código más limpio:** Menos líneas para lograr los mismos resultados.\n",
        "\n",
        "En LCEL **(LangChain Expression Language**), no existen como tal tipos de cadenas predefinidos como \"secuenciales\", \"condicionales\" o \"paralelas\", pero puedes implementar estos patrones de flujo de trabajo utilizando la sintaxis de LCEL y algunas funciones adicionales.\n",
        "\n",
        "En este cuaderno nos vamos a ocupar solo del patron secuencial. En proximos cuadernos veremos el resto de patrones de diseño."
      ],
      "metadata": {
        "id": "5RmTZhkweukJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuración del entorno del cuaderno**\n",
        "---\n",
        "\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).  \n",
        "1. Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "2. Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "3. Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "_jVrgFCnxbte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# Importamos las clases necesarias para trabajar con cadenas\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "0kpJPccSxabU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Prompt + LLM**\n",
        "\n",
        "---\n",
        "Posiblemente esta es la cadena mas simple que podamos crear.  \n",
        "En este caso la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo.\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Observa que esto es basicamente lo que hemos venido haciendo hasta ahora sin cadenas."
      ],
      "metadata": {
        "id": "lDXFodi33rZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos el prompt template de una de esta formas\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¿cómo estás?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Francés\", \"texto\":\"Hola, ¿cómo estás?\"})\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "m-UHdYGEWaO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que esto es lo que hemos hecho hasta ahora sin cadenas: Invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia y este enfoque puede ser más simple para tareas sencillas, carece de la flexibilidad y la extensibilidad que ofrecen las cadenas como se aprecia en cuanto la complejidad escala.\n",
        "\n",
        "Ademas LCEL y la nuevas cadenas proporcionan un nivel de abstracción mayor tanto del prompt como del proceso en general.\n"
      ],
      "metadata": {
        "id": "laEFQWsLRJbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Creamos el prompt y lo formateamos\n",
        "prompt_template = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "prompt_formateado = prompt_template.format(idioma=\"Francés\", texto=\"Hola, ¿cómo estás?\")\n",
        "\n",
        "# Invocamos el LLM directamente\n",
        "respuesta = llm.invoke(prompt_formateado)\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "3OddOArKRXD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Prompt + LLM + funcion personalizada de transformación de la salida**\n",
        "---\n",
        "\n",
        "En este caso la salida del LLM se procesa con una funcion sencilla para pasarla a mayusculas. (pero puede ser tan complicada como se necesite)\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un objeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar.\n",
        "\n",
        "Vamos a usar en adelante display de IPython (en lugar de print) para imprimir el markdown resultante y que se vea menjor en pantalla."
      ],
      "metadata": {
        "id": "GZyeh8OxXrDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos una función de transformación\n",
        "def transformar_texto(output):\n",
        "    return output.content.upper()\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "mJR5vHrfX31N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prompt + LLM + OutputParser + funcion personalizada de transformación de la salida**\n",
        "---\n",
        "\n",
        "Este ejemplo mejora la modularidad reutilizando la cadena en un bucle y usando un OutputParser.\n",
        "\n",
        "En LangChain, los **parsers de salida** estructuran la salida del LLM en formatos específicos (string, JSON, lista, etc.).\n",
        "\n",
        "**Tipos comunes:**\n",
        "\n",
        "- **StrOutputParser**: Convierte a string (usado aquí).\n",
        "    \n",
        "- **JsonOutputParser**: Convierte a JSON.\n",
        "    \n",
        "- **CommaSeparatedListOutputParser**: Crea listas separadas por comas.\n",
        "    \n",
        "- **StructuredOutputParser**: Define esquemas de salida personalizados."
      ],
      "metadata": {
        "id": "qxQbWY-kdB7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una función de transformación\n",
        "# Esta función toma la salida del OutPutParser (str) y la convierte a mayúsculas.\n",
        "def transformar_texto(output):\n",
        "    return output.upper()\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas en un bucle\n",
        "# Observa cómo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¿Quién fue Napoleón Bonaparte?\"},\n",
        "    {\"tema\": \"Programación\", \"pregunta\": \"¿Qué es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "IuwH1XTXdBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Prompt + LLM + OutputParser + Guardar en fichero**\n",
        "---\n",
        "Crea una cadena que genera una lista de elementos y la guarda en un archivo:\n",
        "\n",
        "- **Prompt:** Define un ChatPromptTemplate que instruye al sistema a crear listas sin numerar, en una columna y sin comentarios.\n",
        "    \n",
        "- **LLM:** Usa el modelo gpt-4o-mini de OpenAI para generar la respuesta.\n",
        "    \n",
        "- **OutputParser:** Formatea la respuesta como texto plano con StrOutputParser.\n",
        "    \n",
        "- **Función Personalizada:** La función guardar\\_en\\_archivo guarda la respuesta en \"respuesta.txt\" e imprime la respuesta en la consola.  \n",
        "\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "auY_uQJshX6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Definimos una función para guardar en archivo\n",
        "def guardar_en_archivo(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en -mente\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "sntxjr0xh4kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + JsonOutputParser**\n",
        "---\n",
        "\n",
        "Esta vez genera una respuesta JSON.\n",
        "\n",
        "SimpleJsonOutputParser transforma la salida del modelo en un objeto JSON válido.\n"
      ],
      "metadata": {
        "id": "MMLf1KxWrjcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto en {tema}. Devuelve el resultado en formato JSON con las claves 'respuesta' y 'explicacion'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"tema\": \"Python\", \"pregunta\": \"¿Qué es el Streamlit?\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "25m9G__Gr-Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**👉🏻¿ si el modelo ya da la respuesta en JSON (se le ha indicado en el prompt) porque se se utiliza json_parser = SimpleJsonOutputParser() para contruir la salida ?**\n",
        "\n",
        "\n",
        "La respuesta tiene que ver con cómo funcionan los modelos de lenguaje y cómo se manejan sus salidas en LangChain. Vamos a desglosarlo:\n",
        "\n",
        "#### A\\. **El modelo devuelve una cadena de texto**\n",
        "\n",
        "Aunque le indiques al modelo que genere una respuesta en formato JSON, la salida del modelo sigue siendo una **cadena de texto** (un `string`). Por ejemplo, el modelo podría devolver algo como esto:\n",
        "\n",
        "```\n",
        "'{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web interactivas.\",\n",
        " \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente utilizando\n",
        " scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Esta cadena de texto **no es un objeto JSON** en sí misma, sino una representación textual de un JSON. Para poder trabajar con ella en Python (por ejemplo, acceder a las claves `respuesta` y `explicacion`), necesitas convertirla en un diccionario de Python.\n",
        "\n",
        "#### B\\. **¿Qué hace `SimpleJsonOutputParser`?**\n",
        "\n",
        "`SimpleJsonOutputParser` se encarga de:\n",
        "\n",
        "1. **Tomar la cadena de texto** que el modelo ha generado.\n",
        "    \n",
        "2. **Convertirla en un objeto JSON** (por ejemplo, un diccionario o una lista) utilizando `json.loads()` internamente.\n",
        "    \n",
        "3. **Devolver el objeto JSON** para que puedas manipularlo fácilmente en Python.\n",
        "\n",
        "En otras palabras, **`SimpleJsonOutputParser` convierte la cadena JSON en un diccionario de Python**.\n",
        "\n",
        "\n",
        "#### C\\. **¿Por qué no se puede usar directamente la salida del modelo?**\n",
        "\n",
        "Ya esta dicho. Si no usas `SimpleJsonOutputParser` (o algo similar), la salida del modelo sería una cadena de texto, no un diccionario. Por ejemplo:\n",
        "\n",
        "```\n",
        "output \\= '{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web\n",
        "interactivas.\", \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente\n",
        "utilizando scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Si intentas acceder a `output[\"respuesta\"]`, obtendrás un error, porque `output` **es una cadena, no un diccionario**. Necesitas parsearla primero.\n"
      ],
      "metadata": {
        "id": "2GhBgaxECsUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 6: Prompt + LLM + OutputParser + funcion personalizada**\n",
        "---\n",
        "\n",
        "Vamos a construir una cadena que incluya una funcion personalizada que destaque las palabras de mas de 6 letras del resultado.\n"
      ],
      "metadata": {
        "id": "roleoYVnD62_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Definimos el prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un traductor profesional.\"),\n",
        "    (\"human\", \"Traduce el siguiente texto al inglés: {texto}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Función personalizada para resaltar palabras clave\n",
        "def resaltar_palabras_clave(output):\n",
        "    # Dividimos el texto en palabras\n",
        "    palabras = output.split()\n",
        "    # Resaltamos las palabras clave (en este caso, palabras con más de 6 letras)\n",
        "    texto_resaltado = []\n",
        "    for palabra in palabras:\n",
        "        if len(palabra) > 6:\n",
        "            texto_resaltado.append(f\"_{palabra}_\")  # Resaltamos con asteriscos\n",
        "        else:\n",
        "            texto_resaltado.append(palabra)\n",
        "    return \" \".join(texto_resaltado)\n",
        "\n",
        "# Creamos la cadena con LCEL: prompt | llm | función personalizada\n",
        "chain = prompt_template | llm | StrOutputParser() | resaltar_palabras_clave\n",
        "\n",
        "# 5. Ejecutamos la cadena\n",
        "texto_original = \"La inteligencia artificial está transformando la industria tecnológica.\"\n",
        "respuesta = chain.invoke({\"texto\": texto_original})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "nxMOCF-xdJoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StrOutputParser se encarga de extraer el contenido de texto del objeto AIMessage generado por el modelo. Esto elimina la necesidad de acceder manualmente a output.content en la función personalizada."
      ],
      "metadata": {
        "id": "zHVsqwyqHFZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 7: Cadena con dos LLMs (Generación y Resumen)**\n",
        "---\n",
        "\n",
        "Vamos a crear una cadena donde un LLM genera contenido y otro LLM resumen ese contenido."
      ],
      "metadata": {
        "id": "BXmI1zF0VA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primer LLM: Genera contenido\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un artículo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# Segundo LLM: Resumen del contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente artículo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = generate_prompt | llm1 | summarize_prompt | llm2\n",
        "\n",
        "# Ejecutar la cadena\n",
        "salida = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "display(Markdown(salida.content))"
      ],
      "metadata": {
        "id": "ONG7qg2UVYwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reto: Intenta que se visualizen ambos textos. El articulo completo y el resultado"
      ],
      "metadata": {
        "id": "rZxLlvo3W0MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crea aqui tu solucioón"
      ],
      "metadata": {
        "id": "pe2fA0J_XGHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solución"
      ],
      "metadata": {
        "id": "zIx2KlznXNWf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-Fo4paIQEYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 8: Integración con API Externa (Wikipedia)**\n",
        "---\n",
        "\n",
        "Podemos alimentar al modelo con el resultado de una llamada a una API externa.  \n",
        "Este ejemplo creamos una cadena que usa la API de Wikipedia para obtener un resumen de un artículo utilizando ChatPromptTemplate.\n",
        "\n",
        "Observa que en este caso la llamada a la API de Wikipedia no esta integrada en la cadena. Esto no del del todo correcto, lo hacemos asi para mantener esta vez el codigo más sencillo.  \n",
        "Ademas LangChain proporciona conexiones con diferentes fuentes de datos (entre ellos Wikipedia) y trataremos esto mas adelante."
      ],
      "metadata": {
        "id": "YQFIIKCx3ahf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Función para obtener datos de la API de Wikipedia\n",
        "def fetch_wikipedia_summary(titulo_articulo):\n",
        "    url = f\"https://es.wikipedia.org/api/rest_v1/page/summary/{titulo_articulo}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"extract\", \"No se encontró un resumen para este artículo.\")\n",
        "    else:\n",
        "        return \"Error al obtener el artículo de Wikipedia.\"\n",
        "\n",
        "# LLM: Genera un resumen basado en los datos de Wikipedia\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Prompt para generar un resumen\n",
        "resumen_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Basado en la siguiente información de Wikipedia, escribe un resumen conciso en forma de viñetas o bullets points de entre 3 y 5 frases sobre:\\n{info}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = resumen_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Obtener el resumen de un artículo de Wikipedia\n",
        "articulo = \"LLM\"  # Cambia esto por el título del artículo que desees\n",
        "wikipedia_info = fetch_wikipedia_summary(articulo)\n",
        "\n",
        "# Ejecutar la cadena\n",
        "resumen = chain.invoke({\"info\": wikipedia_info})\n",
        "display(Markdown(resumen))"
      ],
      "metadata": {
        "id": "8uMPCxXq38iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 9: Procesamiento de Contenido de URL**\n",
        "---\n",
        "\n",
        "En este ejemplo, obtenemos la información de Wikipedia (pero podria ser cualquier URL) sin usar su API. En su lugar accedemos al contenido de la pagina con Requests y se lo pasamos al modelo.\n",
        "En resultado lo mostramos en pantalla y lo guardamos en un fichero."
      ],
      "metadata": {
        "id": "9walLFBS5iss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Función para obtener el contenido de una URL\n",
        "def fetch_url_content(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "# LLM: Procesa el contenido de la URL\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    template=\"Analiza el siguiente contenido y proporciona una síntesis:\\n{content}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = fetch_url_content | prompt_template | llm | StrOutputParser()\n",
        "\n",
        "# Ejecutar la cadena y guardar el resultado\n",
        "resumen = chain.invoke(\"https://es.wikipedia.org/wiki/Python\")\n",
        "with open(\"resumen.txt\", \"w\") as file:\n",
        "    file.write(resumen)\n",
        "print(\"Resumen guardado en resumen.txt\")\n",
        "display(Markdown(resumen))"
      ],
      "metadata": {
        "id": "cwq1N7yJ5u60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 10: ???**\n",
        "\n",
        "No hay ejemplo 10. Planteate este reto:\n",
        "\n",
        "XXXXXXXX\n"
      ],
      "metadata": {
        "id": "E31PB1arCy9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe aqui la solución"
      ],
      "metadata": {
        "id": "4JAUlArwDBFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución"
      ],
      "metadata": {
        "id": "uJBxTT9zIdoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AQui va la solución"
      ],
      "metadata": {
        "id": "-zYdgpTnqUjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente tema / cuaderno:  \n",
        "\n",
        "Curso de Langchain en Github: https://github.com/juanfranbrv/curso-langchain\n",
        "\n",
        "Enlaces de interes sobre cadenas LCEL:\n",
        "\n",
        "1. https://python.langchain.com/docs/concepts/lcel/\n",
        "1. https://www.paradigmadigital.com/dev/uso-de-cadenas-langchain-gen-ai/\n",
        "\n",
        "2. https://python.langchain.com/v0.1/docs/modules/chains/\n",
        "\n",
        "3. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "4. https://medium.com/@itsmybestview/unbridling-the-power-of-langchain-framework-with-lcel-9e5f7bf8af74\n",
        "\n",
        "5. https://www.youtube.com/playlist?list=PLGPnu4k-KSC1s7apArPZz1AavDJ4Mvitd\n",
        "\n",
        "6. https://www.youtube.com/watch?v=LzxSY7197ns&t=12s\n",
        "\n",
        "7. https://www.youtube.com/watch?v=H8DrR2pXbww&list=PLilZ1IiRt0R26etAQeQ9xa5h5UzK18zie&index=3\n",
        "\n",
        "8. https://www.youtube.com/watch?v=8BV9TW490nQ&t=523s\n",
        "\n",
        "9. https://www.youtube.com/watch?v=8BV9TW490nQ\n"
      ],
      "metadata": {
        "id": "WD4uu7CIIpTA"
      }
    }
  ]
}