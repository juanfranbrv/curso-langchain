{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH5Ry/KC2NkuygCLPSeH4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadenas%20b%C3%A1sicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las cadenas en LangChain representan una secuencia de operaciones que procesan datos de entrada a través de uno o más modelos de lenguaje. Cada operación de una cadena puede considerarse como un paso de un flujo de trabajo, donde el resultado de un paso sirve como entrada para el siguiente. Estas cadenas pueden ser simples o complejas, según los requisitos del proyecto.\n",
        "\n",
        "Hay varios tipos de cadenas en LangChain:\n",
        "\n",
        "- Cadenas secuenciales : una disposición lineal de tareas donde el resultado de una tarea alimenta directamente a la siguiente.\n",
        "- Cadenas paralelas : tareas que pueden realizarse simultáneamente, lo que permite un procesamiento más rápido.\n",
        "- Cadenas condicionales : cadenas que ejecutan diferentes rutas en función de la entrada proporcionada."
      ],
      "metadata": {
        "id": "0WdsL3-kpSC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuevas cadenas con LangChain Expression Language (LCEL)\n",
        "---\n",
        "\n",
        "En octubre de 2023, LangChain introdujo una nueva forma de trabajar con cadenas, más eficiente y simple, utilizando el operador |.  \n",
        "\n",
        "Esto permite encadenar elementos de manera intuitiva, con soporte para stream, batch y operaciones asíncronas.  \n",
        "\n",
        "Además, con LCEL (LangChain Expression Language), las cadenas se pueden definir de forma declarativa y funcional, eliminando la necesidad de clases complejas.   \n",
        "\n",
        "Aunque las cadenas \"antiguas\" siguen funcionando, es recomendable adoptar esta nueva sintaxis.\n",
        "\n",
        "### Ventajas del nuevo sistema:\n",
        "\n",
        "- **Alineación con el framework:** La nueva sintaxis es el estándar recomendado para futuros proyectos.\n",
        "- **Interface unificada:** Simplifica la integración de múltiples métodos.\n",
        "- **Facilidad de composición:** Permite estructurar cadenas de forma secuencial, paralela o con fallbacks.\n",
        "- **Código más limpio:** Menos líneas para lograr los mismos resultados."
      ],
      "metadata": {
        "id": "5RmTZhkweukJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del entorno del cuaderno\n",
        "---\n",
        "\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).  \n",
        "1. Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "2. Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "3. Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "_jVrgFCnxbte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# Importamos las clases necesarias para trabajar con cadenas\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "0kpJPccSxabU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo mas básico de cadena seria  PROMPT | LLM\n",
        "\n",
        "En este la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo. Esto forma una cadena (LLMChain) que toma un texto, lo pasa al prompt y luego al modelo.\n",
        "\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Esto es basicamente lo que hemos venido haciendo hasta ahora sin cadenas."
      ],
      "metadata": {
        "id": "lDXFodi33rZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos los componentes\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¿cómo estás?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Francés\", \"texto\":\"Hola, ¿cómo estás?\"})\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "m-UHdYGEWaO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observese que esto es lo que hemos hecho hasta ahora sin cadenas... invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia en un ejemplo tan sencillo...\n",
        "\n",
        "La diferencia principal es que:\n",
        "\n",
        "1. No hay abstracción del prompt como objeto\n",
        "2. Tenemos que formatear el texto del prompt manualmente\n",
        "3. No hay una estructura que maneje los inputs de forma estandarizada"
      ],
      "metadata": {
        "id": "laEFQWsLRJbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Creamos el prompt y lo formateamos\n",
        "prompt_template = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt_formateado = prompt_template.format(idioma=\"Francés\", texto=\"Hola, ¿cómo estás?\")\n",
        "\n",
        "# Invocamos el LLM directamente\n",
        "respuesta = llm.invoke(prompt_formateado)\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "3OddOArKRXD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos unos ejemplos sencillos de tres elementos del tipo PROMPT | LLM | TRANSFORMACION.\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un bjeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar"
      ],
      "metadata": {
        "id": "GZyeh8OxXrDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos una función de transformación\n",
        "def transformar_texto(output):\n",
        "    return output.content.upper()\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "mJR5vHrfX31N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mejoremos un poco el codigo y observemos la modularidad de la cadenas, reutilizando facilmnente la misma cadena en un bucle de preguntas.\n",
        "\n",
        "Vamos a hacer uso de un OutputParser. Mas adelante los explicaremos en detalle.\n",
        "\n",
        "En LangChain, los **parsers de salida** son componentes que se encargan de **transformar y estructurar la salida** generada por un modelo de lenguaje (LLM) o una cadena (chain). Su función principal es asegurarse que la salida esté en un formato específico y utilizable, como un string, un objeto JSON, una lista, etc.  \n",
        "\n",
        "#### **Tipos comunes de parsers de salida**\n",
        "\n",
        "- **`StrOutputParser`**: Convierte la salida en un string. (Usaremos este)\n",
        "    \n",
        "- **`JsonOutputParser`**: Convierte la salida en un objeto JSON.\n",
        "    \n",
        "- **`CommaSeparatedListOutputParser`**: Convierte la salida en una lista separada por comas.\n",
        "    \n",
        "- **`StructuredOutputParser`**: Permite definir un esquema personalizado para la salida."
      ],
      "metadata": {
        "id": "qxQbWY-kdB7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una función de transformación\n",
        "# Explicación: Esta función toma la salida del OutPutParse y la convierte a mayúsculas.\n",
        "def transformar_texto(output):\n",
        "    return output.upper()\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas\n",
        "# Observa cómo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¿Quién fue Napoleón Bonaparte?\"},\n",
        "    {\"tema\": \"Programación\", \"pregunta\": \"¿Qué es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "IuwH1XTXdBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otro ejemplo de cadena: PROMPT | LLM | OutputParser | Guardar en archivo\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "auY_uQJshX6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Definimos una función para guardar en archivo\n",
        "def guardar_en_archivo(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en -mente\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "sntxjr0xh4kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hagamos ahora una Cadena de 3 elementos (Prompt + LLM + OutputParser)\n",
        "En este caso, usamos un OutputParser que crearemos nosotros mediante una funcion personalizada para estructurar la salida del LLM en un formato específico, como un diccionario.\n",
        "\n"
      ],
      "metadata": {
        "id": "MMLf1KxWrjcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto en {tema}. Devuelve el resultado en formato JSON con las claves 'respuesta' y 'explicacion'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"tema\": \"Python\", \"pregunta\": \"¿Qué es el Streamlit?\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "25m9G__Gr-Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "si el modelo ya da la respuesta en json (se le ha indicado en el prompt) porque se se utiliza json_parser = SimpleJsonOutputParser() ????\n",
        "\n",
        "\n",
        "La respuesta tiene que ver con cómo funcionan los modelos de lenguaje y cómo se manejan sus salidas en LangChain. Vamos a desglosarlo:\n",
        "\n",
        "---\n",
        "\n",
        "### 1\\. **El modelo devuelve una cadena de texto**\n",
        "\n",
        "Aunque le indiques al modelo que genere una respuesta en formato JSON, la salida del modelo sigue siendo una **cadena de texto** (un `string`). Por ejemplo, el modelo podría devolver algo como esto:\n",
        "\n",
        "python\n",
        "\n",
        "Copy\n",
        "\n",
        "'{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web interactivas.\", \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente utilizando scripts de Python.\"}'\n",
        "\n",
        "Esta cadena de texto **no es un objeto JSON** en sí misma, sino una representación textual de un JSON. Para poder trabajar con ella en Python (por ejemplo, acceder a las claves `respuesta` y `explicacion`), necesitas convertirla en un diccionario de Python.\n",
        "\n",
        "---\n",
        "\n",
        "### 2\\. **¿Qué hace `SimpleJsonOutputParser`?**\n",
        "\n",
        "`SimpleJsonOutputParser` se encarga de:\n",
        "\n",
        "1. **Tomar la cadena de texto** que el modelo ha generado.\n",
        "    \n",
        "2. **Convertirla en un objeto JSON** (por ejemplo, un diccionario o una lista) utilizando `json.loads()` internamente.\n",
        "    \n",
        "3. **Devolver el objeto JSON** para que puedas manipularlo fácilmente en Python.\n",
        "    \n",
        "\n",
        "En otras palabras, **`SimpleJsonOutputParser` convierte la cadena JSON en un diccionario de Python**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3\\. **¿Por qué no se puede usar directamente la salida del modelo?**\n",
        "\n",
        "Si no usas `SimpleJsonOutputParser` (o algo similar), la salida del modelo sería una cadena de texto, no un diccionario. Por ejemplo:\n",
        "\n",
        "```\n",
        "output \\= '{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web interactivas.\", \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente utilizando scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Si intentas acceder a `output[\"respuesta\"]`, obtendrás un error, porque `output` es una cadena, no un diccionario. Necesitas parsearla primero.\n"
      ],
      "metadata": {
        "id": "2GhBgaxECsUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs:\n",
        "\n",
        "1. https://www.paradigmadigital.com/dev/uso-de-cadenas-langchain-gen-ai/\n",
        "\n",
        "2. https://python.langchain.com/v0.1/docs/modules/chains/\n",
        "\n",
        "3. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "4. https://medium.com/@itsmybestview/unbridling-the-power-of-langchain-framework-with-lcel-9e5f7bf8af74\n",
        "\n",
        "5. https://www.youtube.com/playlist?list=PLGPnu4k-KSC1s7apArPZz1AavDJ4Mvitd\n",
        "\n",
        "6. https://www.youtube.com/watch?v=LzxSY7197ns&t=12s\n",
        "\n",
        "7. https://www.youtube.com/watch?v=H8DrR2pXbww&list=PLilZ1IiRt0R26etAQeQ9xa5h5UzK18zie&index=3\n",
        "\n",
        "8. https://www.youtube.com/watch?v=8BV9TW490nQ&t=523s\n",
        "\n",
        "9. https://www.youtube.com/watch?v=8BV9TW490nQ\n",
        "\n",
        "\n",
        "Usar esto para localizar\n",
        "https://www.youtube.com/results?search_query=langchain+lcel+\n"
      ],
      "metadata": {
        "id": "0AN8q5pGWadx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uno de los paso de la cadena puede ser una funcion personalizada. Habitualmente para procesar la salida de una forma difrente a los OutParser qu eproporciona LangChain\n",
        "\n",
        "Vamos a construir una cadena que incluya una funcion personalizada que destaque las palabras de mas de 6 letras del resultado.\n"
      ],
      "metadata": {
        "id": "roleoYVnD62_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# 1. Definimos el prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un traductor profesional.\"),\n",
        "    (\"human\", \"Traduce el siguiente texto al inglés: {texto}\"),\n",
        "])\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Función personalizada para resaltar palabras clave\n",
        "def resaltar_palabras_clave(output):\n",
        "    # Dividimos el texto en palabras\n",
        "    palabras = output.split()\n",
        "    # Resaltamos las palabras clave (en este caso, palabras con más de 6 letras)\n",
        "    texto_resaltado = []\n",
        "    for palabra in palabras:\n",
        "        if len(palabra) > 6:\n",
        "            texto_resaltado.append(f\"_{palabra}_\")  # Resaltamos con asteriscos\n",
        "        else:\n",
        "            texto_resaltado.append(palabra)\n",
        "    return \" \".join(texto_resaltado)\n",
        "\n",
        "# 4. Creamos la cadena con LCEL: prompt | llm | función personalizada\n",
        "chain = prompt_template | llm | StrOutputParser() | resaltar_palabras_clave\n",
        "\n",
        "# 5. Ejecutamos la cadena\n",
        "texto_original = \"La inteligencia artificial está transformando la industria tecnológica.\"\n",
        "respuesta = chain.invoke({\"texto\": texto_original})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "nxMOCF-xdJoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "15e8f663-9caf-4c0a-80c3-920cd83eff53"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_Artificial_ _intelligence_ is _transforming_ the _technology_ _industry._"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StrOutputParser se encarga de extraer el contenido de texto del objeto AIMessage generado por el modelo. Esto elimina la necesidad de acceder manualmente a output.content en la función personalizada."
      ],
      "metadata": {
        "id": "zHVsqwyqHFZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo X: Cadena con dos LLMs (Generación y Resumen)**\n",
        "\n",
        "Vamos a crear una cadena donde un LLM genera contenido y otro LLM resumen ese contenido."
      ],
      "metadata": {
        "id": "BXmI1zF0VA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primera LLM: Genera contenido\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un artículo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# Segunda LLM: Resumen del contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente artículo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = generate_prompt | llm1 | summarize_prompt | llm2\n",
        "\n",
        "# Ejecutar la cadena\n",
        "salida = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "display(Markdown(salida.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ONG7qg2UVYwp",
        "outputId": "00b5502e-af7e-4e8c-9ce3-d4004b34ac44"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Resumen de la Inteligencia Artificial: Transformando el Futuro**\n\nLa Inteligencia Artificial (IA) ha evolucionado desde un concepto teórico a una herramienta práctica que transforma diversos aspectos de la vida. Desde su inicio en 1956, la IA ha pasado por fases de optimismo y desilusión. Recientemente, los avances en aprendizaje automático y procesamiento de datos masivos han revivido el interés en la IA.\n\n**Tipos de IA:**\n* IA Débil (Narrow AI): Realiza tareas específicas (por ejemplo, asistentes virtuales, sistemas de recomendación).\n* IA Fuerte (General AI): Realiza cualquier tarea cognitiva que un humano puede hacer (todavía en desarrollo).\n\n**Aplicaciones de la IA:**\n* Salud: Diagnósticos asistidos, personalización de tratamientos.\n* Movilidad y Transporte: Vehículos autónomos.\n* Finanzas: Detección de fraudes, predicción de tendencias del mercado.\n* Marketing y Publicidad: Campañas personalizadas, segmentación de público.\n* Agricultura: Optimización de la producción, monitoreo de cultivos.\n\n**Desafíos y Ética:**\n* Ética: Los sesgos en los datos de entrenamiento pueden conducir a decisiones injustas.\n* Privacidad: La recopilación masiva de datos puede infringir la privacidad.\n* Seguridad: La dependencia de la IA en sistemas críticos plantea riesgos de ataques cibernéticos.\n\n**El Futuro de la IA:**\n* Integración continua en todos los sectores, mejorando la eficiencia y la productividad.\n* Enfoque cuidadoso en la regulación y la gobernanza para garantizar un desarrollo ético.\n* Colaboraciones y educación para preparar a las generaciones futuras para un mundo impulsado por la IA.\n\nLa IA ofrece un gran potencial para resolver problemas y mejorar la calidad de vida, pero también plantea desafíos que deben abordarse para garantizar su desarrollo beneficioso."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reto: Intenta que se visualizen ambos textos. El articulo completo y el resultado"
      ],
      "metadata": {
        "id": "rZxLlvo3W0MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crea aqui tu solucioón"
      ],
      "metadata": {
        "id": "pe2fA0J_XGHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución"
      ],
      "metadata": {
        "id": "zIx2KlznXNWf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZcXfofoXSI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo X: Integración con API Externa (OpenWeatherMap)\n",
        "\n",
        "Podemos alimentar al modelo con el resultado de una llamada a una API externa.  \n",
        "Este ejemplo consulta la API de OpenWeatherMap, obtiene datos meteorológicos y genera un informe personalizado con un LLM."
      ],
      "metadata": {
        "id": "YQFIIKCx3ahf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Función para obtener datos de la API\n",
        "def fetch_weather(city):\n",
        "    api_key = \"YOUR_OPENWEATHERMAP_API_KEY\"\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "# LLM: Genera un informe meteorológico personalizado\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", api_key=\"YOUR_API_KEY\")\n",
        "weather_prompt = PromptTemplate(\n",
        "    input_variables=[\"weather_data\"],\n",
        "    template=\"Basado en los siguientes datos meteorológicos, proporciona un informe personalizado:\\n{weather_data}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = fetch_weather | weather_prompt | llm\n",
        "\n",
        "# Ejecutar la cadena\n",
        "report = chain.invoke({\"city\": \"Madrid\"})\n",
        "print(\"Informe meteorológico:\\n\", report)"
      ],
      "metadata": {
        "id": "8uMPCxXq38iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo X: Procesamiento de Contenido de URL\n",
        "\n",
        "Crea una cadena que obtiene el contenido de una URL, lo procesa con un LLM y guarda el resultado en un archivo."
      ],
      "metadata": {
        "id": "9walLFBS5iss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Función para obtener el contenido de una URL\n",
        "def fetch_url_content(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "# LLM: Procesa el contenido de la URL\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", api_key=\"YOUR_API_KEY\")\n",
        "process_prompt = PromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    template=\"Analiza el siguiente contenido y proporciona una síntesis:\\n{content}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = fetch_url_content | process_prompt | llm\n",
        "\n",
        "# Ejecutar la cadena y guardar el resultado\n",
        "synthesis = chain.invoke({\"url\": \"https://es.wikipedia.org/wiki/Inteligencia_artificial\"})\n",
        "with open(\"synthesis.txt\", \"w\") as file:\n",
        "    file.write(synthesis)\n",
        "print(\"Síntesis guardada en synthesis.txt\")"
      ],
      "metadata": {
        "id": "cwq1N7yJ5u60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo X: Retroalimentacion\n",
        "\n",
        "La salida de un llm (por ejemplo un texto extenso) se intruce en otro y se le pide qu elo mejore o lo cambien de alguna forma\n"
      ],
      "metadata": {
        "id": "E31PB1arCy9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los prompts y el LLM\n",
        "prompt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Escribe un resumen sobre {topic}.\")\n",
        "prompt2 = PromptTemplate(input_variables=[\"summary\"], template=\"Mejora el siguiente texto:\\n{summary}\")\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "# Crear la cadena con retroalimentación\n",
        "chain = prompt1 | llm | prompt2 | llm\n",
        "\n",
        "# Ejecutar la cadena\n",
        "result = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4JAUlArwDBFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}