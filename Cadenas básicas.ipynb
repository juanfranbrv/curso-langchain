{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0sHCdbuPDX+ElmRLyyXj2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadenas%20b%C3%A1sicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las cadenas en LangChain representan una secuencia de operaciones que procesan datos de entrada a través de uno o más modelos de lenguaje. Cada operación de una cadena puede considerarse como un paso de un flujo de trabajo, donde el resultado de un paso sirve como entrada para el siguiente. Estas cadenas pueden ser simples o complejas, según los requisitos del proyecto.\n",
        "\n",
        "Hay varios tipos de cadenas en LangChain:\n",
        "\n",
        "- Cadenas secuenciales : una disposición lineal de tareas donde el resultado de una tarea alimenta directamente a la siguiente.\n",
        "- Cadenas paralelas : tareas que pueden realizarse simultáneamente, lo que permite un procesamiento más rápido.\n",
        "- Cadenas condicionales : cadenas que ejecutan diferentes rutas en función de la entrada proporcionada."
      ],
      "metadata": {
        "id": "0WdsL3-kpSC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuevas cadenas con LangChain Expression Language (LCEL)\n",
        "---\n",
        "\n",
        "En octubre de 2023, LangChain introdujo una nueva forma de trabajar con cadenas, más eficiente y simple, utilizando el operador |.  \n",
        "\n",
        "Esto permite encadenar elementos de manera intuitiva, con soporte para stream, batch y operaciones asíncronas.  \n",
        "\n",
        "Además, con LCEL (LangChain Expression Language), las cadenas se pueden definir de forma declarativa y funcional, eliminando la necesidad de clases complejas.   \n",
        "\n",
        "Aunque las cadenas \"antiguas\" siguen funcionando, es recomendable adoptar esta nueva sintaxis.\n",
        "\n",
        "### Ventajas del nuevo sistema:\n",
        "\n",
        "- **Alineación con el framework:** La nueva sintaxis es el estándar recomendado para futuros proyectos.\n",
        "- **Interface unificada:** Simplifica la integración de múltiples métodos.\n",
        "- **Facilidad de composición:** Permite estructurar cadenas de forma secuencial, paralela o con fallbacks.\n",
        "- **Código más limpio:** Menos líneas para lograr los mismos resultados."
      ],
      "metadata": {
        "id": "5RmTZhkweukJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del entorno del cuaderno\n",
        "---\n",
        "\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).  \n",
        "1. Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "2. Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "3. Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "_jVrgFCnxbte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# Importamos las clases necesarias para trabajar con cadenas\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "0kpJPccSxabU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo mas básico de cadena seria  PROMPT | LLM\n",
        "\n",
        "En este la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo. Esto forma una cadena (LLMChain) que toma un texto, lo pasa al prompt y luego al modelo.\n",
        "\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Esto es basicamente lo que hemos venido haciendo hasta ahora sin cadenas."
      ],
      "metadata": {
        "id": "lDXFodi33rZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos los componentes\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¿cómo estás?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Francés\", \"texto\":\"Hola, ¿cómo estás?\"})\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "m-UHdYGEWaO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observese que esto es lo que hemos hecho hasta ahora sin cadenas... invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia en un ejemplo tan sencillo...\n",
        "\n",
        "La diferencia principal es que:\n",
        "\n",
        "1. No hay abstracción del prompt como objeto\n",
        "2. Tenemos que formatear el texto del prompt manualmente\n",
        "3. No hay una estructura que maneje los inputs de forma estandarizada"
      ],
      "metadata": {
        "id": "laEFQWsLRJbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Creamos el prompt y lo formateamos\n",
        "prompt_template = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt_formateado = prompt_template.format(idioma=\"Francés\", texto=\"Hola, ¿cómo estás?\")\n",
        "\n",
        "# Invocamos el LLM directamente\n",
        "respuesta = llm.invoke(prompt_formateado)\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "3OddOArKRXD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos unos ejemplos sencillos de tres elementos del tipo PROMPT | LLM | TRANSFORMACION.\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un bjeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar"
      ],
      "metadata": {
        "id": "GZyeh8OxXrDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos una función de transformación\n",
        "def transformar_texto(output):\n",
        "    return output.content.upper()\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "mJR5vHrfX31N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mejoremos un poco el codigo y observemos la modularidad de la cadenas, reutilizando facilmnente la misma cadena en un bucle de preguntas.\n",
        "\n",
        "Vamos a hacer uso de un OutputParser. Mas adelante los explicaremos en detalle.\n",
        "\n",
        "En LangChain, los **parsers de salida** son componentes que se encargan de **transformar y estructurar la salida** generada por un modelo de lenguaje (LLM) o una cadena (chain). Su función principal es asegurarse que la salida esté en un formato específico y utilizable, como un string, un objeto JSON, una lista, etc.  \n",
        "\n",
        "#### **Tipos comunes de parsers de salida**\n",
        "\n",
        "- **`StrOutputParser`**: Convierte la salida en un string. (Usaremos este)\n",
        "    \n",
        "- **`JsonOutputParser`**: Convierte la salida en un objeto JSON.\n",
        "    \n",
        "- **`CommaSeparatedListOutputParser`**: Convierte la salida en una lista separada por comas.\n",
        "    \n",
        "- **`StructuredOutputParser`**: Permite definir un esquema personalizado para la salida."
      ],
      "metadata": {
        "id": "qxQbWY-kdB7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una función de transformación\n",
        "# Explicación: Esta función toma la salida del OutPutParse y la convierte a mayúsculas.\n",
        "def transformar_texto(output):\n",
        "    return output.upper()\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas\n",
        "# Observa cómo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¿Quién fue Napoleón Bonaparte?\"},\n",
        "    {\"tema\": \"Programación\", \"pregunta\": \"¿Qué es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "IuwH1XTXdBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otro ejemplo de cadena: PROMPT | LLM | OutputParser | Guardar en archivo\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "auY_uQJshX6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Definimos una función para guardar en archivo\n",
        "def guardar_en_archivo(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en -mente\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "sntxjr0xh4kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hagamos ahora una Cadena de 3 elementos (Prompt + LLM + OutputParser)\n",
        "En este caso, usamos un OutputParser que crearemos nosotros mediante una funcion personalizada para estructurar la salida del LLM en un formato específico, como un diccionario.\n",
        "\n"
      ],
      "metadata": {
        "id": "MMLf1KxWrjcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto en {tema}. Devuelve el resultado en formato JSON con las claves 'respuesta' y 'explicacion'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"tema\": \"Python\", \"pregunta\": \"¿Qué es el Streamlit?\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "25m9G__Gr-Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "si el modelo ya da la respuesta en json (se le ha indicado en el prompt) porque se se utiliza json_parser = SimpleJsonOutputParser() ????\n",
        "\n",
        "\n",
        "La respuesta tiene que ver con cómo funcionan los modelos de lenguaje y cómo se manejan sus salidas en LangChain. Vamos a desglosarlo:\n",
        "\n",
        "---\n",
        "\n",
        "### 1\\. **El modelo devuelve una cadena de texto**\n",
        "\n",
        "Aunque le indiques al modelo que genere una respuesta en formato JSON, la salida del modelo sigue siendo una **cadena de texto** (un `string`). Por ejemplo, el modelo podría devolver algo como esto:\n",
        "\n",
        "python\n",
        "\n",
        "Copy\n",
        "\n",
        "'{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web interactivas.\", \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente utilizando scripts de Python.\"}'\n",
        "\n",
        "Esta cadena de texto **no es un objeto JSON** en sí misma, sino una representación textual de un JSON. Para poder trabajar con ella en Python (por ejemplo, acceder a las claves `respuesta` y `explicacion`), necesitas convertirla en un diccionario de Python.\n",
        "\n",
        "---\n",
        "\n",
        "### 2\\. **¿Qué hace `SimpleJsonOutputParser`?**\n",
        "\n",
        "`SimpleJsonOutputParser` se encarga de:\n",
        "\n",
        "1. **Tomar la cadena de texto** que el modelo ha generado.\n",
        "    \n",
        "2. **Convertirla en un objeto JSON** (por ejemplo, un diccionario o una lista) utilizando `json.loads()` internamente.\n",
        "    \n",
        "3. **Devolver el objeto JSON** para que puedas manipularlo fácilmente en Python.\n",
        "    \n",
        "\n",
        "En otras palabras, **`SimpleJsonOutputParser` convierte la cadena JSON en un diccionario de Python**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3\\. **¿Por qué no se puede usar directamente la salida del modelo?**\n",
        "\n",
        "Si no usas `SimpleJsonOutputParser` (o algo similar), la salida del modelo sería una cadena de texto, no un diccionario. Por ejemplo:\n",
        "\n",
        "```\n",
        "output \\= '{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web interactivas.\", \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente utilizando scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Si intentas acceder a `output[\"respuesta\"]`, obtendrás un error, porque `output` es una cadena, no un diccionario. Necesitas parsearla primero.\n"
      ],
      "metadata": {
        "id": "2GhBgaxECsUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs:\n",
        "\n",
        "1. https://www.paradigmadigital.com/dev/uso-de-cadenas-langchain-gen-ai/\n",
        "\n",
        "2. https://python.langchain.com/v0.1/docs/modules/chains/\n",
        "\n",
        "3. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "4. https://medium.com/@itsmybestview/unbridling-the-power-of-langchain-framework-with-lcel-9e5f7bf8af74\n",
        "\n",
        "5. https://www.youtube.com/playlist?list=PLGPnu4k-KSC1s7apArPZz1AavDJ4Mvitd\n",
        "\n",
        "6. https://www.youtube.com/watch?v=LzxSY7197ns&t=12s\n",
        "\n",
        "7. https://www.youtube.com/watch?v=H8DrR2pXbww&list=PLilZ1IiRt0R26etAQeQ9xa5h5UzK18zie&index=3\n",
        "\n",
        "8. https://www.youtube.com/watch?v=8BV9TW490nQ&t=523s\n",
        "\n",
        "9. https://www.youtube.com/watch?v=8BV9TW490nQ\n",
        "\n",
        "\n",
        "Usar esto para localizar\n",
        "https://www.youtube.com/results?search_query=langchain+lcel+\n"
      ],
      "metadata": {
        "id": "0AN8q5pGWadx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uno de los paso de la cadena puede ser una funcion personalizada. Habitualmente para procesar la salida de una forma difrente a los OutParser qu eproporciona LangChain\n",
        "\n",
        "Vamos a construir una cadena que incluya una funcion personalizada que destaque las palabras de mas de 6 letras del resultado.\n"
      ],
      "metadata": {
        "id": "roleoYVnD62_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# 1. Definimos el prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un traductor profesional.\"),\n",
        "    (\"human\", \"Traduce el siguiente texto al inglés: {texto}\"),\n",
        "])\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Función personalizada para resaltar palabras clave\n",
        "def resaltar_palabras_clave(output):\n",
        "    # Dividimos el texto en palabras\n",
        "    palabras = output.split()\n",
        "    # Resaltamos las palabras clave (en este caso, palabras con más de 6 letras)\n",
        "    texto_resaltado = []\n",
        "    for palabra in palabras:\n",
        "        if len(palabra) > 6:\n",
        "            texto_resaltado.append(f\"_{palabra}_\")  # Resaltamos con asteriscos\n",
        "        else:\n",
        "            texto_resaltado.append(palabra)\n",
        "    return \" \".join(texto_resaltado)\n",
        "\n",
        "# 4. Creamos la cadena con LCEL: prompt | llm | función personalizada\n",
        "chain = prompt_template | llm | StrOutputParser() | resaltar_palabras_clave\n",
        "\n",
        "# 5. Ejecutamos la cadena\n",
        "texto_original = \"La inteligencia artificial está transformando la industria tecnológica.\"\n",
        "respuesta = chain.invoke({\"texto\": texto_original})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "nxMOCF-xdJoh",
        "outputId": "15e8f663-9caf-4c0a-80c3-920cd83eff53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_Artificial_ _intelligence_ is _transforming_ the _technology_ _industry._"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo X: Cadena con dos LLMs (Generación y Resumen)**\n",
        "\n",
        "Vamos a crear una cadena donde un LLM genera contenido y otro LLM resumen ese contenido."
      ],
      "metadata": {
        "id": "BXmI1zF0VA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primera LLM: Genera contenido\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un artículo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# Segunda LLM: Resumen del contenido generado\n",
        "llm2 = OpenAI(model_name=\"text-davinci-003\", api_key=\"YOUR_API_KEY\")\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente artículo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = generate_prompt | llm1 | summarize_prompt | llm2\n",
        "\n",
        "# Ejecutar la cadena\n",
        "output = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "print(\"Resumen:\\n\", output)"
      ],
      "metadata": {
        "id": "ONG7qg2UVYwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StrOutputParser se encarga de extraer el contenido de texto del objeto AIMessage generado por el modelo. Esto elimina la necesidad de acceder manualmente a output.content en la función personalizada."
      ],
      "metadata": {
        "id": "zHVsqwyqHFZA"
      }
    }
  ]
}