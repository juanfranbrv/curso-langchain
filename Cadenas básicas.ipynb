{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbG6vpYyckteCyHObxPlxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadenas%20b%C3%A1sicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Cadenas en Langchain**\n",
        "\n",
        "Las cadenas en LangChain representan una secuencia de operaciones que procesan datos de entrada a trav칠s de uno o m치s modelos de lenguaje. Cada operaci칩n de una cadena puede considerarse como un paso de un flujo de trabajo, donde el resultado de un paso sirve como entrada para el siguiente. Estas cadenas pueden ser simples o complejas, seg칰n los requisitos del proyecto."
      ],
      "metadata": {
        "id": "0WdsL3-kpSC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nuevas cadenas con LangChain Expression Language (LCEL)**\n",
        "---\n",
        "\n",
        "En octubre de 2023, LangChain introdujo una nueva forma de trabajar con cadenas, m치s eficiente y simple, utilizando el operador | (pipe).  \n",
        "\n",
        "Esto permite encadenar elementos de manera intuitiva, con soporte para stream, batch y operaciones as칤ncronas.  \n",
        "\n",
        "Adem치s, con LCEL (LangChain Expression Language), las cadenas se pueden definir de forma declarativa y funcional, eliminando la necesidad de clases complejas.   \n",
        "\n",
        "Aunque las cadenas \"antiguas\" siguen funcionando, es recomendable adoptar esta nueva sintaxis.\n",
        "\n",
        "### Ventajas del nuevo sistema:\n",
        "\n",
        "- **Alineaci칩n con el framework:** La nueva sintaxis es el est치ndar recomendado para futuros proyectos.\n",
        "- **Interface unificada:** Simplifica la integraci칩n de m칰ltiples m칠todos.\n",
        "- **Facilidad de composici칩n:** Permite estructurar cadenas de forma secuencial, paralela o con fallbacks.\n",
        "- **C칩digo m치s limpio:** Menos l칤neas para lograr los mismos resultados.\n",
        "\n",
        "En LCEL **(LangChain Expression Language**), no existen como tal tipos de cadenas predefinidos como \"secuenciales\", \"condicionales\" o \"paralelas\", pero puedes implementar estos patrones de flujo de trabajo utilizando la sintaxis de LCEL y algunas funciones adicionales.\n",
        "\n",
        "En este cuaderno nos vamos a ocupar solo del patron secuencial. En proximos cuadernos veremos el resto de patrones de dise침o."
      ],
      "metadata": {
        "id": "5RmTZhkweukJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuraci칩n del entorno del cuaderno**\n",
        "---\n",
        "\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).  \n",
        "1. Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "2. Instalamos la librer칤a LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "3. Importamos las clases espec칤ficas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dej치ndolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "_jVrgFCnxbte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la librer칤a `userdata` de Google Colab.\n",
        "# Esta librer칤a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librer칤as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer칤as si ya est치n instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librer칤a principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# Importamos las clases necesarias para trabajar con cadenas\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav칠s de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "0kpJPccSxabU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 1: Prompt + LLM**\n",
        "\n",
        "---\n",
        "Posiblemente esta es la cadena mas simple que podamos crear.  \n",
        "En este caso la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo.\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Observa que esto es basicamente lo que hemos venido haciendo hasta ahora sin cadenas."
      ],
      "metadata": {
        "id": "lDXFodi33rZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos el prompt template de una de esta formas\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, 쯖칩mo est치s?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Franc칠s\", \"texto\":\"Hola, 쯖칩mo est치s?\"})\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "m-UHdYGEWaO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que esto es lo que hemos hecho hasta ahora sin cadenas: Invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia y este enfoque puede ser m치s simple para tareas sencillas, carece de la flexibilidad y la extensibilidad que ofrecen las cadenas como se aprecia en cuanto la complejidad escala.\n",
        "\n",
        "Ademas LCEL y la nuevas cadenas proporcionan un nivel de abstracci칩n mayor tanto del prompt como del proceso en general.\n"
      ],
      "metadata": {
        "id": "laEFQWsLRJbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Creamos el prompt y lo formateamos\n",
        "prompt_template = PromptTemplate.from_template(\"Traduce esto al {idioma}: {texto}\")\n",
        "prompt_formateado = prompt_template.format(idioma=\"Franc칠s\", texto=\"Hola, 쯖칩mo est치s?\")\n",
        "\n",
        "# Invocamos el LLM directamente\n",
        "respuesta = llm.invoke(prompt_formateado)\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "3OddOArKRXD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 2: Prompt + LLM + funcion personalizada de transformaci칩n de la salida**\n",
        "---\n",
        "\n",
        "En este caso la salida del LLM se procesa con una funcion sencilla para pasarla a mayusculas. (pero puede ser tan complicada como se necesite)\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un objeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar.\n",
        "\n",
        "Vamos a usar en adelante display de IPython (en lugar de print) para imprimir el markdown resultante y que se vea menjor en pantalla."
      ],
      "metadata": {
        "id": "GZyeh8OxXrDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos una funci칩n de transformaci칩n\n",
        "def transformar_texto(output):\n",
        "    return output.content.upper()\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"쯈u칠 es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "mJR5vHrfX31N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prompt + LLM + OutputParser + funcion personalizada de transformaci칩n de la salida**\n",
        "---\n",
        "\n",
        "Este ejemplo mejora la modularidad reutilizando la cadena en un bucle y usando un OutputParser.\n",
        "\n",
        "En LangChain, los **parsers de salida** estructuran la salida del LLM en formatos espec칤ficos (string, JSON, lista, etc.).\n",
        "\n",
        "**Tipos comunes:**\n",
        "\n",
        "- **StrOutputParser**: Convierte a string (usado aqu칤).\n",
        "    \n",
        "- **JsonOutputParser**: Convierte a JSON.\n",
        "    \n",
        "- **CommaSeparatedListOutputParser**: Crea listas separadas por comas.\n",
        "    \n",
        "- **StructuredOutputParser**: Define esquemas de salida personalizados."
      ],
      "metadata": {
        "id": "qxQbWY-kdB7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una funci칩n de transformaci칩n\n",
        "# Esta funci칩n toma la salida del OutPutParser (str) y la convierte a may칰sculas.\n",
        "def transformar_texto(output):\n",
        "    return output.upper()\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas en un bucle\n",
        "# Observa c칩mo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"쯈u칠 es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"쯈ui칠n fue Napole칩n Bonaparte?\"},\n",
        "    {\"tema\": \"Programaci칩n\", \"pregunta\": \"쯈u칠 es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "IuwH1XTXdBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Prompt + LLM + OutputParser + Guardar en fichero**\n",
        "---\n",
        "Crea una cadena que genera una lista de elementos y la guarda en un archivo:\n",
        "\n",
        "- **Prompt:** Define un ChatPromptTemplate que instruye al sistema a crear listas sin numerar, en una columna y sin comentarios.\n",
        "    \n",
        "- **LLM:** Usa el modelo gpt-4o-mini de OpenAI para generar la respuesta.\n",
        "    \n",
        "- **OutputParser:** Formatea la respuesta como texto plano con StrOutputParser.\n",
        "    \n",
        "- **Funci칩n Personalizada:** La funci칩n guardar\\_en\\_archivo guarda la respuesta en \"respuesta.txt\" e imprime la respuesta en la consola.  \n",
        "\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "auY_uQJshX6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No a침adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Definimos una funci칩n para guardar en archivo\n",
        "def guardar_en_archivo(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en -mente\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "sntxjr0xh4kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + JsonOutputParser**\n",
        "---\n",
        "\n",
        "Esta vez genera una respuesta JSON.\n",
        "\n",
        "SimpleJsonOutputParser transforma la salida del modelo en un objeto JSON v치lido.\n"
      ],
      "metadata": {
        "id": "MMLf1KxWrjcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto en {tema}. Devuelve el resultado en formato JSON con las claves 'respuesta' y 'explicacion'. No a침adas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"tema\": \"Python\", \"pregunta\": \"쯈u칠 es el Streamlit?\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "25m9G__Gr-Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**游녤游낕 si el modelo ya da la respuesta en JSON (se le ha indicado en el prompt) porque se se utiliza json_parser = SimpleJsonOutputParser() para contruir la salida ?**\n",
        "\n",
        "\n",
        "La respuesta tiene que ver con c칩mo funcionan los modelos de lenguaje y c칩mo se manejan sus salidas en LangChain. Vamos a desglosarlo:\n",
        "\n",
        "#### A\\. **El modelo devuelve una cadena de texto**\n",
        "\n",
        "Aunque le indiques al modelo que genere una respuesta en formato JSON, la salida del modelo sigue siendo una **cadena de texto** (un `string`). Por ejemplo, el modelo podr칤a devolver algo como esto:\n",
        "\n",
        "```\n",
        "'{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web interactivas.\",\n",
        " \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario r치pidamente utilizando\n",
        " scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Esta cadena de texto **no es un objeto JSON** en s칤 misma, sino una representaci칩n textual de un JSON. Para poder trabajar con ella en Python (por ejemplo, acceder a las claves `respuesta` y `explicacion`), necesitas convertirla en un diccionario de Python.\n",
        "\n",
        "#### B\\. **쯈u칠 hace `SimpleJsonOutputParser`?**\n",
        "\n",
        "`SimpleJsonOutputParser` se encarga de:\n",
        "\n",
        "1. **Tomar la cadena de texto** que el modelo ha generado.\n",
        "    \n",
        "2. **Convertirla en un objeto JSON** (por ejemplo, un diccionario o una lista) utilizando `json.loads()` internamente.\n",
        "    \n",
        "3. **Devolver el objeto JSON** para que puedas manipularlo f치cilmente en Python.\n",
        "\n",
        "En otras palabras, **`SimpleJsonOutputParser` convierte la cadena JSON en un diccionario de Python**.\n",
        "\n",
        "\n",
        "#### C\\. **쯇or qu칠 no se puede usar directamente la salida del modelo?**\n",
        "\n",
        "Ya esta dicho. Si no usas `SimpleJsonOutputParser` (o algo similar), la salida del modelo ser칤a una cadena de texto, no un diccionario. Por ejemplo:\n",
        "\n",
        "```\n",
        "output \\= '{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web\n",
        "interactivas.\", \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario r치pidamente\n",
        "utilizando scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Si intentas acceder a `output[\"respuesta\"]`, obtendr치s un error, porque `output` **es una cadena, no un diccionario**. Necesitas parsearla primero.\n"
      ],
      "metadata": {
        "id": "2GhBgaxECsUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 6: Prompt + LLM + OutputParser + funcion personalizada**\n",
        "---\n",
        "\n",
        "Vamos a construir una cadena que incluya una funcion personalizada que destaque las palabras de mas de 6 letras del resultado.\n"
      ],
      "metadata": {
        "id": "roleoYVnD62_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Definimos el prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un traductor profesional.\"),\n",
        "    (\"human\", \"Traduce el siguiente texto al ingl칠s: {texto}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Funci칩n personalizada para resaltar palabras clave\n",
        "def resaltar_palabras_clave(output):\n",
        "    # Dividimos el texto en palabras\n",
        "    palabras = output.split()\n",
        "    # Resaltamos las palabras clave (en este caso, palabras con m치s de 6 letras)\n",
        "    texto_resaltado = []\n",
        "    for palabra in palabras:\n",
        "        if len(palabra) > 6:\n",
        "            texto_resaltado.append(f\"_{palabra}_\")  # Resaltamos con asteriscos\n",
        "        else:\n",
        "            texto_resaltado.append(palabra)\n",
        "    return \" \".join(texto_resaltado)\n",
        "\n",
        "# Creamos la cadena con LCEL: prompt | llm | funci칩n personalizada\n",
        "chain = prompt_template | llm | StrOutputParser() | resaltar_palabras_clave\n",
        "\n",
        "# 5. Ejecutamos la cadena\n",
        "texto_original = \"La inteligencia artificial est치 transformando la industria tecnol칩gica.\"\n",
        "respuesta = chain.invoke({\"texto\": texto_original})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "nxMOCF-xdJoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StrOutputParser se encarga de extraer el contenido de texto del objeto AIMessage generado por el modelo. Esto elimina la necesidad de acceder manualmente a output.content en la funci칩n personalizada."
      ],
      "metadata": {
        "id": "zHVsqwyqHFZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 7: Cadena con dos LLMs (Generaci칩n y Resumen)**\n",
        "---\n",
        "\n",
        "Vamos a crear una cadena donde un LLM genera contenido y otro LLM resumen ese contenido."
      ],
      "metadata": {
        "id": "BXmI1zF0VA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primer LLM: Genera contenido\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un art칤culo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# Segundo LLM: Resumen del contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente art칤culo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = generate_prompt | llm1 | summarize_prompt | llm2\n",
        "\n",
        "# Ejecutar la cadena\n",
        "salida = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "display(Markdown(salida.content))"
      ],
      "metadata": {
        "id": "ONG7qg2UVYwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reto: Intenta que se visualizen ambos textos. El articulo completo y el resultado"
      ],
      "metadata": {
        "id": "rZxLlvo3W0MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crea aqui tu solucio칩n"
      ],
      "metadata": {
        "id": "pe2fA0J_XGHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soluci칩n"
      ],
      "metadata": {
        "id": "zIx2KlznXNWf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-Fo4paIQEYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 8: Integraci칩n con API Externa (Wikipedia)**\n",
        "---\n",
        "\n",
        "Podemos alimentar al modelo con el resultado de una llamada a una API externa.  \n",
        "Este ejemplo creamos una cadena que usa la API de Wikipedia para obtener un resumen de un art칤culo utilizando ChatPromptTemplate.\n",
        "\n",
        "Observa que en este caso la llamada a la API de Wikipedia no esta integrada en la cadena. Esto no del del todo correcto, lo hacemos asi para mantener esta vez el codigo m치s sencillo.  \n",
        "Ademas LangChain proporciona conexiones con diferentes fuentes de datos (entre ellos Wikipedia) y trataremos esto mas adelante."
      ],
      "metadata": {
        "id": "YQFIIKCx3ahf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Funci칩n para obtener datos de la API de Wikipedia\n",
        "def fetch_wikipedia_summary(titulo_articulo):\n",
        "    url = f\"https://es.wikipedia.org/api/rest_v1/page/summary/{titulo_articulo}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"extract\", \"No se encontr칩 un resumen para este art칤culo.\")\n",
        "    else:\n",
        "        return \"Error al obtener el art칤culo de Wikipedia.\"\n",
        "\n",
        "# LLM: Genera un resumen basado en los datos de Wikipedia\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Prompt para generar un resumen\n",
        "resumen_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Basado en la siguiente informaci칩n de Wikipedia, escribe un resumen conciso en forma de vi침etas o bullets points de entre 3 y 5 frases sobre:\\n{info}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = resumen_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Obtener el resumen de un art칤culo de Wikipedia\n",
        "articulo = \"LLM\"  # Cambia esto por el t칤tulo del art칤culo que desees\n",
        "wikipedia_info = fetch_wikipedia_summary(articulo)\n",
        "\n",
        "# Ejecutar la cadena\n",
        "resumen = chain.invoke({\"info\": wikipedia_info})\n",
        "display(Markdown(resumen))"
      ],
      "metadata": {
        "id": "8uMPCxXq38iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 9: Procesamiento de Contenido de URL**\n",
        "---\n",
        "\n",
        "En este ejemplo, obtenemos la informaci칩n de Wikipedia (pero podria ser cualquier URL) sin usar su API. En su lugar accedemos al contenido de la pagina con Requests y se lo pasamos al modelo.\n",
        "En resultado lo mostramos en pantalla y lo guardamos en un fichero."
      ],
      "metadata": {
        "id": "9walLFBS5iss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Funci칩n para obtener el contenido de una URL\n",
        "def fetch_url_content(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "# LLM: Procesa el contenido de la URL\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    template=\"Analiza el siguiente contenido y proporciona una s칤ntesis:\\n{content}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = fetch_url_content | prompt_template | llm | StrOutputParser()\n",
        "\n",
        "# Ejecutar la cadena y guardar el resultado\n",
        "resumen = chain.invoke(\"https://es.wikipedia.org/wiki/Python\")\n",
        "with open(\"resumen.txt\", \"w\") as file:\n",
        "    file.write(resumen)\n",
        "print(\"Resumen guardado en resumen.txt\")\n",
        "display(Markdown(resumen))"
      ],
      "metadata": {
        "id": "cwq1N7yJ5u60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 10: ???**\n",
        "\n",
        "No hay ejemplo 10. Planteate este reto:\n",
        "\n",
        "XXXXXXXX\n"
      ],
      "metadata": {
        "id": "E31PB1arCy9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe aqui la soluci칩n"
      ],
      "metadata": {
        "id": "4JAUlArwDBFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soluci칩n"
      ],
      "metadata": {
        "id": "uJBxTT9zIdoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AQui va la soluci칩n"
      ],
      "metadata": {
        "id": "-zYdgpTnqUjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente tema / cuaderno:  \n",
        "\n",
        "Curso de Langchain en Github: https://github.com/juanfranbrv/curso-langchain\n",
        "\n",
        "Enlaces de interes sobre cadenas LCEL:\n",
        "\n",
        "1. https://python.langchain.com/docs/concepts/lcel/\n",
        "1. https://www.paradigmadigital.com/dev/uso-de-cadenas-langchain-gen-ai/\n",
        "\n",
        "2. https://python.langchain.com/v0.1/docs/modules/chains/\n",
        "\n",
        "3. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "4. https://medium.com/@itsmybestview/unbridling-the-power-of-langchain-framework-with-lcel-9e5f7bf8af74\n",
        "\n",
        "5. https://www.youtube.com/playlist?list=PLGPnu4k-KSC1s7apArPZz1AavDJ4Mvitd\n",
        "\n",
        "6. https://www.youtube.com/watch?v=LzxSY7197ns&t=12s\n",
        "\n",
        "7. https://www.youtube.com/watch?v=H8DrR2pXbww&list=PLilZ1IiRt0R26etAQeQ9xa5h5UzK18zie&index=3\n",
        "\n",
        "8. https://www.youtube.com/watch?v=8BV9TW490nQ&t=523s\n",
        "\n",
        "9. https://www.youtube.com/watch?v=8BV9TW490nQ\n"
      ],
      "metadata": {
        "id": "WD4uu7CIIpTA"
      }
    }
  ]
}