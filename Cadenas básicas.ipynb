{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYJtIu7+RkGtoyIS00eYnb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadenas%20b%C3%A1sicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Cadenas en Langchain**\n",
        "\n",
        "Las cadenas en LangChain representan una secuencia de operaciones que procesan datos de entrada a través de uno o más modelos de lenguaje. Cada operación de una cadena puede considerarse como un paso de un flujo de trabajo, donde el resultado de un paso sirve como entrada para el siguiente. Estas cadenas pueden ser simples o complejas, según los requisitos del proyecto.\n",
        "\n",
        "Hay varios tipos de cadenas en LangChain:\n",
        "\n",
        "- Cadenas secuenciales : una disposición lineal de tareas donde el resultado de una tarea alimenta directamente a la siguiente.\n",
        "- Cadenas paralelas : tareas que pueden realizarse simultáneamente, lo que permite un procesamiento más rápido.\n",
        "- Cadenas condicionales : cadenas que ejecutan diferentes rutas en función de la entrada proporcionada."
      ],
      "metadata": {
        "id": "0WdsL3-kpSC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nuevas cadenas con LangChain Expression Language (LCEL)**\n",
        "---\n",
        "\n",
        "En octubre de 2023, LangChain introdujo una nueva forma de trabajar con cadenas, más eficiente y simple, utilizando el operador | (pipe).  \n",
        "\n",
        "Esto permite encadenar elementos de manera intuitiva, con soporte para stream, batch y operaciones asíncronas.  \n",
        "\n",
        "Además, con LCEL (LangChain Expression Language), las cadenas se pueden definir de forma declarativa y funcional, eliminando la necesidad de clases complejas.   \n",
        "\n",
        "Aunque las cadenas \"antiguas\" siguen funcionando, es recomendable adoptar esta nueva sintaxis.\n",
        "\n",
        "### Ventajas del nuevo sistema:\n",
        "\n",
        "- **Alineación con el framework:** La nueva sintaxis es el estándar recomendado para futuros proyectos.\n",
        "- **Interface unificada:** Simplifica la integración de múltiples métodos.\n",
        "- **Facilidad de composición:** Permite estructurar cadenas de forma secuencial, paralela o con fallbacks.\n",
        "- **Código más limpio:** Menos líneas para lograr los mismos resultados.\n",
        "\n",
        "En LCEL **(LangChain Expression Language**), aunque no hay tipos de cadenas predefinidos como \"secuenciales\", \"condicionales\" o \"paralelas\", puedes implementar estos patrones de flujo de trabajo utilizando la sintaxis de LCEL y algunas funciones adicionales.\n",
        "\n",
        "En este cuaderno nos vamos a ocupar solo del patron secuencial. En proximos cuadernos veremos el resto de patrones de diseño."
      ],
      "metadata": {
        "id": "5RmTZhkweukJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuración del entorno del cuaderno**\n",
        "---\n",
        "\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).  \n",
        "1. Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "2. Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "3. Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "_jVrgFCnxbte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# Importamos las clases necesarias para trabajar con cadenas\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "0kpJPccSxabU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003d1da0-0c2f-4f63-b7b1-0e1d6cfcb40d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo 1: Prompt + LLM**\n",
        "\n",
        "Posiblemenete esta es la cadena mas simple que podamos crear\n",
        "En este caso la cadena conecta un prompt con un modelo.\n",
        "\n",
        "Se usa | para conectar el prompt con el modelo.\n",
        "Luego la cadena se invoca pasandole el PrompTemplate.\n",
        "\n",
        "Observese que esto es basicamente lo que hemos venido haciendo hasta ahora sin cadenas."
      ],
      "metadata": {
        "id": "lDXFodi33rZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos los componentes\n",
        "prompt = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "\n",
        "# Usamos LCEL para crear la cadena\n",
        "chain = prompt | llm\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "# El input del metodo .invoke de una cadena debe ser un unico\n",
        "# asi que puede hacerse asi,\n",
        "#     respuesta = chain.invoke(\"Hola, ¿cómo estás?\")\n",
        "# pero si hay mas de un parametro se espera un diccionario\n",
        "\n",
        "respuesta = chain.invoke({\"idioma\":\"Francés\", \"texto\":\"Hola, ¿cómo estás?\"})\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "m-UHdYGEWaO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa que esto es lo que hemos hecho hasta ahora sin cadenas... invocar un LLM pasandole un PromptTemplate formateado...\n",
        "\n",
        "Aunque no hay gran diferencia y este enfoque puede ser más simple para tareas sencillas, carece de la flexibilidad y la extensibilidad que ofrecen las cadenas como se observa en cuanto la complejidad escala.\n",
        "\n",
        "Ademas LCEL y la nuevas cadenas proporcionan un nivel de abstracción mayor tanto del prompt como del proceso en general.\n"
      ],
      "metadata": {
        "id": "laEFQWsLRJbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Creamos el prompt y lo formateamos\n",
        "prompt_template = PromptTemplate(template=\"Traduce esto al {idioma}: {texto}\", input_variables=[\"idioma\",\"texto\"])\n",
        "prompt_formateado = prompt_template.format(idioma=\"Francés\", texto=\"Hola, ¿cómo estás?\")\n",
        "\n",
        "# Invocamos el LLM directamente\n",
        "respuesta = llm.invoke(prompt_formateado)\n",
        "\n",
        "display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "3OddOArKRXD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo 2: Prompt + LLM + funcion personalizada de transformación de la salida**\n",
        "\n",
        "En este caso la salida del LLM se pocesa con una funcion sencilla para pasarla a mayusculas. (pero puede ser tan complicada como se necesite)\n",
        "\n",
        "Observa que la funcion de transformacion recibe la salida del LLM que es un objeto AIMessage. Asi que debe acceder al .content de este (que es realmente el string con la respuesta) para poder operar.\n",
        "\n",
        "Vamos a usar en adelante display de IPython (en lugar de print) para imprimir el markdown resultante y que se vea menjor en pantalla"
      ],
      "metadata": {
        "id": "GZyeh8OxXrDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# Recuerda que esto es quivalente, con el constructor de clase\n",
        "# prompt_template = PromptTemplate(\n",
        "#     template=\"Responde como si fueras un experto en {tema}: {pregunta}\",\n",
        "#     input_variables=[\"tema\", \"pregunta\"]\n",
        "# )\n",
        "\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos una función de transformación\n",
        "def transformar_texto(output):\n",
        "    return output.content.upper()\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | transformar_texto\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "mJR5vHrfX31N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 3: Prompt + LLM + OutputParser + funcion personalizada de transformación de la salida**\n",
        "\n",
        "Mejoremos un poco el codigo y observemos la modularidad de la cadenas, reutilizando facilmnente la misma cadena en un bucle de preguntas.\n",
        "\n",
        "Vamos a hacer uso de un OutputParser. Mas adelante los explicaremos en detalle.\n",
        "\n",
        "En LangChain, los **parsers de salida** son componentes que se encargan de **transformar y estructurar la salida** generada por un modelo de lenguaje (LLM) o una cadena (chain). Su función principal es asegurarse que la salida esté en un formato específico y utilizable, como un string, un objeto JSON, una lista, etc.  \n",
        "\n",
        "#### **Tipos comunes de parsers de salida**\n",
        "\n",
        "- **`StrOutputParser`**: Convierte la salida en un string. (Usaremos este)\n",
        "    \n",
        "- **`JsonOutputParser`**: Convierte la salida en un objeto JSON.\n",
        "    \n",
        "- **`CommaSeparatedListOutputParser`**: Convierte la salida en una lista separada por comas.\n",
        "    \n",
        "- **`StructuredOutputParser`**: Permite definir un esquema personalizado para la salida."
      ],
      "metadata": {
        "id": "qxQbWY-kdB7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Definimos el PromptTemplate\n",
        "prompt_template = PromptTemplate.from_template(\"Responde como si fueras un experto en {tema}: {pregunta}\")\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Definimos un OutputParser\n",
        "# StrOutputParser convierte la salida del LLM en un string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 4. Definimos una función de transformación\n",
        "# Esta función toma la salida del OutPutParser (str) y la convierte a mayúsculas.\n",
        "def transformar_texto(output):\n",
        "    return output.upper()\n",
        "\n",
        "# 5. Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | transformar_texto\n",
        "\n",
        "# 6. Ejecutamos la cadena con diferentes entradas en un bucle\n",
        "# Observa cómo la cadena puede reutilizarse para diferentes preguntas.\n",
        "preguntas = [\n",
        "    {\"tema\": \"Machine Learning\", \"pregunta\": \"¿Qué es el overfitting?\"},\n",
        "    {\"tema\": \"Historia\", \"pregunta\": \"¿Quién fue Napoleón Bonaparte?\"},\n",
        "    {\"tema\": \"Programación\", \"pregunta\": \"¿Qué es Python?\"}\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = chain.invoke(pregunta)\n",
        "    display(Markdown(f\"**Pregunta:** {pregunta['pregunta']}\"))\n",
        "    display(Markdown(f\"**Respuesta:** {respuesta}\"))\n",
        "    print(\"---\")  # Separador entre preguntas"
      ],
      "metadata": {
        "id": "IuwH1XTXdBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 4: Prompt + LLM + OutputParser + Guardar en fichero**\n",
        "\n",
        "Esta vez, para una mayor correcion, aunque no es necesario, vamos a usar ChatPromptTemplate"
      ],
      "metadata": {
        "id": "auY_uQJshX6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el PromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo. Proporcionas las listas que se te piden y solo la lista sin numerar y en una columna. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"Crea una lista de {numero_elementos} {tipo_elementos}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Definimos un OutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Definimos una función para guardar en archivo\n",
        "def guardar_en_archivo(output):\n",
        "    with open(\"respuesta.txt\", \"w\") as f:\n",
        "        f.write(output)\n",
        "        print(output)\n",
        "    return \"Respuesta guardada en 'respuesta.txt'\"\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | output_parser | guardar_en_archivo\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "respuesta = chain.invoke({\"numero_elementos\": 10, \"tipo_elementos\": \"Adbervios de modo que no acaben en -mente\"})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "sntxjr0xh4kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + JsonOutputParser\n",
        "\n",
        "De nuevo una cadena de 3 elementos\n",
        "En este caso, usamos un OutputParser que crearemos nosotros mediante una funcion personalizada para estructurar la salida del LLM en un formato específico, como un diccionario.\n",
        "\n"
      ],
      "metadata": {
        "id": "MMLf1KxWrjcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente conciso y directo experto en {tema}. Devuelve el resultado en formato JSON con las claves 'respuesta' y 'explicacion'. No añadas ningun tipo de comentario\"),\n",
        "    (\"human\", \"{pregunta}\"),\n",
        "])\n",
        "\n",
        "# Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "\n",
        "# Creamos la cadena con LCEL\n",
        "chain = prompt_template | llm | json_parser\n",
        "\n",
        "# Ejecutamos la cadena\n",
        "result = chain.invoke({\"tema\": \"Python\", \"pregunta\": \"¿Qué es el Streamlit?\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "25m9G__Gr-Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observa: ¿ si el modelo ya da la respuesta en JSON (se le ha indicado en el prompt) porque se se utiliza json_parser = SimpleJsonOutputParser() para contruir la salida ?**\n",
        "\n",
        "\n",
        "La respuesta tiene que ver con cómo funcionan los modelos de lenguaje y cómo se manejan sus salidas en LangChain. Vamos a desglosarlo:\n",
        "\n",
        "---\n",
        "\n",
        "#### 1\\. **El modelo devuelve una cadena de texto**\n",
        "\n",
        "Aunque le indiques al modelo que genere una respuesta en formato JSON, la salida del modelo sigue siendo una **cadena de texto** (un `string`). Por ejemplo, el modelo podría devolver algo como esto:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "'{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web interactivas.\",\n",
        " \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente utilizando\n",
        " scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Esta cadena de texto **no es un objeto JSON** en sí misma, sino una representación textual de un JSON. Para poder trabajar con ella en Python (por ejemplo, acceder a las claves `respuesta` y `explicacion`), necesitas convertirla en un diccionario de Python.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2\\. **¿Qué hace `SimpleJsonOutputParser`?**\n",
        "\n",
        "`SimpleJsonOutputParser` se encarga de:\n",
        "\n",
        "1. **Tomar la cadena de texto** que el modelo ha generado.\n",
        "    \n",
        "2. **Convertirla en un objeto JSON** (por ejemplo, un diccionario o una lista) utilizando `json.loads()` internamente.\n",
        "    \n",
        "3. **Devolver el objeto JSON** para que puedas manipularlo fácilmente en Python.\n",
        "\n",
        "En otras palabras, **`SimpleJsonOutputParser` convierte la cadena JSON en un diccionario de Python**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3\\. **¿Por qué no se puede usar directamente la salida del modelo?**\n",
        "\n",
        "Ya esta dicho. Si no usas `SimpleJsonOutputParser` (o algo similar), la salida del modelo sería una cadena de texto, no un diccionario. Por ejemplo:\n",
        "\n",
        "```\n",
        "output \\= '{\"respuesta\": \"Streamlit es una biblioteca de Python para crear aplicaciones web\n",
        "interactivas.\", \"explicacion\": \"Permite a los desarrolladores crear interfaces de usuario rápidamente\n",
        "utilizando scripts de Python.\"}'\n",
        "```\n",
        "\n",
        "Si intentas acceder a `output[\"respuesta\"]`, obtendrás un error, porque `output` **es una cadena, no un diccionario**. Necesitas parsearla primero.\n"
      ],
      "metadata": {
        "id": "2GhBgaxECsUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 5: Prompt + LLM + OutputParser + funcion personalizada\n",
        "\n",
        "Vamos a construir una cadena que incluya una funcion personalizada que destaque las palabras de mas de 6 letras del resultado.\n"
      ],
      "metadata": {
        "id": "roleoYVnD62_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# 1. Definimos el prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un traductor profesional.\"),\n",
        "    (\"human\", \"Traduce el siguiente texto al inglés: {texto}\"),\n",
        "])\n",
        "\n",
        "# 2. Definimos el modelo de lenguaje\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# 3. Función personalizada para resaltar palabras clave\n",
        "def resaltar_palabras_clave(output):\n",
        "    # Dividimos el texto en palabras\n",
        "    palabras = output.split()\n",
        "    # Resaltamos las palabras clave (en este caso, palabras con más de 6 letras)\n",
        "    texto_resaltado = []\n",
        "    for palabra in palabras:\n",
        "        if len(palabra) > 6:\n",
        "            texto_resaltado.append(f\"_{palabra}_\")  # Resaltamos con asteriscos\n",
        "        else:\n",
        "            texto_resaltado.append(palabra)\n",
        "    return \" \".join(texto_resaltado)\n",
        "\n",
        "# 4. Creamos la cadena con LCEL: prompt | llm | función personalizada\n",
        "chain = prompt_template | llm | StrOutputParser() | resaltar_palabras_clave\n",
        "\n",
        "# 5. Ejecutamos la cadena\n",
        "texto_original = \"La inteligencia artificial está transformando la industria tecnológica.\"\n",
        "respuesta = chain.invoke({\"texto\": texto_original})\n",
        "display(Markdown(respuesta))"
      ],
      "metadata": {
        "id": "nxMOCF-xdJoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "15e8f663-9caf-4c0a-80c3-920cd83eff53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "_Artificial_ _intelligence_ is _transforming_ the _technology_ _industry._"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StrOutputParser se encarga de extraer el contenido de texto del objeto AIMessage generado por el modelo. Esto elimina la necesidad de acceder manualmente a output.content en la función personalizada."
      ],
      "metadata": {
        "id": "zHVsqwyqHFZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo 6: Cadena con dos LLMs (Generación y Resumen)**\n",
        "\n",
        "Vamos a crear una cadena donde un LLM genera contenido y otro LLM resumen ese contenido."
      ],
      "metadata": {
        "id": "BXmI1zF0VA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primera LLM: Genera contenido\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Escribe un artículo detallado sobre {topic}.\"\n",
        ")\n",
        "\n",
        "# Segunda LLM: Resumen del contenido generado\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7)\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Resumen el siguiente artículo:\\n{article}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = generate_prompt | llm1 | summarize_prompt | llm2\n",
        "\n",
        "# Ejecutar la cadena\n",
        "salida = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "display(Markdown(salida.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ONG7qg2UVYwp",
        "outputId": "00b5502e-af7e-4e8c-9ce3-d4004b34ac44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Resumen de la Inteligencia Artificial: Transformando el Futuro**\n\nLa Inteligencia Artificial (IA) ha evolucionado desde un concepto teórico a una herramienta práctica que transforma diversos aspectos de la vida. Desde su inicio en 1956, la IA ha pasado por fases de optimismo y desilusión. Recientemente, los avances en aprendizaje automático y procesamiento de datos masivos han revivido el interés en la IA.\n\n**Tipos de IA:**\n* IA Débil (Narrow AI): Realiza tareas específicas (por ejemplo, asistentes virtuales, sistemas de recomendación).\n* IA Fuerte (General AI): Realiza cualquier tarea cognitiva que un humano puede hacer (todavía en desarrollo).\n\n**Aplicaciones de la IA:**\n* Salud: Diagnósticos asistidos, personalización de tratamientos.\n* Movilidad y Transporte: Vehículos autónomos.\n* Finanzas: Detección de fraudes, predicción de tendencias del mercado.\n* Marketing y Publicidad: Campañas personalizadas, segmentación de público.\n* Agricultura: Optimización de la producción, monitoreo de cultivos.\n\n**Desafíos y Ética:**\n* Ética: Los sesgos en los datos de entrenamiento pueden conducir a decisiones injustas.\n* Privacidad: La recopilación masiva de datos puede infringir la privacidad.\n* Seguridad: La dependencia de la IA en sistemas críticos plantea riesgos de ataques cibernéticos.\n\n**El Futuro de la IA:**\n* Integración continua en todos los sectores, mejorando la eficiencia y la productividad.\n* Enfoque cuidadoso en la regulación y la gobernanza para garantizar un desarrollo ético.\n* Colaboraciones y educación para preparar a las generaciones futuras para un mundo impulsado por la IA.\n\nLa IA ofrece un gran potencial para resolver problemas y mejorar la calidad de vida, pero también plantea desafíos que deben abordarse para garantizar su desarrollo beneficioso."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reto: Intenta que se visualizen ambos textos. El articulo completo y el resultado"
      ],
      "metadata": {
        "id": "rZxLlvo3W0MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crea aqui tu solucioón"
      ],
      "metadata": {
        "id": "pe2fA0J_XGHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución"
      ],
      "metadata": {
        "id": "zIx2KlznXNWf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-Fo4paIQEYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZcXfofoXSI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 7: Integración con API Externa (Wikipedia)**\n",
        "\n",
        "Podemos alimentar al modelo con el resultado de una llamada a una API externa.  \n",
        "Este ejemplo creamos una cadena que usa la API de Wikipedia para obtener un resumen de un artículo utilizando ChatPromptTemplate."
      ],
      "metadata": {
        "id": "YQFIIKCx3ahf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Función para obtener datos de la API de Wikipedia\n",
        "def fetch_wikipedia_summary(title):\n",
        "    url = f\"https://es.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"extract\", \"No se encontró un resumen para este artículo.\")\n",
        "    else:\n",
        "        return \"Error al obtener el artículo de Wikipedia.\"\n",
        "\n",
        "# LLM: Genera un resumen basado en los datos de Wikipedia\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "# Prompt para generar un resumen\n",
        "summary_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Basado en la siguiente información de Wikipedia, escribe un resumen conciso en forma de viñetas o bullets points de entre 3 y 5 frases sobre:\\n{info}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Obtener el resumen de un artículo de Wikipedia\n",
        "article_title = \"LLM\"  # Cambia esto por el título del artículo que desees\n",
        "wikipedia_info = fetch_wikipedia_summary(article_title)\n",
        "\n",
        "# Ejecutar la cadena\n",
        "resumen = chain.invoke({\"info\": wikipedia_info})\n",
        "display(Markdown(resumen))"
      ],
      "metadata": {
        "id": "8uMPCxXq38iF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "a319b6cb-fe2a-43b8-f4b6-621b06536351"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Un modelo extenso de lenguaje (LLM) es una red neuronal con numerosos parámetros, entrenada en grandes volúmenes de texto sin etiquetar mediante aprendizaje autosupervisado o semisupervisado.\n- Los LLMs emergieron alrededor de 2018 y han demostrado un rendimiento destacado en una amplia gama de tareas de procesamiento del lenguaje natural.\n- Su desarrollo ha transformado la investigación en el campo, alejándose de modelos supervisados especializados hacia enfoques más generales y versátiles."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 8: Procesamiento de Contenido de URL**\n",
        "\n",
        "En este ejemplo, obtenemos la información de Wikipedia (pero podria ser cualquier URL) sin usar su API. En su lugar accedemos al contenido de la pagina con Requests y se lo pasamos al modelo.\n",
        "En resultado lo mostramos en pantalla y lo guardamos en un fichero."
      ],
      "metadata": {
        "id": "9walLFBS5iss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Función para obtener el contenido de una URL\n",
        "def fetch_url_content(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "# LLM: Procesa el contenido de la URL\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)\n",
        "\n",
        "process_prompt = PromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    template=\"Analiza el siguiente contenido y proporciona una síntesis:\\n{content}\"\n",
        ")\n",
        "\n",
        "# Cadena completa usando LCEL\n",
        "chain = fetch_url_content | process_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Ejecutar la cadena y guardar el resultado\n",
        "synthesis = chain.invoke(\"https://es.wikipedia.org/wiki/Python\")\n",
        "with open(\"synthesis.txt\", \"w\") as file:\n",
        "    file.write(synthesis)\n",
        "print(\"Síntesis guardada en synthesis.txt\")\n",
        "display(Markdown(synthesis))"
      ],
      "metadata": {
        "id": "cwq1N7yJ5u60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "outputId": "72731db7-07e0-448b-8670-59e443f54360"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Síntesis guardada en synthesis.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "El contenido proporcionado es una página de Wikipedia dedicada al lenguaje de programación Python. Aquí hay una síntesis de los puntos más relevantes:\n\n### Python: Resumen\n- **Descripción**: Python es un lenguaje de programación de alto nivel, interpretado y multiparadigma, diseñado para ser fácil de leer y escribir, con un fuerte énfasis en la legibilidad del código.\n- **Desarrollo**: Creado a finales de los años 80 por Guido van Rossum, Python se lanzó oficialmente el 20 de febrero de 1991. Su nombre se inspira en el grupo de comedia británico \"Monty Python\".\n- **Características**: \n  - Soporta múltiples paradigmas de programación, incluyendo la programación orientada a objetos, la programación imperativa y la programación funcional.\n  - Es conocido por su tipado dinámico y su sistema de gestión de memoria basado en conteo de referencias.\n  - Python utiliza indentación para delimitar bloques de código, lo que fomenta un estilo de codificación limpio y estructurado.\n- **Uso**: Ampliamente utilizado en diversos campos como el desarrollo web, análisis de datos, inteligencia artificial, y automatización de tareas.\n- **Biblioteca estándar**: Python viene con una extensa biblioteca estándar que incluye módulos para tareas comunes, lo que permite a los programadores implementar soluciones sin necesidad de escribir mucho código desde cero.\n- **Implementaciones**: Existen varias implementaciones de Python, siendo CPython la más común. Otras incluyen Jython, IronPython y PyPy.\n- **Versiones**: Python 2.7 fue oficialmente descontinuado en 2020, y la comunidad se ha centrado en el desarrollo de Python 3.x, que incluye mejoras significativas y nuevas características.\n\n### Filosofía y Comunidad\n- **Zen de Python**: Python tiene una filosofía que resalta principios como \"la legibilidad cuenta\" y \"simple es mejor que complejo\", encapsulados en el \"Zen de Python\", una lista de guías para escribir código Python.\n- **Comunidad**: Python cuenta con una sólida comunidad de desarrolladores que contribuyen al lenguaje y sus bibliotecas, asegurando un crecimiento constante y una amplia adopción en la industria.\n\n### Conclusión\nPython es un lenguaje de programación versátil y accesible, ideal tanto para principiantes como para desarrolladores experimentados, y se ha consolidado como uno de los lenguajes más populares en el mundo de la programación."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 9: Retroalimentacion**\n",
        "\n",
        "La salida de un llm (por ejemplo un texto extenso) se intruce en otro y se le pide qu elo mejore o lo cambien de alguna forma\n"
      ],
      "metadata": {
        "id": "E31PB1arCy9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los prompts y el LLM\n",
        "prompt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Escribe un resumen sobre {topic}.\")\n",
        "prompt2 = PromptTemplate(input_variables=[\"summary\"], template=\"Mejora el siguiente texto:\\n{summary}\")\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "# Crear la cadena con retroalimentación\n",
        "chain = prompt1 | llm | prompt2 | llm\n",
        "\n",
        "# Ejecutar la cadena\n",
        "result = chain.invoke({\"topic\": \"inteligencia artificial\"})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4JAUlArwDBFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejemplo 10: ??**"
      ],
      "metadata": {
        "id": "Sdra6NqtqTxq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zYdgpTnqUjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despedida y cierre. Enlace al siguinete Colab. Enlace al curso en Github"
      ],
      "metadata": {
        "id": "O70y5d5pqZOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs:\n",
        "\n",
        "1. https://www.paradigmadigital.com/dev/uso-de-cadenas-langchain-gen-ai/\n",
        "\n",
        "2. https://python.langchain.com/v0.1/docs/modules/chains/\n",
        "\n",
        "3. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "4. https://medium.com/@itsmybestview/unbridling-the-power-of-langchain-framework-with-lcel-9e5f7bf8af74\n",
        "\n",
        "5. https://www.youtube.com/playlist?list=PLGPnu4k-KSC1s7apArPZz1AavDJ4Mvitd\n",
        "\n",
        "6. https://www.youtube.com/watch?v=LzxSY7197ns&t=12s\n",
        "\n",
        "7. https://www.youtube.com/watch?v=H8DrR2pXbww&list=PLilZ1IiRt0R26etAQeQ9xa5h5UzK18zie&index=3\n",
        "\n",
        "8. https://www.youtube.com/watch?v=8BV9TW490nQ&t=523s\n",
        "\n",
        "9. https://www.youtube.com/watch?v=8BV9TW490nQ\n",
        "\n",
        "\n",
        "Usar esto para localizar\n",
        "https://www.youtube.com/results?search_query=langchain+lcel+\n"
      ],
      "metadata": {
        "id": "0AN8q5pGWadx"
      }
    }
  ]
}