{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDZoMJY5kgfIRaa3Kwr3js",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Cadenas_(Chains)_B%C3%A1sicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuevas cadenas con LangChain Expression Language (LCEL)\n",
        "\n",
        "En octubre del 2023 se realizó un cambio en LangChain que modifica sustancialmente el modo en el que se usan las cadenas. Además de cambiar la forma en la que se escribe el código, estas nuevas cadenas son más eficientes, permiten ser usadas en stream o batch incluso de manera asíncrona.\n",
        "\n",
        "Estas nuevas cadenas se reconocen rápidamente al usar el operador | para encadenar elementos dentro de la cadena, un ejemplo de cadena muy sencillo en el que lanzamos un prompt a un LLM sería así:\n",
        "\n",
        "```none\n",
        "chain = prompt | llm\n",
        "```\n",
        "\n",
        "Aunque las “viejas” cadenas continúan funcionando, se recomienda el uso de esta nueva sintaxis para los proyectos nuevos. Estos son los principales motivos:\n",
        "\n",
        "1. Porque ahora es como trabaja el framework.\n",
        "2. Una _interface_ unificada para varios métodos (Stream, batch, asíncronas).\n",
        "3. Facilidad de composición (Secuencial, paralelo, añadir fallbacks...).\n",
        "4. Código mucho más simple (Menos líneas para el mismo resultado)."
      ],
      "metadata": {
        "id": "5RmTZhkweukJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del entorno del cuaderno\n",
        "---\n",
        "\n",
        "Configuramos el entorno de trabajo para utilizar LangChain con distintos modelos de lenguaje (LLMs).  \n",
        "1. Obtenemos las claves API para acceder a los servicios de OpenAI, Groq, Google y Hugging Face.\n",
        "\n",
        "2. Instalamos la librería LangChain y las integraciones necesarias para cada uno de estos proveedores.\n",
        "\n",
        "3. Importamos las clases específicas de LangChain que permiten crear plantillas de prompts e interactuar con los diferentes modelos de lenguaje, dejándolo todo listo para empezar a desarrollar aplicaciones basadas en LLMs. (Este codigo se explico con detalle en el primer cuaderno)\n",
        "\n",
        "Comenta (#) las librerias y modelos que no desees usar."
      ],
      "metadata": {
        "id": "_jVrgFCnxbte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "0kpJPccSxabU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-UHdYGEWaO9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar el modelo\n",
        "llm1 = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "llm2 = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)"
      ],
      "metadata": {
        "id": "iMp-Ec7qWaIf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las plantillas\n",
        "template_sistema = \"\"\"\\\n",
        "Eres un tutor amigable de {idioma}. Tu trabajo es:\n",
        "1. Mantener una conversación simple con el estudiante\n",
        "2. Corregir errores básicos de gramática y ortografía si los hay\n",
        "3. Ser paciente y motivador\n",
        "\"\"\"\n",
        "\n",
        "template_usuario = \"\"\"\\\n",
        "El estudiante dice: {mensaje_usuario}\n",
        "\n",
        "Por favor, responde de manera natural y si hay algún error gramatical,\n",
        "corrígelo sutilmente en tu respuesta.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QUyxibiHWaE2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(template_sistema),\n",
        "    HumanMessagePromptTemplate.from_template(template_usuario),\n",
        "])\n",
        "\n",
        "# Podria ser asi ???? explicar\n",
        "\n",
        "# Crear el ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template_sistema),\n",
        "    (\"human\", template_usuario),\n",
        "])\n",
        ""
      ],
      "metadata": {
        "id": "-GjKToSSWZ9q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatear_en_idioma(mensaje_usuario, idioma=\"español\"):\n",
        "    \"\"\"\n",
        "    Función simple para procesar mensajes del usuario y obtener respuestas\n",
        "    1. Recibe el mensaje del usuario\n",
        "    2. Lo inyecta en el chatprompttemplate\n",
        "    3. Invoca el modelo con el\n",
        "    4. Devuelve la respuesta\n",
        "    \"\"\"\n",
        "    # Formatear el prompt con el mensaje del usuario y el idioma\n",
        "    prompt = chat_prompt.format(\n",
        "        mensaje_usuario=mensaje_usuario,\n",
        "        idioma=idioma\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del modelo\n",
        "    respuesta = llm.invoke(prompt)\n",
        "\n",
        "    return respuesta.content"
      ],
      "metadata": {
        "id": "fNlBd-RHWZ6H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"¡Bienvenido al Chatbot de Aprendizaje de Idiomas!\")\n",
        "    print(\"Escribe 'salir' para terminar la conversación\")\n",
        "\n",
        "    while True:\n",
        "        # Obtener entrada del usuario\n",
        "        mensaje = input(\"\\nTú: \")\n",
        "\n",
        "        if mensaje.lower() == 'salir':\n",
        "            print(\"¡Hasta luego! Gracias por practicar.\")\n",
        "            break\n",
        "\n",
        "        # Obtener y mostrar la respuesta\n",
        "        respuesta = chatear_en_idioma1(mensaje)\n",
        "        print(\"\\nTutor:\", respuesta)"
      ],
      "metadata": {
        "id": "N6zL1MIuqAn5",
        "outputId": "b29f49c5-9f5a-4a90-dc02-1af49919b02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Bienvenido al Chatbot de Aprendizaje de Idiomas!\n",
            "Escribe 'salir' para terminar la conversación\n",
            "\n",
            "Tú: Ola\n",
            "\n",
            "Tutor: ¡Hola! Me alegro de que estés aquí. ¿Cómo te llamas?\n",
            "\n",
            "Tú: me a mordido un perro\n",
            "\n",
            "Tutor: ¡Ay, lo siento mucho! ¿Estás bien? ¿Te duele mucho?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-215a5458e6d4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Obtener entrada del usuario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmensaje\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTú: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmensaje\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'salir'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs:\n",
        "\n",
        "1. https://www.paradigmadigital.com/dev/uso-de-cadenas-langchain-gen-ai/\n",
        "\n",
        "2. https://python.langchain.com/v0.1/docs/modules/chains/\n",
        "\n",
        "3. https://medium.com/@anuragmishra_27746/practical-hands-on-with-langchain-expression-language-lcel-for-building-langchain-agent-chain-2a9364dc4ca3\n",
        "\n",
        "4. https://medium.com/@itsmybestview/unbridling-the-power-of-langchain-framework-with-lcel-9e5f7bf8af74\n",
        "\n",
        "5. https://www.youtube.com/playlist?list=PLGPnu4k-KSC1s7apArPZz1AavDJ4Mvitd\n",
        "\n",
        "6. https://www.youtube.com/watch?v=LzxSY7197ns&t=12s\n",
        "\n",
        "7. https://www.youtube.com/watch?v=H8DrR2pXbww&list=PLilZ1IiRt0R26etAQeQ9xa5h5UzK18zie&index=3\n",
        "\n",
        "8. https://www.youtube.com/watch?v=8BV9TW490nQ&t=523s\n",
        "\n",
        "9. https://www.youtube.com/watch?v=8BV9TW490nQ\n",
        "\n",
        "\n",
        "Usar esto para localizar\n",
        "https://www.youtube.com/results?search_query=langchain+lcel+\n"
      ],
      "metadata": {
        "id": "0AN8q5pGWadx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxMOCF-xdJoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
