{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/agents_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Preparando el cuaderno**\n",
        "---\n",
        "bla, bla, bla"
      ],
      "metadata": {
        "id": "HwyvZdZVeOOd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DsW4sWtvd7zT"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "\n",
        "# Importar la librer√≠a `userdata` de Google Colab.\n",
        "# Esta librer√≠a se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Importar librer√≠a para subir ficheros\n",
        "from google.colab import files\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')\n",
        "TOGETHER_API_KEY=userdata.get('TOGETHER_API_KEY')\n",
        "ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "DEEPSEEK_API_KEY=userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "# Instalar las librer√≠as necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librer√≠as si ya est√°n instaladas (`-U`).\n",
        "%pip install langchain -qU  # Instalar la librer√≠a principal de LangChain.\n",
        "%pip install langgraph -qU  # Instalar la librer√≠a de grafos de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "%pip install langchain-openai -qU\n",
        "%pip install langchain-groq -qU\n",
        "%pip install langchain-google-genai -qU\n",
        "%pip install langchain-huggingface -qU\n",
        "%pip install langchain_mistralai -qU\n",
        "%pip install langchain-together -qU\n",
        "%pip install langchain-anthropic -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "# # Importamos las clases necesarias para trabajar con cadenas\n",
        "# from langchain.chains import LLMChain\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a trav√©s de LangChain.\n",
        "from langchain_openai import ChatOpenAI  # Esta tambien para DeepSeek\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_together import ChatTogether\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "# Importamos la libreria para formatear mejor la salida\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, TypedDict\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "\n",
        "# 1. Actualizar el estado\n",
        "class TranslationState(TypedDict):\n",
        "    original_text: str\n",
        "    source_language: str\n",
        "    target_language: str\n",
        "    translator_output: str\n",
        "    accuracy_suggestions: List[str]  # Cambiado a espec√≠fico para Revisor 1\n",
        "    fluency_suggestions: List[str]   # Nuevo campo para Revisor 2\n",
        "    editor_output: str\n",
        "    translation_iterations: int\n",
        "\n",
        "# 2. Modificar las plantillas\n",
        "translator_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are a senior lawyer-translator working for an international organisation on Intellectual Property. \"\n",
        "        \"You have more than 10 years of professional experience, specializing in translation from {source_language} to {target_language}. \"\n",
        "        \"With many years of experience in specialized translation, you use the most advanced translation techniques to provide high-quality, fluent, and accurate translations.\"\n",
        "        \"The translation should be ready for publication, and you need to ensure that a mistranslation may result in serious economic and legal consequences for your client.\"\n",
        "        \"Please ensure that the translation is perfect.\"\n",
        "    )),\n",
        "    (\"human\", (\n",
        "        \"Your task is to provide a professional, high-quality translation from {source_language} to {target_language} of the following text:\\n{original_text}. \"\n",
        "        \"The resulting translation should have the highest standards, and should convey all the meaning of the source text in a way that is fluent and appropriate for the target culture.\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "accuracy_reviewer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are an expert Accuracy Reviewer, specializing in reviewing translations from {source_language} to {target_language}. Focus ONLY on: \"\n",
        "        \"- Accuracy (by correcting errors of addition, mistranslation, omission or untranslated text.\\n\"\n",
        "        \"- Terminology (ensuring that the terminology used is appropriate for the context, that the use is coherent and consistent throughout the same document, etc.)\\n\"\n",
        "        \"- Other accuracy errors (proper noun translations, numerical consistency, technical accuracy.\\n\"\n",
        "        \"- Only suggest improvements that enhance the accuracy of the translation. Do not make any preferential changes.\\n\"\n",
        "        \"Return EXACTLY 'NO_SUGGESTIONS' if the accuracy of the translation is perfect and you think that no Accuracy improvements could be done.  \"\n",
        "        \"Format: bullet points with 'ERROR: [issue] ‚Üí SUGGESTION: [fix]'\"\n",
        "    )),\n",
        "    (\"human\", (\n",
        "        \"Source ({source_language}):\\n{original_text}\\n\\n\"\n",
        "        \"Translation ({target_language}):\\n{translator_output}\\n\\n\"\n",
        "        \"Critical accuracy review (ONLY technical/terminology issues):\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "fluency_reviewer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are an expert Fluency Reviewer, specializing in reviewing translations from {source_language} to {target_language}. Focus ONLY on:\\n\"\n",
        "        \"- Fluency (by applying {target_language} grammar, spelling and punctuation rules, and ensuring there are no unnecessary repetitions).\\n\"\n",
        "        \"- Style (by ensuring the translations reflect the style of the source text and takes into account any cultural context).\\n\"\n",
        "        \"- Other fluency errors (natural flow, idiomatic expressions, readability, cultural adaptation).\\n\"\n",
        "        \"- Only suggest improvements that enhance the fluency of the translation. Do not make any preferential changes.\\n\"\n",
        "        \"Return EXACTLY 'NO_SUGGESTIONS' if the fluency of the translation is perfect and you think that no Fluency improvements could be done.\"\n",
        "        \"Format: bullet points with 'ISSUE: [description] ‚Üí IMPROVEMENT: [suggestion]'\"\n",
        "    )),\n",
        "    (\"human\", (\n",
        "        \"Translation to refine:\\n{translator_output}\\n\\n\"\n",
        "        \"Target language: {target_language}\\n\"\n",
        "        \"Fluency and style improvements:\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "editor_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You are a senior editor, having worked for more than 10 years as a lawyer-linguist in an Intellectual Property organisation.\"\n",
        "        \"You specialize in editing translations from {source_language} to {target_language}. \"\n",
        "        \"Your task is to carefully read, and then edit, the translation by the Translator, and edit it\"\n",
        "        \"by taking into account a list of expert suggestions and constructive criticisms by the Reviewers.\"\n",
        "        \"Synthesize inputs from both reviewers. Prioritize accuracy changes, then implement fluency improvements.\"\n",
        "    )),\n",
        "    (\"human\", (\n",
        "        \"Original Text ({source_language}):\\n{original_text}\\n\\n\"\n",
        "        \"Draft Translation:\\n{translator_output}\\n\\n\"\n",
        "        \"Accuracy Issues:\\n{accuracy_suggestions}\\n\\n\"\n",
        "        \"Fluency Suggestions:\\n{fluency_suggestions}\\n\\n\"\n",
        "        \"Produce a final, polished translation that is ready for publication and has the highest-quality standards:\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 3. Configurar modelos (ajustar seg√∫n necesidades)\n",
        "\n",
        "# Con OpenAI\n",
        "\n",
        "translator_llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.3)\n",
        "accuracy_llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.1)  # Modelo m√°s conservador\n",
        "fluency_llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.7)   # Modelo m√°s creativo\n",
        "editor_llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.4)\n",
        "\n",
        "# Con Deepseek\n",
        "#model=deepseek-reasoner para R1\n",
        "# translator_llm = ChatOpenAI(model=\"deepseek-chat\", api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0.3)\n",
        "# accuracy_llm = ChatOpenAI(model=\"deepseek-chat\", api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0.1)  # Modelo m√°s conservador\n",
        "# fluency_llm = ChatOpenAI(model=\"deepseek-chat\", api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0.7)  # Modelo m√°s creativo\n",
        "# editor_llm = ChatOpenAI(model=\"deepseek-chat\", api_key=DEEPSEEK_API_KEY, base_url=\"https://api.deepseek.com\", temperature=0.4)\n",
        "\n",
        "# 4. Crear nuevas cadenas\n",
        "translator_chain = translator_prompt | translator_llm\n",
        "accuracy_review_chain = accuracy_reviewer_prompt | accuracy_llm\n",
        "fluency_review_chain = fluency_reviewer_prompt | fluency_llm\n",
        "editor_chain = editor_prompt | editor_llm\n",
        "\n",
        "# 5. Actualizar funciones de nodo\n",
        "def translate(state: TranslationState) -> dict:\n",
        "    print(\"\\n--- TRANSLATING ---\")\n",
        "    result = translator_chain.invoke(state)\n",
        "    display(Markdown(\"üìÑ Traducci√≥n generada:\\n\" + result.content))\n",
        "    return {\"translator_output\": result.content}\n",
        "\n",
        "def review_accuracy(state: TranslationState) -> dict:\n",
        "    print(\"\\n--- ACCURACY REVIEW ---\")\n",
        "    result = accuracy_review_chain.invoke(state)\n",
        "    content = result.content.strip()\n",
        "\n",
        "    suggestions = []\n",
        "    if content != \"NO_SUGGESTIONS\":\n",
        "        suggestions = [line.strip() for line in content.split(\"\\n\") if line.strip()]\n",
        "        print(\"‚ö†Ô∏è Sugerencias de precisi√≥n:\")\n",
        "        for sug in suggestions:\n",
        "            display(Markdown(f\"    {sug}\"))\n",
        "    else:\n",
        "        print(\"‚úÖ Sin problemas de precisi√≥n\")\n",
        "\n",
        "    return {\"accuracy_suggestions\": suggestions}\n",
        "\n",
        "def review_fluency(state: TranslationState) -> dict:\n",
        "    print(\"\\n--- FLUENCY REVIEW ---\")\n",
        "    result = fluency_review_chain.invoke(state)\n",
        "    content = result.content.strip()\n",
        "\n",
        "    suggestions = []\n",
        "    if content != \"NO_SUGGESTIONS\":\n",
        "        suggestions = [line.strip() for line in content.split(\"\\n\") if line.strip()]\n",
        "        print(\"‚úèÔ∏è Sugerencias de estilo:\")\n",
        "        for sug in suggestions:\n",
        "            display(Markdown(f\"    {sug}\"))\n",
        "    else:\n",
        "        print(\"‚úÖ Sin problemas de fluidez\")\n",
        "\n",
        "    return {\"fluency_suggestions\": suggestions}\n",
        "\n",
        "def edit(state: TranslationState) -> dict:\n",
        "    print(\"\\n--- FINAL EDITING ---\")\n",
        "    result = editor_chain.invoke(state)\n",
        "    print(\"üîñ Versi√≥n final:\")\n",
        "    display(Markdown(result.content))\n",
        "    return {\"editor_output\": result.content}\n",
        "\n",
        "# 6. Reconstruir el grafo\n",
        "builder = StateGraph(TranslationState)\n",
        "\n",
        "builder.add_node(\"translate\", translate)\n",
        "builder.add_node(\"review_accuracy\", review_accuracy)\n",
        "builder.add_node(\"review_fluency\", review_fluency)\n",
        "builder.add_node(\"edit\", edit)\n",
        "\n",
        "builder.set_entry_point(\"translate\")\n",
        "\n",
        "# Flujo principal CORREGIDO\n",
        "builder.add_edge(\"translate\", \"review_accuracy\")\n",
        "builder.add_edge(\"review_accuracy\", \"review_fluency\")\n",
        "builder.add_edge(\"review_fluency\", \"edit\")\n",
        "builder.add_edge(\"edit\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "# # 8. Ejecutar\n",
        "# inputs = {\n",
        "#     \"original_text\": \"Los Angeles firefighters were making modest progress in taming the region‚Äôs two largest fires on Saturday as they raced to suppress them ahead of high winds that were expected to intensify later in the day. After a night of expanded evacuation orders and spreading flames that continued to plunge the area into what Lindsey Horvath, a Los Angeles County supervisor, called 'unimaginable terror and heartbreak,' crews had contained 11 percent of the 22,660-acre Palisades fire and 15 percent of the 14,000-acre Eaton fire, near Altadena and Pasadena, according to Cal Fire.\",\n",
        "#     \"source_language\": \"ingl√©s\",\n",
        "#     \"target_language\": \"espa√±ol\",\n",
        "#     \"translation_iterations\": 0\n",
        "# }\n",
        "# result = graph.invoke(inputs)\n",
        "\n",
        "# 8. Ejecutar\n",
        "print(\"üì§ Por favor, sube tu archivo de texto (.txt):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Obtener texto del archivo subido\n",
        "uploaded_file_name = next(iter(uploaded))\n",
        "original_text = uploaded[uploaded_file_name].decode('utf-8')\n",
        "\n",
        "inputs = {\n",
        "    \"original_text\": original_text,  # Usar texto del archivo subido\n",
        "    \"source_language\": \"ingl√©s\",\n",
        "    \"target_language\": \"espa√±ol\",\n",
        "    \"translation_iterations\": 0\n",
        "}\n",
        "\n",
        "result = graph.invoke(inputs)\n",
        "\n",
        "\n",
        "# 9. Con display(Markdown())\n",
        "display(Markdown(f\"\"\"\n",
        "### üìù **Tracci√≥n Finalizada**\n",
        "üó£Ô∏è **Origen:** {result['source_language']} ‚Üí **Destino:** {result['target_language']}\n",
        "\n",
        "üî§ **Texto original:**\n",
        "*{result['original_text']}...*\n",
        "\n",
        "üéØ **Traducci√≥n:**\n",
        "{result.get('editor_output', '')}\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "_J0WZPK7uDO_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "1f37d38b-3c81-4450-8ad8-351de900ffa9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Por favor, sube tu archivo de texto (.txt):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2ab86744-8114-4917-a231-44495f69a5bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2ab86744-8114-4917-a231-44495f69a5bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving draft.txt to draft.txt\n",
            "\n",
            "--- TRANSLATING ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "üìÑ Traducci√≥n generada:\nEn este contexto, el presente estudio tiene como objetivo evaluar el rendimiento de dos sistemas de traducci√≥n autom√°tica neuronal (NMT) de vanguardia (Google Translate y DeepL) en comparaci√≥n con un modelo de lenguaje de √∫ltima generaci√≥n (GPT-4o) en la traducci√≥n de expresiones multi-palabra (MWEs) del espa√±ol al ingl√©s. Al centrarnos tanto en formas continuas como discontinuas de las MWEs, buscamos determinar hasta qu√© punto los modelos de lenguaje pueden abordar las limitaciones de los sistemas NMT tradicionales. Para guiar esta investigaci√≥n, planteamos las siguientes preguntas de investigaci√≥n:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ACCURACY REVIEW ---\n",
            "‚ö†Ô∏è Sugerencias de precisi√≥n:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ERROR: \"GPT-4o\" ‚Üí SUGGESTION: \"GPT-4\" (correct the model name to match the standard terminology)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ERROR: \"expresiones multi-palabra\" ‚Üí SUGGESTION: \"expresiones de varias palabras\" (use a more commonly accepted term for MWEs in Spanish)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ERROR: \"sistemas de traducci√≥n autom√°tica neuronal (NMT)\" ‚Üí SUGGESTION: \"sistemas de traducci√≥n autom√°tica neuronal (TAN)\" (consider using the Spanish acronym for consistency)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ERROR: \"modelos de lenguaje\" ‚Üí SUGGESTION: \"modelos de lenguaje de √∫ltima generaci√≥n\" (to maintain consistency with the term used for LLM in the source text)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FLUENCY REVIEW ---\n",
            "‚úèÔ∏è Sugerencias de estilo:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ISSUE: La frase \"de vanguardia\" podr√≠a ser considerada redundante en este contexto. ‚Üí IMPROVEMENT: Reemplazar \"de vanguardia\" por \"avanzados\" para mayor claridad."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ISSUE: El t√©rmino \"√∫ltima generaci√≥n\" puede ser redundante al referirse a \"GPT-4o\", ya que se sobreentiende. ‚Üí IMPROVEMENT: Simplificar a \"modelo de lenguaje GPT-4o\"."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ISSUE: La estructura \"tanto en formas continuas como discontinuas de las MWEs\" puede ser confusa. ‚Üí IMPROVEMENT: Reestructurar a \"tanto en las formas continuas como en las discontinuas de las MWEs\"."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    - ISSUE: La frase \"abordar las limitaciones de los sistemas NMT tradicionales\" puede ser mejorada para mayor fluidez. ‚Üí IMPROVEMENT: Cambiar a \"superar las limitaciones de los sistemas NMT tradicionales\"."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    Revisando el texto completo con las mejoras sugeridas:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "    \"En este contexto, el presente estudio tiene como objetivo evaluar el rendimiento de dos sistemas de traducci√≥n autom√°tica neuronal (NMT) avanzados (Google Translate y DeepL) en comparaci√≥n con un modelo de lenguaje GPT-4o en la traducci√≥n de expresiones multi-palabra (MWEs) del espa√±ol al ingl√©s. Al centrarnos tanto en las formas continuas como en las discontinuas de las MWEs, buscamos determinar hasta qu√© punto los modelos de lenguaje pueden superar las limitaciones de los sistemas NMT tradicionales. Para guiar esta investigaci√≥n, planteamos las siguientes preguntas de investigaci√≥n:\""
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FINAL EDITING ---\n",
            "üîñ Versi√≥n final:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "En este contexto, el presente estudio tiene como objetivo evaluar el rendimiento de dos sistemas de traducci√≥n autom√°tica neuronal (TAN) avanzados (Google Translate y DeepL) en comparaci√≥n con un modelo de lenguaje GPT-4 en la traducci√≥n de expresiones de varias palabras (MWEs) del espa√±ol al ingl√©s. Al centrarnos tanto en las formas continuas como en las discontinuas de las MWEs, buscamos determinar hasta qu√© punto los modelos de lenguaje de √∫ltima generaci√≥n pueden superar las limitaciones de los sistemas TAN tradicionales. Para guiar esta investigaci√≥n, planteamos las siguientes preguntas de investigaci√≥n:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n### üìù **Tracci√≥n Finalizada**\nüó£Ô∏è **Origen:** ingl√©s ‚Üí **Destino:** espa√±ol\n\nüî§ **Texto original:**\n*In this context, the present study aims to evaluate the performance of two state-of-the art NMT\r\nsystems (Google Translate and DeepL) against a state-of-the-art LLM (GPT4o) in translating\r\nSpanish-to-English MWEs. By focusing on both continuous and discontinuous forms of MWEs,\r\nwe seek to determine the extent to which LLMs can address the limitations of traditional NMT\r\nsystems. To guide this investigation, we pose the following research questions:\r\n...*\n\nüéØ **Traducci√≥n:**\nEn este contexto, el presente estudio tiene como objetivo evaluar el rendimiento de dos sistemas de traducci√≥n autom√°tica neuronal (TAN) avanzados (Google Translate y DeepL) en comparaci√≥n con un modelo de lenguaje GPT-4 en la traducci√≥n de expresiones de varias palabras (MWEs) del espa√±ol al ingl√©s. Al centrarnos tanto en las formas continuas como en las discontinuas de las MWEs, buscamos determinar hasta qu√© punto los modelos de lenguaje de √∫ltima generaci√≥n pueden superar las limitaciones de los sistemas TAN tradicionales. Para guiar esta investigaci√≥n, planteamos las siguientes preguntas de investigaci√≥n:\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}