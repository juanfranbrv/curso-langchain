{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSvk3WVG28m+UshIXWq41w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Prompts_e_Ingenier%C3%ADa_de_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts e Ingeniería de Prompts dentro de LangChain  \n",
        "---\n",
        "Este cuaderno se enfoca en los **Prompts** y en la **Ingeniería de Prompts** dentro de LangChain. Abordaremos los fundamentos de los prompts, por qué son tan importantes y cómo LangChain nos ayuda a crearlos y reutilizarlos de manera efectiva con distintas clases de Templates: `PromptTemplate`, `FewShotPromptTemplate` y `ChatPromptTemplate`."
      ],
      "metadata": {
        "id": "-jfpHiwS8-O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción a los Prompts\n",
        "\n",
        "En el contexto de los grandes modelos de lenguaje (Large Language Models - LLMs), el prompt es el mensaje, instrucción o pregunta que proporcionamos para guiar la respuesta del modelo, por ejemplo:\n",
        "\n",
        "-  “Explícame en tres frases qué es el *Machine Learning*”.\n",
        "\n",
        "Cuando diseñamos prompts, buscamos que el modelo entienda el contexto y la tarea específica que queremos resolver.\n",
        "\n",
        "Un buen prompt permite orientar al LLM para que genere una respuesta más precisa y alineada con la tarea.  \n",
        "\n",
        "Mediante técnicas como *few-shot prompting*, *Chain-of-Thought (CoT) Prompting* y otras, podemos moldear la personalidad y estilo de la respuesta.  \n",
        "\n",
        "Los prompts son fundamentales en el manejo de modelos de lenguaje ya que guían al modelo para generar respuestas coherentes y útiles. Un prompt bien diseñado puede marcar la diferencia entre una salida precisa y relevante, y una respuesta confusa o fuera de contexto.  \n",
        "\n",
        "LangChain ofrece herramientas específicas para gestionar y optimizar prompts de manera eficiente. Estas herramientas permiten crear plantillas de prompts reutilizables, adaptarlas dinámicamente según el contexto y personalizarlas para diferentes aplicaciones.\n"
      ],
      "metadata": {
        "id": "-XbkJmqr9gCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuración del entorno del cuaderno\n",
        "\n",
        "Instalamos los paquetes necesarios para LangChain, incluyendo soporte para OpenAI y Groq, y recupera las claves API correspondientes desde los secretos de Colab para fines de autenticación.\n",
        "(Este codigo se explico con detalle en el cuaderno anterior)\n",
        "\n"
      ],
      "metadata": {
        "id": "CeFmOSpM_Mnb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEAvzKy0y4wf"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -qU\n",
        "\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "# !pip install langchain-google-genai -qU\n",
        "# !pip install langchain-huggingface -qU\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. PromptTemplate\n",
        "---\n",
        "Los prompt templates en Langchain permiten crear prompts dinámicos y flexibles mediante la incorporación de variables y opciones de formato, lo que le permite personalizar los prompts en función de los datos de entrada o tareas específicas.\n",
        "\n",
        "- Ayudan en la construccion de prompts complejos.\n",
        "- Son dinamicos: tienen *placeholders* para variables {variable}  \n",
        "- Pueden reutilizarse  \n",
        "\n",
        "Imagina que quieres reutilizar la misma estructura de prompt para múltiples entradas; en lugar de escribir el prompt completo cada vez, puedes definir una “plantilla” con huecos que se llenan de forma programática.\n",
        "\n",
        "Ejemplos:\n",
        "\n",
        "- \"Explicame {tema} con nivel de dificultad {nivel}\"\n",
        "\n",
        "- \"Traduce {frase} del {idioma_entrada} a {idioma_salida}\"\n",
        "\n",
        "Prompts como los anteriores son genericos y reutilizables, pues se usan com plantillas para la construcción de prompts concretos."
      ],
      "metadata": {
        "id": "TiD0D54ZBTGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sintaxis básica\n",
        "\n",
        "`PromptTemplate` es la clase base que nos permite crear plantillas de prompts con variables dinámicas.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "prompt_template = PromptTemplate(\n",
        "            input_variables=[\"variable1\", \"variable2\"],  # Lista de variables dinámicas\n",
        "            template=\"Este es un ejemplo donde {variable1} y {variable2} son variables dinámicas.\"\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "*Observación: De momento no vamos a consultara ningún LLM. Sólo estamos construyendo un prompt que luego podriamos pasar a un modelo.*\n",
        "\n",
        "1. Definimos una cadena de texto (**`plantilla_texto`**) con dos variables dinámicas: `{tema}` y `{pregunta}`.\n",
        "\n",
        "2. Usamos el contructor de clase `PromptTemplate()` para crear un objeto PromptTemplate indicando cuáles son esas variables (input_variables) y cuál es la plantilla (template).\n",
        "\n",
        "3. Luego usamos el metodo `.format()` de la clase, para rellenar los huecos del template."
      ],
      "metadata": {
        "id": "e63XeAs-DlJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "plantilla_texto = \"\"\"\n",
        "Eres un experto en {tema}.\n",
        "Por favor, responde la siguiente pregunta con detalle:\n",
        "Pregunta: {pregunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"tema\", \"pregunta\"],\n",
        "    template=plantilla_texto\n",
        ")\n",
        "\n",
        "prompt=prompt_template.format(tema=\"programacion\", pregunta=\"Que es python?\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "3dhZ-0XZEoky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase `PromptTemplate` tiene un metodo que permite instanciar objetos sin necesidad de usar el contructor de clase lo que permite que el codigo sea mas compacto y legible. Este metodo es `.from_template()`\n",
        "\n",
        "\n",
        "```\n",
        "prompt_template = \"Hola, {nombre}. Hoy es {dia}. ¿Cómo estás?\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "`PromptTemplate.from_template()` es un _método de clase_ que permite crear una instancia de `PromptTemplate` de manera directa a partir de una cadena de texto que contiene variables de relleno (por ejemplo, `{tema}` y `{pregunta}`). A diferencia de usar el constructor estándar (donde se debe especificar `input_variables` y la plantilla por separado), esta función:\n",
        "\n",
        "1. **Extrae automáticamente** los nombres de las variables desde las llaves (p. ej., `{tema}`, `{pregunta}`).\n",
        "2. **Crea y retorna** un objeto `PromptTemplate` listo para usar, lo que hace el código más compacto y legible.   \n",
        "\n",
        "Este es el metodo preferido para crear PromptTemplates"
      ],
      "metadata": {
        "id": "7BYVnqgMFu5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "#Instanciar usando .from_template es la forma preferida\n",
        "prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "Eres un experto en {tema}.\n",
        "Por favor, responde la siguiente pregunta con detalle:\n",
        "Pregunta: {pregunta}\n",
        "\"\"\")\n",
        "prompt = prompt_template.format(tema=\"programación\", pregunta=\"Que es python?\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "jsni2Y0G-djc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos ahora a crear un prompt template y enviarselo a un LLM\n",
        "\n",
        "1.   Elemento de lista\n",
        "2.   Elemento de lista\n",
        "\n"
      ],
      "metadata": {
        "id": "blp6w6QcLVS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
        "llm2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "plantilla_texto = \"\"\"\n",
        "Eres un experto en traducción del {idioma_origen} al inglés.\n",
        "Por favor, traduce el siguiente texto:\n",
        "---\n",
        "Texto: {texto_a_traducir}\n",
        "---\n",
        "Mantén la intención, el estilo y el tono original.\n",
        "Proporciona solo el texto traducido, sin comentarios adicionales\n",
        "\"\"\"\n",
        "\n",
        "# Creamos el PromptTemplate usando las variables definidas\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"idioma_origen\", \"texto_a_traducir\"],\n",
        "    template=plantilla_texto\n",
        ")\n",
        "\n",
        "# Ejemplo de uso\n",
        "prompt = prompt_template.format(\n",
        "    idioma_origen=\"español\",\n",
        "    texto_a_traducir=\"¡Hola! ¿Cómo te va?\"\n",
        ")\n",
        "\n",
        "\n",
        "#Invocamos los modelos pasandoles el prompt\n",
        "respuesta1 = llm1.invoke(prompt)\n",
        "respuesta2 = llm2.invoke(prompt)\n",
        "\n",
        "\n",
        "# Extraemos de la respuesta el nombre del modelo y el resultado\n",
        "print(respuesta1.response_metadata[\"model_name\"]) # imprimimos el nombre del modelo\n",
        "print(respuesta1.content)\n",
        "print(\"---\")\n",
        "print(respuesta2.response_metadata[\"model_name\"]) # imprimimos el nombre del modelo\n",
        "print(respuesta2.content)\n"
      ],
      "metadata": {
        "id": "FulupEhVHKdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reto 1: Crea tu propio PromptTemplate\n",
        "\n",
        "**Objetivo**  \n",
        "\n",
        "Diseñar un `PromptTemplate` que reciba variables para traducir un texto de un idioma a otro, especificando además el estilo de traducción. Luego, pasar este prompt a un LLM para obtener el resultado y verificar su eficacia.\n",
        "\n",
        "**Instrucciones**\n",
        "\n",
        "1. **Crea un PromptTemplate** con **tres** variables:\n",
        "    \n",
        "    - `{idioma_origen}`: El idioma original en el que está el texto.\n",
        "    - `{idioma_destino}`: El idioma al que se va a traducir.\n",
        "    - `{texto_original}`: El texto que se quiere traducir.\n",
        "2. **Incluye** en tu plantilla alguna instrucción sobre el **estilo** o **tono** de la traducción. Por ejemplo, podrías pedir que sea “formal” o “informal”.\n",
        "    \n",
        "3. **Crea** el prompt final usando `.format(...)` y pásaselo a un LLM de tu elección. Obtén la respuesta y **muéstrala** en pantalla.\n",
        "    \n",
        "4. (Opcional) **Experimenta** con distintos valores de _temperature_ o _top\\_p_ (dependiendo del LLM que uses) para comparar la **consistencia** de la traducción."
      ],
      "metadata": {
        "id": "VDv9qxAWRh4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escribe aquí tu solución para el reto 1"
      ],
      "metadata": {
        "id": "CQa8iipcWi8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solución Reto 1"
      ],
      "metadata": {
        "id": "SOLsFcGkVjJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realmente no es necesario importar de nuevo las librerías.\n",
        "# Solo lo hacemos con fines ilustrativos, para que el ejemplo sea autosuficiente.\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# 1. Define la plantilla con las variables {idioma_origen}, {idioma_destino}, y {texto_original}\n",
        "mi_plantilla = \"\"\"\n",
        "Eres un traductor experto. Traducirás cuidadosamente del {idioma_origen} al {idioma_destino}.\n",
        "Por favor, mantén un tono formal en la traducción.\n",
        "---\n",
        "Texto original: {texto_original}\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "# 2. Crea el PromptTemplate\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"idioma_origen\", \"idioma_destino\", \"texto_original\"],\n",
        "    template=mi_plantilla\n",
        ")\n",
        "\n",
        "# 3. Usa .format para rellenar los huecos\n",
        "prompt = prompt_template.format(\n",
        "    idioma_origen=\"español\",\n",
        "    idioma_destino=\"francés\",\n",
        "    texto_original=\"¡Hola! ¿Cómo estás hoy?\"\n",
        ")\n",
        "\n",
        "# 4. Invoca tu LLM y muestra el resultado\n",
        "# (Ejemplo con un LLM hipotético `my_llm`)\n",
        "response = llm.invoke(prompt, temperature=0.5)\n",
        "\n",
        "print(\"Prompt final:\\n\", prompt)\n",
        "print(\"\\nTraducción:\\n\", response.content)\n"
      ],
      "metadata": {
        "id": "-2Plf-wTHKaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. FewShotPromptTemplate: proporcionando ejemplos de referencia\n",
        "\n",
        "**Few-shot prompting** es una técnica donde proporcionamos ejemplos dentro del prompt para que el modelo entienda mejor el contexto y la forma de la respuesta. Con `FewShotPromptTemplate` podemos manejar varios ejemplos y luego añadir la pregunta final.\n",
        "\n",
        "### Sintaxis\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "FewShotPromptTemplate(\n",
        "    examples=ejemplos,               # Lista de ejemplos\n",
        "    example_prompt=example_prompt,   # PromptTemplate para cada ejemplo\n",
        "    prefix=prefix_text,              # Texto que aparecerá antes de los ejemplos\n",
        "    suffix=suffix_text,              # Texto que aparecerá después de los ejemplos, habitualmente la pregunta\n",
        "    input_variables=[\"nuevo_input\"], # Variables adicionales que usarás\n",
        "    example_separator=\"\\n---\\n\"      # Separador entre ejemplos\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### ¿Cómo funciona FewShotPromptTemplate?\n",
        "\n",
        "**¿Cómo funciona `FewShotPromptTemplate`?**\n",
        "\n",
        "1. **`examples`:** Una lista de diccionarios. Cada diccionario representa un ejemplo y contiene pares clave-valor con las entradas y salidas deseadas para una tarea específica.\n",
        "    \n",
        "2. **`example_prompt`:** Una instancia de `PromptTemplate` que define cómo se formatea cada ejemplo individualmente. Define las variables de entrada que se utilizarán para cada ejemplo.\n",
        "    \n",
        "3. **`prefix`:** Una cadena que va al principio del prompt completo. Suele ser una instrucción general o una descripción de la tarea.\n",
        "    \n",
        "4. **`suffix`:** Una cadena que va al final del prompt completo. Normalmente aquí es donde se introduce la pregunta al modelo, que debria ser como los ejemplos, pero sin el resultado\n",
        "    \n",
        "5. **`input_variables`:** Una lista de nombres de variables que se utilizarán en el `suffix`.\n",
        "    \n",
        "6. **`example_separator`:** Una cadena que separa cada ejemplo en el prompt. Por defecto es `\\n\\n`.\n",
        "    \n",
        "    \n",
        "\n",
        "**En resumen, `FewShotPromptTemplate` toma los ejemplos, los formatea usando `example_prompt`, los une con `example_separator`, y los coloca entre `prefix` y `suffix` para crear el prompt final. Este prompt contiene el contexto necesario (los ejemplos) para que el modelo de lenguaje pueda inferir la tarea y aplicarla a la nueva entrada proporcionada en el `suffix`.**\n",
        "\n",
        "Hay que usar la sintaxis con el constructor de clase. No hay disponible ningun metodo que simplifique la sintaxis."
      ],
      "metadata": {
        "id": "lEUBfAU2Vrrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "\n",
        "# Lista de ejemplo, donde cada ejemplo es un diccionario\n",
        "ejemplos = [\n",
        "    {\"input\": \"Hello\", \"output\": \"Bonjour\"},\n",
        "    {\"input\": \"Goodbye\", \"output\": \"Au revoir\"},\n",
        "    {\"input\": \"Thank you\", \"output\": \"Merci\"},\n",
        "]\n",
        "\n",
        "#Instancia de PromptTemplate para formatear los ejemplos\n",
        "traduccion_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Creamos el prompt pasando:\n",
        "# la lista de ejemplos\n",
        "# el formateador de los ejemplos\n",
        "# el prefix, que suele ser la descripcion general de la tarea\n",
        "# el suffix, que donde introduciomes la pregunta con la variables\n",
        "# La lista de variables\n",
        "\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,\n",
        "    example_prompt=traduccion_prompt_template,\n",
        "    prefix=\"Translate the following English words to French:\",\n",
        "    suffix=\"Input: {english_word}\\nOutput:\",\n",
        "    input_variables=[\"english_word\"],\n",
        ")\n",
        "\n",
        "print(prompt.format(english_word=\"Car\"))"
      ],
      "metadata": {
        "id": "4S8x65xPHKXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a usarlo para realizar una llamada a los modelos que estamos usando en este cuaderno"
      ],
      "metadata": {
        "id": "jVVSVmF9V6gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Creamos las instancias de cada modelo.\n",
        "llm1 = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "llm2 = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "\n",
        "# 1) Definimos algunos ejemplos de traducción: (input → output)\n",
        "ejemplos = [\n",
        "    {\"input\": \"Buenos días\", \"output\": \"Good morning\"},\n",
        "    {\"input\": \"¿Dónde está el baño?\", \"output\": \"Where is the bathroom?\"}\n",
        "]\n",
        "\n",
        "# 2) Creamos un PromptTemplate para cada ejemplo: cómo se mostrará \"input\" y \"output\" en el prompt\n",
        "traduccion_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"\"\"\n",
        "Ejemplo de Entrada: {input}\n",
        "Ejemplo de Salida: {output}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 3) Definimos un prefijo y un sufijo para contextualizar la tarea antes y después de los ejemplos\n",
        "prefix_text = \"\"\"Eres un traductor experto del español al inglés.\n",
        "Estos son algunos ejemplos de cómo traducir oraciones al inglés:\"\"\"\n",
        "\n",
        "suffix_text = \"\"\"\n",
        "Ahora traduce la siguiente frase:\n",
        "Entrada: {nueva_frase}\n",
        "Salida:\n",
        "\"\"\"\n",
        "\n",
        "# 4) Creamos el FewShotPromptTemplate\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,                  # Nuestros ejemplos\n",
        "    example_prompt=traduccion_prompt_template,      # Formato de cada ejemplo\n",
        "    prefix=prefix_text,                 # Texto antes de los ejemplos\n",
        "    suffix=suffix_text,                 # Texto después de los ejemplos\n",
        "    input_variables=[\"nueva_frase\"],    # Variable requerida al formatear\n",
        "    example_separator=\"\\n---\\n\"         # Separador entre ejemplos\n",
        ")\n",
        "\n",
        "# 5) Generamos el prompt final para la nueva frase\n",
        "prompt = few_shot_prompt.format(nueva_frase=\"¡Hola! ¿Cómo te va?\")\n",
        "\n",
        "print(\"=== Prompt Generado ===\")\n",
        "print(prompt)\n",
        "\n",
        "# 6) Pasamos el prompt al primer modelo (ChatOpenAI - gpt-4o-mini)\n",
        "print(\"\\n=== Respuesta con ChatOpenAI (gpt-4o-mini) ===\")\n",
        "response_llm1 = llm1.invoke(prompt)\n",
        "print(response_llm1.content)\n",
        "\n",
        "# 7) Pasamos el mismo prompt al segundo modelo (ChatGroq - llama-3.3-70b-versatile)\n",
        "print(\"\\n=== Respuesta con ChatGroq (llama-3.3-70b-versatile) ===\")\n",
        "response_llm2 = llm2.invoke(prompt)\n",
        "print(response_llm2.content)\n"
      ],
      "metadata": {
        "id": "-qM0EWM2HKUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reto 2: Few-Shot Prompting para Traducción Creativa\n",
        "\n",
        "El reto consiste en usar la clase FewShotPromptTemplate para traducir oraciones del español al inglés con un estilo específico, incorporando ejemplos previos que sirvan de guía para el LLM.  \n",
        "\n",
        "### **Instrucciones**\n",
        "\n",
        "1. **Crea una lista de ejemplos** de traducción que contengan:\n",
        "    \n",
        "    - La frase de entrada en español (clave: `\"input\"`).\n",
        "    - Su traducción al inglés, **pero con cierto estilo o “sabor”** (clave: `\"output\"`).\n",
        "        - Por ejemplo, podrías elegir que la traducción sea **muy “entusiasta”** o **tenga un tono humorístico**.\n",
        "2. **Diseña un `PromptTemplate`** para mostrar cada ejemplo con un formato claro. Algo así como:\n",
        "    \n",
        "    python\n",
        "    \n",
        "    Copiar código\n",
        "    \n",
        "    `\"Oración original: {input}\\nTraducción estilizada: {output}\\n\"`\n",
        "    \n",
        "    (puedes ajustar el texto a tu preferencia).\n",
        "    \n",
        "3. **Define** el `prefix` y el `suffix` de tu `FewShotPromptTemplate`:\n",
        "    \n",
        "    - El `prefix` debe _explicar_ que el LLM es un traductor con cierto estilo (por ejemplo, “traductor humorístico” o “traductor formal”).\n",
        "    - El `suffix` debe contener la **nueva frase** a traducir (`{frase_nueva}`) y especificar que se aplique el mismo estilo.\n",
        "4. **Crea** la instancia de `FewShotPromptTemplate` con:\n",
        "    \n",
        "    - `examples`: tu lista de ejemplos.\n",
        "    - `example_prompt`: tu plantilla de ejemplo.\n",
        "    - `prefix`, `suffix`, `example_separator`, etc.\n",
        "5. **Usa `.format()`** para generar el prompt final, pasando la nueva frase a traducir en la variable `{frase_nueva}`.\n",
        "    \n",
        "6. **Envía** ese prompt a un LLM (puede ser `ChatOpenAI`, `ChatGroq` o cualquier otro). Muestra la respuesta que obtienes.\n",
        "    \n",
        "7. (Opcional) **Experimenta** con distintos estilos (más formal, más bromista, etc.) para ver cómo cambian las traducciones.\n",
        "\n",
        "8. (Opcional). Puedes incluir el estilo de traducción como una varibale dinámica del prompt ?"
      ],
      "metadata": {
        "id": "Jhd_bZrReiqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución Reto 2"
      ],
      "metadata": {
        "id": "2cR2kNLmYdwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "# 1. Ejemplos\n",
        "ejemplos = [\n",
        "    {\n",
        "        \"input\": \"Buenos días, ¿cómo amaneciste?\",\n",
        "        \"output\": \"Good morning, how did you wake up, buddy? (super friendly tone)\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"¿Quieres salir a correr más tarde?\",\n",
        "        \"output\": \"Do you want to go for a run later, pal? (energetic style)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# 2. Plantilla de ejemplo\n",
        "ejemplo_plantilla = \"\"\"\n",
        "Oración original: {input}\n",
        "Traducción con estilo: {output}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=ejemplo_plantilla\n",
        ")\n",
        "\n",
        "# 3. Prefijo y Sufijo\n",
        "prefix_text = \"\"\"Eres un traductor que siempre usa un tono muy amigable y entusiasta.\n",
        "Aquí tienes algunos ejemplos de cómo traduces del español al inglés:\n",
        "\"\"\"\n",
        "suffix_text = \"\"\"\n",
        "Ahora quiero que traduzcas la siguiente frase con el mismo tono amistoso:\n",
        "Oración: {frase_nueva}\n",
        "Traducción:\n",
        "\"\"\"\n",
        "\n",
        "# 4. FewShotPromptTemplate\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix_text,\n",
        "    suffix=suffix_text,\n",
        "    input_variables=[\"frase_nueva\"],\n",
        "    example_separator=\"\\n---\\n\"\n",
        ")\n",
        "\n",
        "# 5. Formateo del prompt\n",
        "prompt_final = few_shot_prompt.format(frase_nueva=\"¡Hola! ¿Listo para la aventura de hoy?\")\n",
        "\n",
        "# 6. (Pseudo-código) Llamada al LLM\n",
        "# response = llm.invoke(prompt_final)\n",
        "# print(response.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "xWHO02kHYnvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "040q1XBhWqzS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcMivP2eHKRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9S3W_xdEHKOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyQDLensHKLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para eviar el uso de algunas variables, se suele escribir de forma mas compacta\n",
        "# en adelante usaremos esta forma.\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Cuentame un chiste sobre {tema}\")\n",
        "prompt = prompt_template.format(tema=\"enanos\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "rQ8vwCKhYkMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instanciar usando .from_template es la forma preferida\n",
        "prompt_template = PromptTemplate.from_template(\"Cuentame un chiste {adjetivo} sobre {tema}\")\n",
        "prompt = prompt_template.format(adjetivo=\"malo\", tema=\"enanos\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "hExxeHalErRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando el constructor de clase\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"adjetivo\", \"tema\"],\n",
        "    template=\"Cuentame un chiste {adjetivo} sobre {tema}\")\n",
        "\n",
        "prompt = prompt_template.format(adjetivo=\"malo\", tema=\"enanos\")\n",
        "\n",
        "prompt\n"
      ],
      "metadata": {
        "id": "nrA5MUcHIG8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template('''\n",
        "Proporciona un resumen conciso del siguiente tema: {tema}\n",
        "Usa un nivel de lenguaje {nivel}\n",
        "Incluye al menos los siguinetes puntos clave:\n",
        "* Información de antecedentes\n",
        "* Ideas principales\n",
        "* Detalles de apoyo\n",
        "* Importancia o impacto\n",
        "Manten una actitud neutral y objetiva en tu resumen.\n",
        "''')\n",
        "prompt = prompt_template.format(tema=\"Cambio climatico\", nivel=\"técnico\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "JpkJGD8iQ3_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diagnostico_prompt_template = PromptTemplate.from_template('''\n",
        "Eres un médico informado y empático. Con base en los siguientes síntomas descritos por un paciente, proporciona un diagnóstico potencial y sugiere los próximos pasos para el tratamiento.\n",
        "Síntomas del paciente: {síntomas}\n",
        "''')\n",
        "\n",
        "prompt=diagnostico_prompt_template.format(síntomas=\"Fiebre, dolor de garganta, tos seca\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "PkNXGGfEXvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consejo_financiero_prompt_template = PromptTemplate.from_template('''\n",
        "Eres un asesor financiero certificado. En función del perfil financiero del usuario, brindar asesoramiento de inversión personalizado.\n",
        "Perfil de usuario:\n",
        "- Edad: {edad}\n",
        "- Ingresos: {ingresos}\n",
        "- Tolerancia al riesgo: {tolerancia_riesgo}\n",
        "- Objetivos de inversión: {objetivos_inversion}\n",
        "''')\n",
        "\n",
        "prompt = consejo_financiero_prompt_template.format(edad=50, ingresos=\"100000€\", tolerancia_riesgo=\"moderada\", objetivos_inversion=\"ahorros jubilación\" )\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "kYgEPiWeax93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`FewShotPromptTemplate` en LangChain es una herramienta poderosa para el aprendizaje de pocos ejemplos (few-shot learning) en aplicaciones de lenguaje. Te permite definir un conjunto de ejemplos de entrada/salida y, junto con una plantilla de prompt, generar prompts para un modelo de lenguaje que imitan un comportamiento deseado. Básicamente, ayuda a entrenar a un modelo para que aprenda a partir de unos pocos ejemplos cómo realizar una tarea.\n",
        "\n",
        "Aquí te explico su funcionamiento y te doy ejemplos:\n",
        "\n",
        "**¿Cómo funciona `FewShotPromptTemplate`?**\n",
        "\n",
        "1. **`examples`:** Un array de diccionarios. Cada diccionario representa un ejemplo y contiene pares clave-valor con las entradas y salidas deseadas para una tarea específica.\n",
        "    \n",
        "2. **`example_prompt`:** Una instancia de `PromptTemplate` que define cómo se formatea cada ejemplo individualmente. Define las variables de entrada que se utilizarán para cada ejemplo.\n",
        "    \n",
        "3. **`prefix`:** Una cadena que va al principio del prompt completo. Suele ser una instrucción general o una descripción de la tarea.\n",
        "    \n",
        "4. **`suffix`:** Una cadena que va al final del prompt completo. Normalmente aquí es donde se introduce la nueva entrada a procesar por el modelo.\n",
        "    \n",
        "5. **`input_variables`:** Una lista de nombres de variables que se utilizarán en el `suffix`.\n",
        "    \n",
        "6. **`example_separator`:** Una cadena que separa cada ejemplo en el prompt. Por defecto es `\\n\\n`.\n",
        "    \n",
        "7. **`template_format`:** Define el formato en el que se va a procesar la plantilla. Por defecto es `f-string`. Otras opciones son `jinja2`.\n",
        "    \n",
        "\n",
        "**En resumen, `FewShotPromptTemplate` toma los ejemplos, los formatea usando `example_prompt`, los une con `example_separator`, y los coloca entre `prefix` y `suffix` para crear el prompt final. Este prompt contiene el contexto necesario (los ejemplos) para que el modelo de lenguaje pueda inferir la tarea y aplicarla a la nueva entrada proporcionada en el `suffix`.**\n",
        "\n",
        "Hay que usar la sintaxis con el constructor de clase. No hay disponible ningun metodo que simplifique la sintaxis."
      ],
      "metadata": {
        "id": "nyuWJG_Pe56-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "\n",
        "# Lista de ejemplo, donde cada ejemplo es un diccionario\n",
        "ejemplos = [\n",
        "    {\"input\": \"Hello\", \"output\": \"Bonjour\"},\n",
        "    {\"input\": \"Goodbye\", \"output\": \"Au revoir\"},\n",
        "    {\"input\": \"Thank you\", \"output\": \"Merci\"},\n",
        "]\n",
        "\n",
        "#Instancia de PromptTemplate para formatear los ejemplos\n",
        "traduccion_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Creamos el prompt pasando:\n",
        "# la lista de ejemplos\n",
        "# el formateador de los ejemplos\n",
        "# el prefix, que suele ser la descripcion general de la tarea\n",
        "# el suffix, que donde introduciomes la pregunta con la variables\n",
        "# La lista de variables\n",
        "\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,\n",
        "    example_prompt=traduccion_prompt_template,\n",
        "    prefix=\"Translate the following English words to French:\",\n",
        "    suffix=\"Input: {english_word}\\nOutput:\",\n",
        "    input_variables=[\"english_word\"],\n",
        ")\n",
        "\n",
        "print(prompt.format(english_word=\"Car\"))"
      ],
      "metadata": {
        "id": "FXgo2n6qe5Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear un prompt con ejemplos para que indique la capital de un pais"
      ],
      "metadata": {
        "id": "Fq1_JFYc0Gnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs:\n",
        "\n"
      ],
      "metadata": {
        "id": "YW5cRwDPNhog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las prompt templatespermiten crear solicitudes dinámicas y flexibles mediante la incorporación de variables y opciones de formato, lo que le permite personalizar las solicitudes en función de los datos de entrada o tareas específicas.\n",
        "\n",
        "- Son dinamicas: tinene placeholders para variables\n",
        "- Pueden reutilizarse\n",
        "\n",
        "Un prompt template como este \"Explicame {tema} con nivel de dificultad {nivel}\" es reutilizable y flexible, sirve para muchos situaciones.\n",
        "\n",
        "\"traduce {frase} del {idioma_entrada} a {idioma_salida}\"\n",
        "\n",
        "\n",
        "Diferencias entre ChatPromptTemplate y PrompTemplate\n",
        "Difrenecias entre .format y .invoke\n",
        "\n",
        "- Pueden tener locga condicional interna (if)\n",
        "- Pueden anidarse\n",
        "\n",
        "---\n",
        "¿Qué es Prompt?  \n",
        "\n",
        "Un mensaje para un modelo de lenguaje es un conjunto de instrucciones o entradas proporcionadas por un usuario para guiar la respuesta del modelo, ayudándolo a comprender el contexto y generar resultados relevantes y coherentes basados ​​en el lenguaje, como responder preguntas, completar oraciones o participar en una conversación.  \n",
        "\n",
        "---\n",
        "3 tipos de plantillas de indicaciones de LangChain¶\n",
        "Cuando se solicita en LangChain, se recomienda (aunque no es obligatorio) utilizar una clase de plantilla predefinida como:\n",
        "\n",
        "PromptTemplatepara crear indicaciones básicas.\n",
        "FewShotPromptTemplatepara un aprendizaje de unos pocos disparos.\n",
        "ChatPromptTemplatepara modelar interacciones de chatbot."
      ],
      "metadata": {
        "id": "zy0Qe8UGSZMo"
      }
    }
  ]
}