{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzjp4V8VkY/Hgt9OjwRjQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Prompts_e_Ingenier%C3%ADa_de_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts e Ingeniería de Prompts dentro de LangChain  \n",
        "---\n",
        "Este cuaderno se enfoca en los **Prompts** y en la **Ingeniería de Prompts** dentro de LangChain. Abordaremos los fundamentos de los prompts, por qué son tan importantes y cómo LangChain nos ayuda a crearlos y reutilizarlos de manera efectiva con distintas clases de Templates: `PromptTemplate`, `FewShotPromptTemplate` y `ChatPromptTemplate`.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "-jfpHiwS8-O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción a los Prompts\n",
        "\n"
      ],
      "metadata": {
        "id": "-XbkJmqr9gCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pEAvzKy0y4wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9294d924-21e9-4096-d6af-8a3ce0d96d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain -qU\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "#Instanciar usando .from_template es la forma preferida\n",
        "plantilla =\"Cuentame un chiste sobre {tema}\"\n",
        "prompt_template = PromptTemplate.from_template(plantilla)\n",
        "prompt = prompt_template.format(tema=\"enanos\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jsni2Y0G-djc",
        "outputId": "0194714c-b3c4-4290-8485-789399e3dc8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cuentame un chiste sobre enanos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para eviar el uso de algunas variables, se suele escribir de forma mas compacta\n",
        "# en adelante usaremos esta forma.\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Cuentame un chiste sobre {tema}\")\n",
        "prompt = prompt_template.format(tema=\"enanos\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "rQ8vwCKhYkMc",
        "outputId": "e2990e73-0df7-48e9-e55c-a741fbedfa5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cuentame un chiste sobre enanos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instanciar usando .from_template es la forma preferida\n",
        "prompt_template = PromptTemplate.from_template(\"Cuentame un chiste {adjetivo} sobre {tema}\")\n",
        "prompt = prompt_template.format(adjetivo=\"malo\", tema=\"enanos\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hExxeHalErRM",
        "outputId": "d32e7ae7-b4c8-4b5b-c989-4369f9e3f61f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cuentame un chiste malo sobre enanos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando el constructor de clase\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"adjetivo\", \"tema\"],\n",
        "    template=\"Cuentame un chiste {adjetivo} sobre {tema}\")\n",
        "\n",
        "prompt = prompt_template.format(adjetivo=\"malo\", tema=\"enanos\")\n",
        "\n",
        "prompt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nrA5MUcHIG8E",
        "outputId": "2c80af5e-5fc8-4305-a171-212211aed47f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cuentame un chiste malo sobre enanos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template('''\n",
        "Proporciona un resumen conciso del siguiente tema: {tema}\n",
        "Usa un nivel de lenguaje {nivel}\n",
        "Incluye al menos los siguinetes puntos clave:\n",
        "* Información de antecedentes\n",
        "* Ideas principales\n",
        "* Detalles de apoyo\n",
        "* Importancia o impacto\n",
        "Manten una actitud neutral y objetiva en tu resumen.\n",
        "''')\n",
        "prompt = prompt_template.format(tema=\"Cambio climatico\", nivel=\"técnico\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "JpkJGD8iQ3_f",
        "outputId": "1144589f-8d1d-4954-dd8c-1a64af67666d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nProporciona un resumen conciso del siguiente tema: Cambio climatico\\nUsa un nivel de lenguaje técnico\\nIncluye al menos los siguinetes puntos clave:\\n* Información de antecedentes\\n* Ideas principales\\n* Detalles de apoyo\\n* Importancia o impacto\\nManten una actitud neutral y objetiva en tu resumen.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diagnostico_prompt_template = PromptTemplate.from_template('''\n",
        "Eres un médico informado y empático. Con base en los siguientes síntomas descritos por un paciente, proporciona un diagnóstico potencial y sugiere los próximos pasos para el tratamiento.\n",
        "Síntomas del paciente: {síntomas}\n",
        "''')\n",
        "\n",
        "prompt=diagnostico_prompt_template.format(síntomas=\"Fiebre, dolor de garganta, tos seca\")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "PkNXGGfEXvPE",
        "outputId": "0e9700f7-4001-4746-9b27-f10a81c70752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEres un médico informado y empático. Con base en los siguientes síntomas descritos por un paciente, proporciona un diagnóstico potencial y sugiere los próximos pasos para el tratamiento.\\nSíntomas del paciente: Fiebre, dolor de garganta, tos seca\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consejo_financiero_prompt_template = PromptTemplate.from_template('''\n",
        "Eres un asesor financiero certificado. En función del perfil financiero del usuario, brindar asesoramiento de inversión personalizado.\n",
        "Perfil de usuario:\n",
        "- Edad: {edad}\n",
        "- Ingresos: {ingresos}\n",
        "- Tolerancia al riesgo: {tolerancia_riesgo}\n",
        "- Objetivos de inversión: {objetivos_inversion}\n",
        "''')\n",
        "\n",
        "prompt = consejo_financiero_prompt_template.format(edad=50, ingresos=\"100000€\", tolerancia_riesgo=\"moderada\", objetivos_inversion=\"ahorros jubilación\" )\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "kYgEPiWeax93",
        "outputId": "b4665c9a-0d71-4034-8164-a6a615222f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEres un asesor financiero certificado. En función del perfil financiero del usuario, brindar asesoramiento de inversión personalizado.\\nPerfil de usuario:\\n- Edad: 50\\n- Ingresos: 100000€\\n- Tolerancia al riesgo: moderada\\n- Objetivos de inversión: ahorros jubilación\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`FewShotPromptTemplate` en LangChain es una herramienta poderosa para el aprendizaje de pocos ejemplos (few-shot learning) en aplicaciones de lenguaje. Te permite definir un conjunto de ejemplos de entrada/salida y, junto con una plantilla de prompt, generar prompts para un modelo de lenguaje que imitan un comportamiento deseado. Básicamente, ayuda a entrenar a un modelo para que aprenda a partir de unos pocos ejemplos cómo realizar una tarea.\n",
        "\n",
        "Aquí te explico su funcionamiento y te doy ejemplos:\n",
        "\n",
        "**¿Cómo funciona `FewShotPromptTemplate`?**\n",
        "\n",
        "1. **`examples`:** Un array de diccionarios. Cada diccionario representa un ejemplo y contiene pares clave-valor con las entradas y salidas deseadas para una tarea específica.\n",
        "    \n",
        "2. **`example_prompt`:** Una instancia de `PromptTemplate` que define cómo se formatea cada ejemplo individualmente. Define las variables de entrada que se utilizarán para cada ejemplo.\n",
        "    \n",
        "3. **`prefix`:** Una cadena que va al principio del prompt completo. Suele ser una instrucción general o una descripción de la tarea.\n",
        "    \n",
        "4. **`suffix`:** Una cadena que va al final del prompt completo. Normalmente aquí es donde se introduce la nueva entrada a procesar por el modelo.\n",
        "    \n",
        "5. **`input_variables`:** Una lista de nombres de variables que se utilizarán en el `suffix`.\n",
        "    \n",
        "6. **`example_separator`:** Una cadena que separa cada ejemplo en el prompt. Por defecto es `\\n\\n`.\n",
        "    \n",
        "7. **`template_format`:** Define el formato en el que se va a procesar la plantilla. Por defecto es `f-string`. Otras opciones son `jinja2`.\n",
        "    \n",
        "\n",
        "**En resumen, `FewShotPromptTemplate` toma los ejemplos, los formatea usando `example_prompt`, los une con `example_separator`, y los coloca entre `prefix` y `suffix` para crear el prompt final. Este prompt contiene el contexto necesario (los ejemplos) para que el modelo de lenguaje pueda inferir la tarea y aplicarla a la nueva entrada proporcionada en el `suffix`.**\n",
        "\n",
        "Hay que usar la sintaxis con el constructor de clase. No hay disponible ningun metodo que simplifique la sintaxis."
      ],
      "metadata": {
        "id": "nyuWJG_Pe56-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "\n",
        "# Lista de ejemplo, donde cada ejemplo es un diccionario\n",
        "ejemplos = [\n",
        "    {\"input\": \"Hello\", \"output\": \"Bonjour\"},\n",
        "    {\"input\": \"Goodbye\", \"output\": \"Au revoir\"},\n",
        "    {\"input\": \"Thank you\", \"output\": \"Merci\"},\n",
        "]\n",
        "\n",
        "#Instancia de PromptTemplate para formatear los ejemplos\n",
        "traduccion_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Creamos el prompt pasando:\n",
        "# la lista de ejemplos\n",
        "# el formateador de los ejemplos\n",
        "# el prefix, que suele ser la descripcion general de la tarea\n",
        "# el suffix, que donde introduciomes la pregunta con la variables\n",
        "# La lista de variables\n",
        "\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=ejemplos,\n",
        "    example_prompt=traduccion_prompt_template,\n",
        "    prefix=\"Translate the following English words to French:\",\n",
        "    suffix=\"Input: {english_word}\\nOutput:\",\n",
        "    input_variables=[\"english_word\"],\n",
        ")\n",
        "\n",
        "print(prompt.format(english_word=\"Car\"))"
      ],
      "metadata": {
        "id": "FXgo2n6qe5Aw",
        "outputId": "64c7472d-1197-4ecf-9e65-7d11530869f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the following English words to French:\n",
            "\n",
            "Input: Hello\n",
            "Output: Bonjour\n",
            "\n",
            "Input: Goodbye\n",
            "Output: Au revoir\n",
            "\n",
            "Input: Thank you\n",
            "Output: Merci\n",
            "\n",
            "Input: Car\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear un prompt con ejemplos para que indique la capital de un pais"
      ],
      "metadata": {
        "id": "Fq1_JFYc0Gnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refs:\n",
        "\n"
      ],
      "metadata": {
        "id": "YW5cRwDPNhog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las prompt templatespermiten crear solicitudes dinámicas y flexibles mediante la incorporación de variables y opciones de formato, lo que le permite personalizar las solicitudes en función de los datos de entrada o tareas específicas.\n",
        "\n",
        "- Son dinamicas: tinene placeholders para variables\n",
        "- Pueden reutilizarse\n",
        "\n",
        "Un prompt template como este \"Explicame {tema} con nivel de dificultad {nivel}\" es reutilizable y flexible, sirve para muchos situaciones.\n",
        "\n",
        "\"traduce {frase} del {idioma_entrada} a {idioma_salida}\"\n",
        "\n",
        "\n",
        "Diferencias entre ChatPromptTemplate y PrompTemplate\n",
        "Difrenecias entre .format y .invoke\n",
        "\n",
        "- Pueden tener locga condicional interna (if)\n",
        "- Pueden anidarse\n",
        "\n",
        "---\n",
        "¿Qué es Prompt?  \n",
        "\n",
        "Un mensaje para un modelo de lenguaje es un conjunto de instrucciones o entradas proporcionadas por un usuario para guiar la respuesta del modelo, ayudándolo a comprender el contexto y generar resultados relevantes y coherentes basados ​​en el lenguaje, como responder preguntas, completar oraciones o participar en una conversación.  \n",
        "\n",
        "---\n",
        "3 tipos de plantillas de indicaciones de LangChain¶\n",
        "Cuando se solicita en LangChain, se recomienda (aunque no es obligatorio) utilizar una clase de plantilla predefinida como:\n",
        "\n",
        "PromptTemplatepara crear indicaciones básicas.\n",
        "FewShotPromptTemplatepara un aprendizaje de unos pocos disparos.\n",
        "ChatPromptTemplatepara modelar interacciones de chatbot."
      ],
      "metadata": {
        "id": "zy0Qe8UGSZMo"
      }
    }
  ]
}