{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlzYc4agbuhIOciRPO2Mkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanfranbrv/curso-langchain/blob/main/Soluci%C3%B3n.%20Generador%20de%20Res%C3%BAmenes%20Inteligente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Configuración del entorno del cuaderno**\n",
        "---\n",
        "En esta celda se realiza la configuración inicial del entorno:\n",
        "- Se importa la librería `userdata` de Google Colab para acceder a claves API almacenadas de forma segura.\n",
        "- Se obtienen las claves API de servicios como OpenAI, Groq, Google GenAI y Hugging Face.\n",
        "- Se instalan las librerías necesarias, como `langchain` y sus integraciones con diferentes proveedores de modelos de lenguaje (LLMs).\n",
        "- Se importan las clases de LangChain para crear plantillas de prompts (`ChatPromptTemplate`, `SystemMessagePromptTemplate`, `HumanMessagePromptTemplate`) y para interactuar con los LLMs (`ChatOpenAI`, `ChatGroq`, etc.).\n",
        "- También se importa `Markdown` de `IPython.display` para mejorar la presentación de texto en el notebook."
      ],
      "metadata": {
        "id": "IvMzAL24KR3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoT2cYJWCso8"
      },
      "outputs": [],
      "source": [
        "# Importar la librería `userdata` de Google Colab.\n",
        "# Esta librería se utiliza para acceder a datos de usuario almacenados de forma segura en el entorno de Colab.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Obtener las claves API de diferentes servicios desde el almacenamiento seguro de Colab.\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY=userdata.get('GROQ_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN=userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Instalar las librerías necesarias usando pip.\n",
        "# El flag `-qU` instala en modo silencioso (`-q`) y actualiza las librerías si ya están instaladas (`-U`).\n",
        "!pip install langchain -qU  # Instalar la librería principal de LangChain.\n",
        "\n",
        "# Instalar las integraciones de LangChain con diferentes proveedores de LLMs.\n",
        "!pip install langchain-openai -qU\n",
        "!pip install langchain-groq -qU\n",
        "!pip install langchain-google-genai -qU\n",
        "!pip install langchain-huggingface -qU\n",
        "\n",
        "# Importar las clases necesarias de LangChain para crear plantillas de prompt.\n",
        "# `ChatPromptTemplate` es la clase base para plantillas de chat.\n",
        "# `SystemMessagePromptTemplate` se usa para mensajes del sistema (instrucciones iniciales).\n",
        "# `HumanMessagePromptTemplate` se usa para mensajes del usuario.\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Importar las clases para interactuar con los diferentes LLMs a través de LangChain.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Configuración del modelo de OpenAI**\n",
        "---\n",
        "Aquí se configura el modelo de OpenAI (`gpt-4o`) utilizando la clave API obtenida previamente. Se establece un parámetro de `temperature` en 0.7 para controlar la creatividad de las respuestas del modelo."
      ],
      "metadata": {
        "id": "CNqxDxxQMJD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\",\n",
        "                 api_key=OPENAI_API_KEY,\n",
        "                 temperature=0.7)"
      ],
      "metadata": {
        "id": "_QsfXYdnDQqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Definimos las plantilla para los mensajes**\n",
        "---\n",
        "Se definen dos plantillas de prompts:\n",
        "1. **`template_sistema`:** Instrucciones detalladas para el modelo, explicando su tarea de generar dos tipos de resúmenes (breve y detallado) a partir de un texto dado.\n",
        "2. **`template_usuario`:** Plantilla para el mensaje del usuario, que incluye el texto que se desea resumir."
      ],
      "metadata": {
        "id": "zGZMRdfCMaTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las plantillas\n",
        "template_sistema = \"\"\"\\\n",
        "Eres un asistente experto en resumir textos largos. Tu tarea es generar dos tipos de resúmenes para el texto que recibas:\n",
        "\n",
        "1. Resumen breve: Un resumen conciso de 3 a 4 oraciones que capture las ideas principales del texto.\n",
        "2. Resumen detallado: Un resumen más extenso, de al menos un párrafo, que incluya detalles clave, ejemplos o puntos importantes del texto.\n",
        "\n",
        "Instrucciones:\n",
        "- Lee siempre el texto cuidadosamente.\n",
        "- Identifica las ideas principales y los detalles relevantes.\n",
        "- Genera primero el resumen breve y luego el resumen detallado.\n",
        "- Asegúrate de que ambos resúmenes sean claros, coherentes y fieles al contenido original.\n",
        "\n",
        "Ejemplo de formato de salida:\n",
        "Resumen breve:   El cambio climático es un desafío global que causa el aumento de temperaturas, el derretimiento de glaciares y la subida del nivel del mar. Sus efectos impactan negativamente en los ecosistemas y las comunidades humanas. Para mitigarlo, es crucial reducir las emisiones y adoptar energías renovables.\n",
        "Resumen detallado: El cambio climático representa uno de los problemas más urgentes de la actualidad, caracterizado por el aumento de las temperaturas globales, el derretimiento acelerado de los glaciares y el incremento del nivel del mar. Estos fenómenos tienen consecuencias graves, como la pérdida de biodiversidad, la destrucción de ecosistemas y el desplazamiento de comunidades humanas. Para enfrentar este desafío, es fundamental implementar medidas como la reducción de emisiones de gases de efecto invernadero, la transición hacia energías renovables y la adopción de prácticas sostenibles en la industria, la agricultura y otros sectores clave.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "template_usuario = \"\"\"\\\n",
        "Texto a resumir: {mensaje_usuario}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NeV4GFftDTIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Creación del ChatPromptTemplate**\n",
        "---\n",
        "Se crea un `ChatPromptTemplate` utilizando las plantillas definidas en la celda anterior. Este template combina las instrucciones del sistema (`template_sistema`) y el mensaje del usuario (`template_usuario`) en un formato que el modelo puede entender."
      ],
      "metadata": {
        "id": "Sg30cRW7MwgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el ChatPromptTemplate\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template_sistema),\n",
        "    (\"human\", template_usuario),\n",
        "])"
      ],
      "metadata": {
        "id": "dqyW-EAxFY4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Pedimos el texto a resumir**\n",
        "---"
      ],
      "metadata": {
        "id": "_cISKmFJM_KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_input = input(\"Escribe aquí el texto a resumir\")"
      ],
      "metadata": {
        "id": "ZEhSslbQGotL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6. Generación de resúmenes**\n",
        "---\n",
        "Formateamos el ChatPromptTemplate e invocamos al modelo pasandoselo\n",
        "Presentamos la respuestas."
      ],
      "metadata": {
        "id": "NgUMAc5FNK1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Formatear el prompt con el mensaje del usuario y el idioma\n",
        "    prompt = chat_prompt_template.format(\n",
        "        mensaje_usuario=texto_input,\n",
        "\n",
        "    )\n",
        "\n",
        "    # Obtener la respuesta del modelo\n",
        "    respuesta = llm.invoke(prompt)\n",
        "    display(Markdown(respuesta.content))"
      ],
      "metadata": {
        "id": "Xvifd1PDFsM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}